-- phpMyAdmin SQL Dump
-- version 3.5.1
-- http://www.phpmyadmin.net
--
-- Host: localhost
-- Generation Time: Jul 09, 2013 at 04:13 PM
-- Server version: 5.5.24-log
-- PHP Version: 5.3.13

SET SQL_MODE="NO_AUTO_VALUE_ON_ZERO";
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;

--
-- Database: `acadse`
--
CREATE DATABASE `acadse` DEFAULT CHARACTER SET latin1 COLLATE latin1_swedish_ci;
USE `acadse`;

-- --------------------------------------------------------

--
-- Table structure for table `admin`
--

CREATE TABLE IF NOT EXISTS `admin` (
  `name` text
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

-- --------------------------------------------------------

--
-- Table structure for table `authors`
--

CREATE TABLE IF NOT EXISTS `authors` (
  `Id_aut` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(40) NOT NULL,
  PRIMARY KEY (`Id_aut`),
  UNIQUE KEY `name` (`name`)
) ENGINE=InnoDB  DEFAULT CHARSET=latin1 AUTO_INCREMENT=1384 ;

--
-- Dumping data for table `authors`
--

INSERT INTO `authors` (`Id_aut`, `name`) VALUES
(1269, 'A. Olivero'),
(1333, 'Abraham Lempel'),
(1338, 'Adrian Perrig'),
(1361, 'Alan Mainwaring'),
(1297, 'Albert-lszl Barabsi Director'),
(1344, 'Alec Woo'),
(1320, 'Amir Ronen'),
(1258, 'Andrew Y. Ng'),
(1379, 'authors'),
(1291, 'Ben Liang'),
(1314, 'Ben Shneiderman'),
(1325, 'Bernhard E. Boser'),
(1371, 'Berthold K. P. Horn'),
(1322, 'Brendan J. Frey'),
(1264, 'C. Courcoubetis'),
(1294, 'Corinna Cortes'),
(1359, 'David A. Maltz'),
(1300, 'David Andersen'),
(1358, 'David B. Johnson'),
(1341, 'David Culler'),
(1370, 'David Harel'),
(1349, 'David Heckerman'),
(1353, 'David M. Chickering'),
(1272, 'David W. Aha'),
(1350, 'Deborah Estrin'),
(1273, 'Dennis Kibler'),
(1337, 'E. Cayirci'),
(1284, 'Erdal Cayirci'),
(1326, 'et al.'),
(1321, 'Frank R. Kschischang'),
(1302, 'Frans Kaashoek'),
(1313, 'Gary Bishop'),
(1372, 'H.M. Hilden'),
(1323, 'Hans-Andrea Loeliger'),
(1301, 'Hari Balakrishnan'),
(1354, 'Henry A. Rowley'),
(1308, 'Hugues Hoppe'),
(1329, 'Hyeonjoon Moon'),
(1334, 'I. F. Akyildiz'),
(1304, 'Ivan Edward Sutherland'),
(1342, 'J. D. Tygar'),
(1270, 'J. Sifakis'),
(1332, 'Jacob Ziv'),
(1343, 'Jason Hill'),
(1290, 'Jing Deng'),
(1369, 'Jitendra Malik'),
(1362, 'John Anderson'),
(1352, 'John Heidemann'),
(1311, 'John K. Ousterhout'),
(1363, 'John Lafferty'),
(1306, 'Jonathan A. Zarge'),
(1360, 'Joseph Polastre'),
(1346, 'Kristofer Pister'),
(1315, 'L. Greengard'),
(1281, 'Lan F. Akyildiz'),
(1312, 'Leonard McMillan'),
(1324, 'Lov K. Grover'),
(1280, 'M. E. J. Newman'),
(1274, 'Marc K. Albert'),
(1347, 'Martin Roesch'),
(1383, 'Me'),
(1317, 'Michael E. Wolf'),
(1367, 'Michael F. Cohen'),
(1309, 'Michael Garland'),
(1259, 'Michael I. Jordan'),
(1318, 'Monica S. Lam'),
(1265, 'N. Halbwachs'),
(1286, 'Ning Cai'),
(1319, 'Noam Nisan'),
(1328, 'P. Jonathon Phillips'),
(1299, 'P. R. Kumar'),
(1267, 'P.-H. Ho'),
(1292, 'Panagiotis Papadimitratos'),
(1331, 'Patrick J. Rauss'),
(1368, 'Paul E. Debevec'),
(1310, 'Paul S. Heckbert'),
(1261, 'Peter Clark'),
(1298, 'Piyush Gupta'),
(1381, 'popo'),
(1263, 'R. Alur'),
(1365, 'Radek Grzeszczuk'),
(1275, 'Rakesh Agrawal'),
(1276, 'Ramakrishnan Srikant'),
(1351, 'Ramesh Govindan'),
(1327, 'Randal E. Bryant'),
(1288, 'Raymond W. Yeung'),
(1296, 'Reka Zsuzsanna Albert'),
(1366, 'Richard Szeliski'),
(1279, 'Rina Dechter'),
(1278, 'Robert E. Schapire'),
(1303, 'Robert Morris'),
(1339, 'Robert Szewczyk'),
(1285, 'Rudolf Ahlswede'),
(1293, 'S. Sajama'),
(1271, 'S. Yovine'),
(1380, 'sa'),
(1382, 'SE'),
(1345, 'Seth Hollar'),
(1373, 'Shariar Negahdaripour'),
(1378, 'shashank'),
(1356, 'Shumeet Baluja'),
(1287, 'Shuo-yen Robert Li'),
(1348, 'Stanford Telecommunications'),
(1364, 'Steven J. Gortler'),
(1355, 'Student Member'),
(1330, 'Syed A. Rizvi'),
(1266, 'T. A. Henzinger'),
(1357, 'Takeo Kanade'),
(1262, 'Tim Niblett'),
(1316, 'V. Rokhlin'),
(1340, 'Victor Wen'),
(1295, 'Vladimir Vapnik'),
(1335, 'W. Su'),
(1282, 'Welljan Su'),
(1307, 'William E. Lorensen'),
(1305, 'William J. Schroeder'),
(1268, 'X. Nicollin'),
(1336, 'Y. Sankarasubramaniam'),
(1260, 'Yair Weiss'),
(1277, 'Yoav Freund'),
(1283, 'Yogesh Sankarasubramaniam'),
(1289, 'Zygmunt J. Haas');

-- --------------------------------------------------------

--
-- Table structure for table `keywords`
--

CREATE TABLE IF NOT EXISTS `keywords` (
  `Id_key` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `word` varchar(30) NOT NULL,
  PRIMARY KEY (`Id_key`),
  UNIQUE KEY `word` (`word`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1 AUTO_INCREMENT=1 ;

-- --------------------------------------------------------

--
-- Table structure for table `paper_author`
--

CREATE TABLE IF NOT EXISTS `paper_author` (
  `Id_aut` int(10) unsigned NOT NULL,
  `Id_ppr` int(10) unsigned NOT NULL,
  PRIMARY KEY (`Id_aut`,`Id_ppr`),
  KEY `Id_aut` (`Id_aut`),
  KEY `Id_ppr` (`Id_ppr`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

--
-- Dumping data for table `paper_author`
--

INSERT INTO `paper_author` (`Id_aut`, `Id_ppr`) VALUES
(1258, 542),
(1259, 542),
(1259, 547),
(1259, 558),
(1259, 559),
(1260, 542),
(1261, 543),
(1262, 543),
(1263, 544),
(1264, 544),
(1265, 544),
(1266, 544),
(1267, 544),
(1268, 544),
(1269, 544),
(1270, 544),
(1271, 544),
(1272, 545),
(1273, 545),
(1274, 545),
(1275, 546),
(1276, 546),
(1277, 548),
(1278, 548),
(1279, 549),
(1280, 550),
(1281, 551),
(1282, 551),
(1283, 551),
(1284, 551),
(1285, 552),
(1286, 552),
(1287, 552),
(1288, 552),
(1289, 553),
(1290, 553),
(1291, 553),
(1292, 553),
(1293, 553),
(1294, 554),
(1295, 554),
(1296, 555),
(1297, 555),
(1298, 556),
(1299, 556),
(1300, 557),
(1301, 557),
(1302, 557),
(1303, 557),
(1304, 560),
(1305, 561),
(1306, 561),
(1307, 561),
(1308, 562),
(1309, 563),
(1310, 563),
(1311, 564),
(1312, 565),
(1313, 565),
(1314, 566),
(1315, 567),
(1316, 567),
(1317, 568),
(1318, 568),
(1319, 569),
(1320, 569),
(1321, 570),
(1322, 570),
(1323, 570),
(1324, 571),
(1325, 572),
(1326, 572),
(1327, 573),
(1328, 574),
(1329, 574),
(1330, 574),
(1331, 574),
(1332, 575),
(1333, 575),
(1334, 576),
(1335, 576),
(1336, 576),
(1337, 576),
(1338, 577),
(1339, 577),
(1339, 578),
(1339, 585),
(1340, 577),
(1341, 577),
(1341, 578),
(1341, 585),
(1342, 577),
(1343, 578),
(1344, 578),
(1345, 578),
(1346, 578),
(1347, 579),
(1348, 579),
(1349, 580),
(1349, 582),
(1350, 581),
(1351, 581),
(1352, 581),
(1353, 582),
(1354, 583),
(1355, 583),
(1356, 583),
(1357, 583),
(1358, 584),
(1359, 584),
(1360, 585),
(1361, 585),
(1362, 585),
(1363, 586),
(1364, 587),
(1365, 587),
(1366, 587),
(1367, 587),
(1368, 588),
(1369, 588),
(1370, 589),
(1371, 590),
(1372, 590),
(1373, 590),
(1378, 603),
(1379, 604),
(1380, 605),
(1381, 606),
(1382, 607),
(1383, 608);

-- --------------------------------------------------------

--
-- Table structure for table `paper_contents`
--

CREATE TABLE IF NOT EXISTS `paper_contents` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `paperId` int(11) NOT NULL,
  `title` varchar(300) NOT NULL,
  `contents` text NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB  DEFAULT CHARSET=latin1 AUTO_INCREMENT=352 ;

--
-- Dumping data for table `paper_contents`
--

INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(291, 542, 'On Spectral Clustering: Analysis and an algorithm', '\r \r \r \r\r \r \r \r\r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r\r \r \r  \r \r\r \r  \r \r \r\r  \r \r \r \r  \r \r \r \r \r \r  \r \r  \r   \r \r \r \r\r \r \r  \r \r \r \r \r \r \r \r \r \r \r  \r \r \r \r \r \r \r \r \r \r  \r   \r \r \r\r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r\r \r \r \r \r \r  \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r\r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r  \r \r    \r \r \r \r \r \r \r \r \r \r \r\r \r \r\r \r \r \r \r  \r   \r \r\r \r \r \r\r \r \r \r \r \r \r \r \r \r \r \r  \r \r \r \r \r \r  \r \r\r \r \r \r \r \r \r \r \r \r \r \r \r \r \r\r \r\r \r \r\r \r \r \r \r \r \r\r \r \r \r \r\r \r \r  \r  \r \r \r \r \r \r \r \r \r \r \r \r  \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r \r\r \r \r \r \r \r \r \r \r  \r \r\r \r \r \r\r \r \r \r \r \r \r \r  \r \r \r \r \r \r \r0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nnips, 8 clusters\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nlineandballs, 3 clusters\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nfourclouds, 2 clusters\n 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nsquiggles, 4 clusters\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\ntwocircles, 2 clusters\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nthreecircles-joined, 2 clusters\n 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nthreecircles-joined, 3 clusters\n-0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\nRows of Y (jittered, randomly subsampled) for twocircles\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\ntwo circles, 2 clusters (K-means)\n 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nthreecircles-joined, 3 clusters (connected components)\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nlineandballs, 3 clusters (Meila and Shi algorithm)\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\nnips, 8 clusters (Kannan et al. algorithm)\n \r \r \r \r \r \r\r \r \r  \r \r \r  \r \r \r \r \r \r \r \r   \r \r  \r \r \r  \r \r  \r \r  \r  \r  \r \r \r \r  \r \r'),
(292, 543, 'The CN2 Induction Algorithm', 'In: Machine Learning Journal, 3 (4), pp261-283, Netherlands: Kluwer (1989)\r\nhttp://www.cs.utexas.edu/users/pclark/papers\r\nThe CN2 Induction Algorithm\r\nPeter Clark (pete@turing.ac.uk)\r\nTim Niblett (tim@turing.ac.uk)\r\nThe Turing Institute,\r\n36 N. Hanover St.,\r\nGlasgow, G1 2AD, U.K.\r\nOctober 1988\r\nAbstract\r\nSystems for inducing concept descriptions from examples are valuable tools for\r\nassisting in the task of knowledge acquisition for expert systems. This paper presents\r\na description and empirical evaluation of a new induction system, cn2, designed for\r\nthe ecient induction of simple, comprehensible production rules in domains where\r\nproblems of poor description language and/or noise may be present. Implementations\r\nof the cn2, id3 and aq algorithms are compared on three medical classication tasks.\r\nKeywords: concept learning, rule induction, noise, comprehensibility, cn2.\r\n1\r\n1 Introduction\r\nIn the task of constructing expert systems, systems for inducing concept descriptions\r\nfrom examples have proved useful in easing the bottleneck of knowledge acquisition [1].\r\nTwo families of systems, based on the id3 [2] and aq [3] algorithms, have been especially\r\nsuccessful. These basic algorithms assume no noise in the domain, searching for a concept\r\ndescription that classies training data perfectly. However for the application of systems\r\nbased on these algorithms to real-world domains, methods for handling noisy data are\r\nrequired. In particular, mechanisms for avoiding the overtting of the induced concept\r\ndescription to the data are needed, requiring relaxation of the constraint that the induced\r\ndescription must be classify the training data perfectly.\r\nFortunately the id3 algorithm lends itself to easy modication allowing this constraint\r\nto be relaxed, by the nature of its general-to-specic search. Tree pruning techniques (e.g.\r\n[4, 5]), as used for example in the systems c4 [6] and assistant [7], have proved to be\r\neective methods of avoiding overtting. The aq algorithm, however, is less easy to\r\nmodify due to its dependence on specic training examples during its search. Existing\r\nimplementations (e.g. aq11 [8] and aq15 [9]) deal with noisy data by using pre- and\r\npost-processing techniques while leaving the basic aq algorithm intact. Our objective in\r\ndesigning cn2 is to modify the aq algorithm itself in such a way that this dependence\r\non specic examples is removed and the space of rules searched is increased. As a result\r\nstatistical techniques analogous to those used for tree pruning can then be applied in the\r\ngeneration of if-then rules, and a simpler algorithm is achieved.\r\nWe can identify several requirements that learning systems should meet if they are to\r\nprove useful in a variety of real-world situations:\r\nAccurate classication. The induced rules should be able to classify new examples\r\naccurately, even in the presence of noise.\r\nSimple rules. For the sake of comprehensibility, the induced rules should be as short\r\nas possible. However, when noise is present, rules that are overtted tend to be\r\nlong. Thus, to induce short rules, one must usually relax the requirement that the\r\ninduced rules be consistent with all the training data. The choice of how much to\r\nrelax this requirement involves a trade-o between accuracy and simplicity [10].\r\nEcient rule generation. If one expects to use large example sets, it is important that\r\nthe algorithm scales up to complex situations. In practice, it is desirable that the\r\ntime taken for rule generation be linear in the size of the example set.\r\n2\r\nWith these requirements in mind, this paper presents a description and empirical\r\nevaluation of cn2, a new induction algorithm. It combines the eciency and ability to\r\ncope with noisy data of id3 with the if-then rule form and \rexible search strategy of the\r\naq family. The representation for rules output by cn2 is an ordered set of if-then rules,\r\nalso known as a `decision list [11]. cn2 uses a heuristic function to terminate search\r\nduring rule construction, based on an estimate of the noise present in the data. This\r\nresults in rules that do not necessarily classify all the training examples correctly, but\r\nthat perform well on new data.\r\nIn the following section we describe cn2 and three other systems used for our com-\r\nparative study. These include: aqr, the authors reconstruction of Michalski et als aq\r\nalgorithm; Kononenko, Bratko and Roskars (1984) assistant, a variant of id3; and a\r\nsimple Bayesian classier which is used to provide a reference for the performance of the\r\nother algorithms. In each case we consider the time complexity of the various algorithms.\r\nIn section 4, we compare the performance of the algorithms on three medical tasks; we\r\nalso compare the performance of assistant and cn2 on two synthetic tasks. In section 5\r\nwe discuss the signicance of these results, and we follow this with some suggestions for\r\nfuture work in section 6.\r\n2 CN2 and Algorithms for Comparative Study\r\ncn2 and the other algorithms used in experiments are now presented. Because cn2\r\nhas been developed from study of both the id3 and aq algorithms, we rst present the\r\nid3-based system assistant and the aq-based system aqr before presenting cn2 and\r\ndiscussing its relationship to these algorithms.\r\nWe characterize the systems along three dimensions. These are:\r\n The representation language for the induced knowledge;\r\n The performance engine for executing the rules; and\r\n The learning algorithm and its associated search heuristics.\r\nIn all of our experiments, the example description language consisted of attributes, at-\r\ntribute values and user-specied classes. This language was the same for each algorithm.\r\n2.1 Assistant\r\nThe assistant algorithm [7] is a descendant of Quinlans id3 (1983), and incorporates\r\na tree pruning mechanism for handling noisy data.\r\n3\r\nLet: E be a set of examples\r\nA be a set of attributes for describing examples\r\nTE(E) be a termination criterion\r\nIDM(a\r\ni\r\n; E) be an evaluation function where a\r\ni\r\n2 A\r\nProcedure assistant(E) returning TREE:\r\nIf: E satises the termination criterion TE(E) then return a leaf node\r\nfor TREE, labelled with the most common class of examples in E.\r\nElse: determine the attribute a\r\nbest\r\n2 A with the largest value of the\r\nfunction IDM(a\r\nbest\r\n; E). Then, for each value v\r\nj\r\nof attribute a\r\nbest\r\n,\r\ngenerate subtrees using assistant(E\r\nj\r\n) where E\r\nj\r\nare those examples\r\nin E with value v\r\nj\r\nfor attribute a\r\nbest\r\n. Return a node labelled as a\r\ntest on attribute a\r\nbest\r\nwith these subtrees attached.\r\nTable 1: The core of the assistant algorithm\r\n2.1.1 Concept Description Language and Interpretation\r\nassistant represents acquired knowledge in the form of decision trees. An internal node\r\nof a tree species a test of an attribute, with each outgoing branch corresponding to a\r\npossible result of this test. Leaf nodes represent the classication to be assigned to an\r\nexample.\r\nTo classify a new example, a path from the root of the decision tree to a leaf node\r\nis traced. At each internal node reached, the branch corresponding to the value of the\r\nattribute tested at that node is followed. The class at the leaf node represents the class\r\nprediction for that example.\r\n2.1.2 Learning Algorithm\r\nassistant induces a decision tree by repeatedly specializing leaf nodes of an initially\r\nsingle-noded tree. The specialization operation involves replacing a leaf node with an\r\nattribute test, and adding new leaves to that node corresponding to the possible results of\r\nthat test. Heuristics determine which attribute to test on and when to stop specialization.\r\nTable 1 summarizes this algorithm.\r\n4\r\n2.1.3 Heuristic Functions\r\nassistant uses an entropy measure to guide the growth of the decision tree, as described\r\nby Quinlan (1983) . This corresponds to the function IDM in Table 1. In addition, the\r\nalgorithm can apply a tree cuto method based on an estimate of maximal classication\r\nprecision. This technique estimates whether additional branching would reduce classica-\r\ntional accuracy and if so, terminates search (there are no user-changeable parameters in\r\nthis calculation). This cuto criterion corresponds to the function TE in the Table 1. If\r\nassistant is to generate an `unpruned tree, the termination criterion TE(E) is satised\r\nif all the examples E have the same class value.\r\n2.2 AQR\r\naqr is an induction system that uses the basic aq algorithm [3] to generate a set of\r\nclassication rules. Many systems use this algorithm in a more sophisticated manner\r\nthan aqr to improve predictive accuracy and rule simplicity (e.g., aq11 [8] uses a more\r\ncomplex method of rule interpretation that involves degrees of conrmation). aqr is a\r\nreconstruction of a straightforward aq-based system.\r\n2.2.1 Concept Description Language and Interpretation\r\naqr induces a set of decision rules, one for each class. Each rule is of the form `if <cover>\r\nthen predict <class>, where <cover> is a boolean combination of attribute tests as we\r\nnow describe.\r\nThe basic test on an attribute is called a selector. The following are examples of\r\nselectors:\r\nhCloudy = yesi\r\nhWeather = wet ^ stormyi\r\nhTemp > 60i\r\naqr allows tests in the set f=;; >; 6=g. A conjunct of selectors is called a complex,\r\nand a disjunct of complexes is called a cover. We say that an expression (a selector,\r\ncomplex, or cover) covers an example if the expression is true of the example. Thus,\r\nthe empty complex (conjunct of zero attribute tests) covers all examples and the empty\r\ncover (disjunct of zero complexes) covers no examples. A cover is stored along with an\r\nassociated class value, representing the most common class of those training examples\r\nwhich it covers.\r\n5\r\nIn aqr, a new example is classied by nding which of the induced rules have their\r\nconditions satised by the example. If the example satises only one rule, then the class\r\npredicted by that rule is assigned to the example. If the example satis'),
(293, 544, 'The algorithmic analysis of hybrid systems', 'Theoretical Computer Science   	\n \0\r\nThe Algorithmic Analysis of Hybrid Systems\r\nR Alur\r\n\0\r\nC Courcoubetis\r\ny\r\nN Halbwachs\r\nz\r\nTA Henzinger\r\nx\r\nPH Ho\r\nx\r\nX Nicollin\r\nz\r\nA Olivero\r\nz\r\nJ Sifakis\r\nz\r\nS Yovine\r\nz\r\nAbstract\r\nWe present a general framework for the formal specication and algorithmic analysis of hybrid systems\r\nA hybrid system consists of a discrete program with an analog environment We model hybrid systems\r\nas nite automata equipped with variables that evolve continuously with time according to dynamical\r\nlaws For verication purposes we restrict ourselves to linear hybrid systems where all variables\r\nfollow piecewiselinear trajectories We provide decidability and undecidability results for classes of\r\nlinear hybrid systems and we show that standard programanalysis techniques can be adapted to linear\r\nhybrid systems In particular we consider symbolic modelchecking and minimization procedures that\r\nare based on the reachability analysis of an innite state space The procedures iteratively compute\r\nstate sets that are denable as unions of convex polyhedra in multidimensional real space We also\r\npresent approximation techniques for dealing with systems for which the iterative procedures do not\r\nconverge\r\n\0 Introduction\r\nA hybrid system consists of a discrete program with an analog environment We assume that a run of a\r\nhybrid system is a sequence of steps Within each step the system state evolves continuously according\r\nto a dynamical law until a transition occurs Transitions are instantaneous state changes that separate\r\ncontinuous state evolutions\r\nWe model a hybrid system as a nite automaton that is equipped with a set of variables The control\r\nlocations of the automaton are labeled with evolution laws At a location the values of the variables\r\nchange continuously with time according to the associated law The transitions of the automaton are\r\nlabeled with guarded sets of assignments A transition is enabled when the associated guard is true\r\nand its execution modies the values of the variables according to the assignments Each location is also\r\nlabeled with an invariant condition that must hold when the control resides at the location This model\r\nfor hybrid systems is inspired by the phase transition systems of MMP NSY	 and can be viewed as\r\na generalization of timed safety automata AD\n HNSY\n	\r\nThe purpose of this paper is to demonstrate that standard programanalysis techniques can be adapted\r\nto hybrid systems For verication purposes we restrict ourselves to linear hybrid systems In a linear\r\nhybrid system for each variable the rate of change is constantthough this constant may vary from\r\nlocation to locationand the terms involved in the invariants guards and assignments are required to\r\nbe linear An interesting special case of a linear hybrid system is a timed automaton AD\n	 In a\r\ntimed automaton each continuously changing variable is an accurate clock whose rate of change with\r\ntime is always \r Furthermore in a timed automaton all terms involved in assignments are constants\r\nand all invariants and guards only involve comparisons of clock values with constants Even though the\r\n\0\r\nATT Bell Laboratories Murray Hill NJ USA\r\ny\r\nUniversity of Crete and ICS FORTH Heraklion Greece Partially supported by EspritBRA \0 REACTP\r\nz\r\nVERIMAGSPECTRE Grenoble France VERIMAG is a joint laboratory of CNRS INPG UJF and VERILOG SA\r\nassociated with Institut IMAG SPECTRE is an INRIA project Partially supported by EspritBRA \0 REACTP\r\nx\r\nComputer Science Department Cornell University Ithaca NY USA Supported in part by the National Science\r\nFoundation under grant CCR	\n	 by the United States Air Force Oce of Scienti\rc Research under contract F	\r\n	\0 and by the Defense Advanced Research Projects Agency under grant NAG	\r\nTheoretical Computer Science   	\n \r\nreachability problem for linear hybrid systems is undecidable it can be solved for timed automata In this\r\npaper we provide new decidability and undecidability results for classes of linear hybrid systems and we\r\nshow that some algorithms for the analysis of timed automata can be extended to linear hybrid systems\r\nto obtain semidecision procedures for various verication problems\r\nIn particular we consider the symbolic modelchecking method for timed automata presented\r\nin HNSY\n	 and the minimization procedure for timed automata presented in ACD\r\n\0\r\n	 Both methods\r\nperform a reachability analysis over an innite state space The procedures compute state sets by iterative\r\napproximation such that each intermediate result is denable by a linear formula that is each computed\r\nstate set is a nite union of convex polyhedra in multidimensional real space The termination of the pro\r\ncedures however is not guaranteed for linear hybrid systems To cope with this problem approximate\r\nanalysis techniques are used to enforce the convergence of iterations by computing upper approximations\r\nof state sets Approximate techniques yield either necessary or sucient verication conditions\r\nThe paper is essentially a synthesis of the results presented in ACHH NOSY HPR\n	 Sec\r\ntion  presents a general model for hybrid systems Section  denes linear hybrid systems and presents\r\ndecidability and undecidability results for the reachability problem of subclasses of linear hybrid sys\r\ntems The verication methods are presented in Section \n Some paradigmatic examples are specied\r\nand veried to illustrate the application of our results These examples are analyzed using the Kronos\r\ntool NSY NOSY	 available from Grenoble and the HyTech tool AHH HH\n	 available from\r\nCornell two symbolic model checkers for timed and hybrid systems\r\n A Model for Hybrid Systems\r\nWe specify hybrid systems by graphs whose edges represent discrete transitions and whose vertices repre\r\nsent continuous activities\r\nA hybrid system H  LocVarLabEdg  Act Inv consists of six components\r\n\0 A nite set Loc of vertices called locations\r\n\0 A nite set Var of realvalued variables A valuation  for the variables is a function that assigns a\r\nrealvalue x  R to each variable x  Var  We write V for the set of valuations\r\nA state is a pair   consisting of a location   Loc and a valuation   V  We write  for the\r\nset of states\r\n\0 A nite set Lab of synchronization labels that contains the stutter label   Lab\r\n\0 A nite set Edg of edges called transitions Each transition e   a  \r\n\0\r\n consists of a source\r\nlocation   Loc a target location \r\n\0\r\n Loc a synchronization label a  Lab and a transition\r\nrelation   V\r\n\r\n We require that for each location   Loc there is a stutter transition of the\r\nform   Id   where Id  f  j   V g\r\nThe transition e is enabled in a state   if for some valuation \r\n\0\r\n V   \r\n\0\r\n   The state \r\n\0\r\n \r\n\0\r\n\r\nthen is a transition successor of the state  \r\n\0 A labeling function Act that assigns to each location   Loc a set of activities Each activity is a\r\nfunction from the nonnegative reals R\r\n\r\nto V  We require that the activities of each location are\r\ntimeinvariant  for all locations   Loc activities f  Act and nonnegative reals t  R\r\n\r\n also\r\nf  t  Act where f  tt\r\n\0\r\n  ft  t\r\n\0\r\n for all t\r\n\0\r\n R\r\n\r\n\r\nFor all locations   Loc activities f  Act and variables x  Var  we write f\r\nx\r\nthe function\r\nfrom R\r\n\r\nto R such that f\r\nx\r\nt  ftx\r\n\0 A labeling function Inv that assigns to each location   Loc an invariant Inv  V \r\nThe hybrid system H is timedeterministic if for every location   Loc and every valuation   V \r\nthere is at most one activity f  Act with f   The activity f  then is denoted by \r\n\r\n	\r\nTheoretical Computer Science   	\n \r\nThe runs of a hybrid system\r\nAt any time instant the state of a hybrid system is given by a control location and values for all variables\r\nThe state can change in two ways\r\n\0 By a discrete and instantaneous transition that changes both the control location and the values of\r\nthe variables according to the transition relation\r\n\0 By a time delay that changes only the values of the variables according to the activities of the\r\ncurrent location\r\nThe system may stay at a location only if the location invariant is true that is some discrete transition\r\nmust be taken before the invariant becomes false\r\nA run of the hybrid system H then is a nite or innite sequence\r\n  	\r\n\r\n\r\nt\r\n\0\r\nf\r\n\0\r\n	\r\n\r\n\r\nt\r\n\r\nf\r\n\r\n	\r\n\r\n\r\nt\r\n\r\nf\r\n\r\n  \r\nof states 	\r\ni\r\n \r\ni\r\n \r\ni\r\n   nonnegative reals t\r\ni\r\n R\r\n\r\n and activities f\r\ni\r\n Act\r\ni\r\n such that for all i  \r\n\r f\r\ni\r\n  \r\ni\r\n\r\n for all   t  t\r\ni\r\n f\r\ni\r\nt  Inv \r\ni\r\n\r\n the state 	\r\ni\0\r\nis a transition successor of the state 	\r\n\0\r\ni\r\n \r\ni\r\n f\r\ni\r\nt\r\ni\r\n\r\nThe state 	\r\n\0\r\ni\r\nis called a time successor of the state 	\r\ni\r\n the state 	\r\ni\0\r\n a successor of 	\r\ni\r\n We write H	 for\r\nthe set of runs of the hybrid system H\r\nNotice that if we require all activities to be smooth functions then the run  can be described by\r\na piecewise smooth function whose values at the points of higherorder discontinuity are sequences of\r\ndiscrete state changes Also notice that for timedeterministic systems we can omit the subscripts f\r\ni\r\nfrom the next relation \r\nThe run  diverges if  is innite and the innite sum\r\nP\r\ni\r\nt\r\ni\r\ndiverges The hybrid system H is nonzeno\r\nif every nite run of H is a prex of some divergent run of H Nonzeno systems can be executed AH\n	\r\nHybrid systems as transition systems\r\nWith the hybrid system H we associate the labeled transition system T\r\nH\r\n Lab	R\r\n\r\n where the\r\nstep relation  is the union of the transitionstep relations \r\na\r\n for a  '),
(294, 545, 'Instance-based learning algorithms', 'Machine Learning, 6, 37-66 (1991)\r\n 1991 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\r\nInstance-Based Learning Algorithms\r\nDAVID W. AHA (AHA@ICS.UCI.EDU)\r\nDENNIS KIBLER (KIBLER@ICS.UCI.EDU)\r\nMARC K. ALBERT (ALBERT@ICS.UCI.EDU)\r\nDepartment of Information and Computer Science, University of California, Irvine, CA 92717\r\nEditor: J.R. Quinlan\r\nAbstract. Storing and using specific instances improves the performance of several supervised learning algorithms.\r\nThese include algorithms that learn decision trees, classification rules, and distributed networks. However, no\r\ninvestigation has analyzed algorithms that use only specific instances to solve incremental learning tasks. In this\r\npaper, we describe a framework and methodology, called instance-based learning, that generates classification\r\npredictions using only specific instances. Instance-based learning algorithms do not maintain a set of abstractions\r\nderived from specific instances. This approach extends the nearest neighbor algorithm, which has large storage\r\nrequirements. We describe how storage requirements can be significantly reduced with, at most, minor sacrifices\r\nin learning rate and classification accuracy. While the storage-reducing algorithm performs well on several real-\r\nworld databases, its performance degrades rapidly with the level of attribute noise in training instances. Therefore,\r\nwe extended it with a significance test to distinguish noisy instances. This extended algorithms performance degrades\r\ngracefully with increasing noise levels and compares favorably with a noise-tolerant decision tree algorithm.\r\nKeywords. Supervised concept learning, instance-based concept descriptions, incremental learning, learning theory,\r\nnoise, similarity\r\n1. Introduction\r\nSeveral different representations have been used to describe concepts for supervised learn-\r\ning tasks. These include decision trees (Quinlan, 1986), connectionist networks (Rumelhart,\r\nMcClelland, & The PDP Research Group, 1987), and rules (Michalski, Mozetic, Hong,\r\n& Lavrac, 1986; Clark & Niblett, 1989). Numerous empirical studies have reported that\r\nthese algorithms record excellent performances (e.g., high classification accuracies) in a\r\nlarge and varied set of applications.\r\nAlgorithms that employ these representations were improved by either storing and using\r\nspecific instances or by supporting partial matching techniques. For example, Utgoff (1989)\r\nshowed how the updating costs of incremental decision tree algorithms can be significantly\r\ndecreased by saving specific instances. Similarly, Volper and Hampson (1987) showed that\r\nthe perceptrons learning rate can be significantly increased in some applications by using\r\nspecific instance detectors. Finally, Michalski et al. (1986) extended AQ11 (Michalski &\r\nLarson, 1978) to employ partial matching strategies to support the description of probabilistic\r\nconcepts as defined in Smith and Medin (1981). The topic of this article is instance-based\r\nlearning algorithms, which use specific instances rather than pre-compiled abstractions\r\nduring prediction tasks. These algorithms can also describe probabilistic concepts because\r\nthey use similarity functions to yield graded matches between instances.\r\n38 D.W. AHA, D. KIBLER, AND M.K. ALBERT\r\nFurthermore, domain-specific systems that use instance-based learning algorithms have\r\nperformed extremely well in industrial applications. One example is ALFA (Jabbour,\r\nRiveros, Landsbergen, & Meyer, 1987), a load forecasting assistant used by the Niagara\r\nMohawk Power Company of central New York State to estimate power load. ALFA uses\r\nan instance-based learning algorithm to generate an initial prediction for power load, which\r\nis then modified according to a rule-based domain theory. ALFA achieves the same accuracy\r\nas experts but requires only two minutes to make load predictions (experts require two\r\nhours). Another example is Clarks (1989) system for geologic prospect appraisal, which\r\nis being used by Enterprise Oil to generate predictions of oil reservoir thickness and porosity\r\nat a prospect site based on prospecting information gathered from nearby sites.\r\nUsing specific instances in supervised learning algorithms decreases the costs incurred\r\nwhen updating concept descriptions, increases learning rates, allows for the representation\r\nof probabilistic concept descriptions, and focuses theory-based reasoning in real-world appli-\r\ncations. However, no investigation has analyzed algorithms that use only specific instances\r\nto solve incremental, supervised learning tasks.\r\nIn this article, we describe a methodology for instance-based learning (IBL) algorithms,\r\nprovide a geometric analysis to describe their generality and underlying intuition, address\r\ntwo problems with this approach, and summarize convincing empirical evidence which\r\nsuggests that IBL algorithms perform well in applications to artificial and real-world domains.\r\n1.1. History and related work\r\nIBL algorithms are derived from the nearest neighbor pattern classifier (Cover & Hart,\r\n1967). They are highly similar to edited nearest neighbor algorithms (Hart, 1968; Gates,\r\n1972; Dasarathy, 1980), which also save and use only selected instances to generate classifica-\r\ntion predictions. While several researchers demonstrated that edited nearest neighbor algo-\r\nrithms can reduce storage requirements with, at most, small losses in classification accuracy,\r\nthey were unable to predict the expected savings in storage requirements. We present an\r\nanalysis in Section 2.5 that answers this question: the expected storage requirements are\r\npolynomial in the size of the target concepts boundary in the instance space.\r\nEdited nearest neighbor algorithms are nonincremental and their primary goal is to main-\r\ntain perfect consistency with the initial training set. Although they summarize data, they\r\ndo not attempt to maximize classification accuracy on novel instances. This ignores real-\r\nworld problems such as noise. Therefore, edited nearest neighbor algorithms are brittle.\r\nIBL algorithms are instead incremental and their goals include maximizing classification\r\naccuracy on subsequently presented instances. We address how IBL algorithms tolerate\r\nnoise in Section 4. Further discussions on how IBL algorithms can solve real-world prob-\r\nlems can be found in Aha (1989b).\r\nThe design of IBL algorithms was also inspired by exemplar-based models of categoriza-\r\ntion (Smith & Medin, 1981).1 Exemplar-based models are one of three proposed models\r\nof categorization in the psychological literature (the others are the classical and probabilistic\r\nmodels). Several researchers have defended the exemplar-based model as psychologically\r\nplausible (Medin &Schaffer, 1978; Brooks, 1978; Hintzman, 1986; Nosofsky, 1986). How-\r\never, computationally efficient exemplar-based process models of incremental learning have\r\nINSTANCE-BASED LEARNING ALGORITHMS 39\r\nnot been published in this literature. Moreover, there is no discussion of real-world issues\r\nsuch as storage reduction and noise.\r\nMore recently, case-based reasoning (CBR) systems have been introduced to solve diag-\r\nnosis and other problems (Bareiss, Porter, & Wier, 1987; Koton, 1988; Stanfill & Waltz,\r\n1986; Rissland, Kolodner, & Waltz, 1989). Like IBL algorithms, these systems use previously\r\nprocessed cases to focus problem-solving activity on new cases. However, CBR systems\r\nalso modify cases and use parts of cases during problem solving. Case-based reasoning\r\nsystems address several issues simultaneously, the most frequent of which concerns the\r\nindexing and retrieval of cases in memory. Instance-based learning is a carefully focused\r\ncase-based learning approach that contributes evaluated algorithms for selecting good cases\r\nfor classification, reducing storage requirements, tolerating noise, and learning attribute\r\nrelevances (Kibler & Aha, 1988; Aha, 1989a). This focus allowed us to provide precise\r\nanalyses of the capabilities of IBL algorithms. Similar analyses are absent in the case-based\r\nreasoning literature due to the simultaneous concern with numerous issues. We believe\r\nthat, without a clear understanding of the capabilities and limitations of instance-based\r\nlearning algorithms, it is difficult (if not impossible) to characterize the capabilities and\r\nlimitations of more elaborate case-based reasoning systems.\r\n1.2. Outline of this article\r\nInstance-based learning algorithms suffer from several problems that must be solved before\r\nthey can be successfully applied to real-world learning tasks. For example, Breiman, Fried-\r\nman, Olshen, and Stone (1984) described several problems confronting derivatives of the\r\nnearest neighbor algorithm:\r\n1. they are computationally expensive classifiers since they save all training instances,\r\n2. they are intolerant of attribute noise,\r\n3. they are intolerant of irrelevant attributes,\r\n4. they are sensitive to the choice of the algorithms similarity function,\r\n5. there is no natural way to work with nominal-valued attributes or missing attributes, and\r\n6. they provide little usable information regarding the structure of the data.\r\nThis article focuses on reducing storage requirements (Section 3) and tolerating noisy in-\r\nstances (Section 4), the first two problems listed above. Section 5.1 includes references\r\nto research efforts that address the latter four problems.\r\nSection 2 introduces IBL algorithms and describes an analysis that is used to motivate the\r\nneed for the more elaborate algorithms described in Sections 3 and 4. We conclude in Section\r\n5 with a discussion of the limitations and advantages of applying IBL algorithms in super-\r\nvised learning tasks and prioritize those research issues which require further investigation.\r\n2. Instance-based learning\r\nIn this section we present an overview of the incremental learning task, describe a framework\r\nfor instance-based learning algorithms, detail the simplest IBL algorith'),
(295, 546, 'Fast algorithms for mining association rules', '0\n10\n20\n30\n40\n50\n60\n70\n80\n0.25 0.33 0.5 0.75 1 1.5 2\nTime (sec)\nMinimum Support\nSETM\nAIS\nAprioriTid\nApriori\n0\n20\n40\n60\n80\n100\n120\n140\n160\n0.25 0.33 0.5 0.75 1 1.5 2\nTime (sec)\nMinimum Support\nAIS\nAprioriTid\nApriori\n 0\n50\n100\n150\n200\n250\n300\n350\n0.25 0.33 0.5 0.75 1 1.5 2\nTime (sec)\nMinimum Support\nAIS\nAprioriTid\nApriori\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n0.25 0.33 0.5 0.75 1 1.5 2\nTime (sec)\nMinimum Support\nAIS\nAprioriTid\nApriori\n 0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n0.25 0.33 0.5 0.75 1 1.5 2\nTime (sec)\nMinimum Support\nAIS\nAprioriTid\nApriori\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n0.25 0.33 0.5 0.75 1 1.5 2\nTime (sec)\nMinimum Support\nAIS\nAprioriTid\nApriori\n1\n10\n100\n1000\n10000\n100000\n1e+06\n1e+07\n1 2 3 4 5 6 7\nNumber of Itemsets\nPass Number\nC-bar-k (SETM)\nC-bar-k (AprioriTid)\nC-k (AIS, SETM)\nC-k (Apriori, AprioriTid)\nL-k\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.1 0.25 0.5 0.75 1 1.5 2\nTime (sec)\nMinimum Support\nSETM\nAIS\nAprioriTid\nApriori\n\n\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\n5000\n0.1 0.05 0.025 0.01\nTime (sec)\nMinimum Support\nSETM\nAIS\nAprioriTid\nApriori\n 0\n2000\n4000\n6000\n8000\n10000\n12000\n14000\n16000\n18000\n2 1.5 1 0.75 0.5 0.25\nTime (sec)\nMinimum Support\nAIS\nAprioriTid\nApriori\n\n 0\n2\n4\n6\n8\n10\n12\n14\n1 2 3 4 5 6 7\nTime (sec)\nPass #\nApriori\nAprioriTid\n0\n100\n200\n300\n400\n500\n600\n700\n0.1 0.05 0.025 0.01\nTime (sec)\nMinimum Support\nAprioriTid\nApriori\nAprioriHybrid\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n2 1.5 1 0.75 0.5 0.25\nTime (sec)\nMinimum Support\nAprioriTid\nApriori\nAprioriHybrid\n 0\n5\n10\n15\n20\n25\n30\n35\n40\n0.25 0.33 0.5 0.75 1 1.5 2\nTime (sec)\nMinimum Support\nAprioriTid\nApriori\nAprioriHybrid\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n0.25 0.33 0.5 0.75 1 1.5 2\nTime (sec)\nMinimum Support\nAprioriTid\nApriori\nAprioriHybrid\n 0\n50\n100\n150\n200\n0.25 0.33 0.5 0.75 1 1.5 2\nTime (sec)\nMinimum Support\nAprioriTid\nApriori\nAprioriHybrid\n0\n100\n200\n300\n400\n500\n600\n700\n0.25 0.33 0.5 0.75 1 1.5 2\nTime (sec)\nMinimum Support\nAprioriTid\nApriori\nAprioriHybrid\n0\n2\n4\n6\n8\n10\n12\n100 250 500 750 1000\nRelative Time\nNumber of Transactions (in 000s)\nT20.I6\nT10.I4\nT5.I2\n0\n2\n4\n6\n8\n10\n12\n14\n1 2.5 5 7.5 10\nRelative Time\nNumber of Transactions (in Millions)\nT20.I6\nT10.I4\nT5.I2\n 0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n1000 2500 5000 7500 10000\nTime (sec)\nNumber of Items\nT20.I6\nT10.I4\nT5.I2\n 0\n5\n10\n15\n20\n25\n30\n5 10 20 30 40 50\nTime (sec)\nTransaction Size\n500\n750\n1000\n'),
(296, 547, 'Hierarchical mixtures of experts and the EM algorithm', 'Proceedings o f  1993 International Joint Conference on Neural Networks \r\nHierarchical mixtures of experts and the EM algorithm \r\nMichael I. Jordan \r\nDepartment of Brain and Cognitive Sciences \r\nMIT \r\nCambridge, MA 02139 \r\nAbstract \r\nWe present a tree-structured architecture \r\nfor supervised learning. The statistical \r\nmodel underlying the architecture is a hi- \r\nerarchical mixture model in which both \r\nthe mixture coefficients and the mixture \r\ncomponents are generalized linear models \r\n(GLIMs). Learning is treated as a max- \r\nimum likelihood problem; in particular, \r\nwe present an Expectation-Maximization \r\n(EM) algorithm for adjusting the parame- \r\nters of the architecture. We also develop an \r\non-line learning algorithm in which the pa- \r\nrameters are updated incrementally. Com- \r\nparative simulation results are presented in \r\nthe robot dynamics domain. \r\n1 INTRODUCTION \r\nIn the statistical literature and in the machine learn- \r\ning literature, divide-and-conquer algorithms have \r\nbecome increasingly popular. The CART algorithm \r\n[l], the MARS algorithm [5], and the ID3 algorithm \r\n[12] are well-known examples. These algorithms \r\nfit surfaces to data by explicitly dividing the input \r\nspace into a nested sequence of regions, and by fit- \r\nting simple surfaces (e.g., constant functions) within \r\nthese regions. The advantages of these algorithms \r\ninclude the interpretability of their solutions and the \r\nspeed of the traiining process. \r\nIn this paper wte present a neural network archi- \r\ntecture that is a close cousin to  architectures such \r\nas CART and MARS. As in our earlier work [6,7], \r\nwe formulate the learning problem for this architec- \r\nture as a maximum likelihood problem. In the cur- \r\nrent paper we utiilize the Expectation-Maximization \r\n(EM) framework to  derive the learning algorithm. \r\n2 HIERARCHICAL MIXTURES \r\nOF EXPIERTS \r\nThe algorithms that we discuss in this paper are \r\nsupervised learning algorithms. We explicitly ad- \r\ndress the case of regression, in which the input vec- \r\ntors are elements of Sm and the output vectors are \r\nelements of Xn. Our model also handles classifi- \r\nRobert A. Jacobs \r\nDepartment of Psychology \r\nUniversity of Rochester \r\nRochester, NY 14627 \r\n \r\nGatng \r\nX \r\nExpert Expert \r\nNetwork Network \r\nT T \r\n1 . \r\nI x  I x  \r\nFigure 1: A two-level hierarchical mixture of ex- \r\nperts. \r\ncation problems and counting problems in which \r\nthe outputs are integer-valued. The data are as- \r\nsumed to form a countable set of paired observations \r\nX = {(dt), y))}. In the case of the batch algorithm \r\ndiscussed below, this set is assumed to  be finite; in \r\nthe case of the on-line algorithm, the set may be \r\ninfinite. \r\nWe propose to  solve nonlinear supervised learning \r\nproblems by dividing the input space into a nested \r\nset of regions and fitting simple surfaces to the data \r\nthat fall in these regions. The regions have soft \r\nboundaries, meaning that data points may lie simul- \r\ntaneously in multiple regions. The boundaries be- \r\ntween regions are themselves simple parameterized \r\nsurfaces that are adjusted by the learning algorithm. \r\nThe hierarchical mixture-of-experts (HME) archi- \r\ntecture is shown in Figure l. The architecture is \r\na tree in which the gating networks sit a t  the non- \r\nterminals of the tree. These networks receive the \r\nvector x as input and produce scalar outputs that \r\nTo simplify the presentation, we restrict ourselves to \r\na two-level hierarchy throughout the paper. All of the \r\nalgorithms that we describe, however, generalize readily \r\nto hierarchies of arbitrary depth. See [9] for a recursive \r\nformalism that handles arbitrary trees. \r\n1339 \r\nare a partition of unity at each point in the input \r\nspzce. The experi networks sit at the leaves of the \r\ntree. Each expert produces an output vector pij \r\nfor each input vector. These output vectors proceed \r\nup the tree, being multiplied by the gating network \r\noutputs and summed at the nonterminals. \r\n-411 of the expert networks in the tree are linear with \r\na single output nonlinearity. We will refer to such a \r\nnetwork as generalized linear, borrowing the ter- \r\nminology from statistics [ll]. Expert network (i, j )  \r\nproduces its output pij as a generalized linear func- \r\ntion of the input x: \r\nwhere Uij is a weight matrix and f is a fixed continu- \r\nous nonlinearity. The vector x is assumed to include \r\na fixed component of one to allow for an intercept \r\nterm. \r\nFor regression problems, f(.) is the identity function \r\n(i.e., the experts are linear). For binary classification \r\nproblems, f(.) is generally taken to be the logistic \r\nfunction, in which case the expert outputs are inter- \r\npreted as the log odds of success under a Bernoulli \r\nprobability model. Other models (e.g., multiway \r\nclassification, counting, rate estimation and survival \r\nestimation) are handled readily by making other \r\nchoices for f(.). These models are smoothed piece- \r\nwise analogs of the corresponding generalized linear \r\nmodels (GLIMs; cf. [ll]). \r\nThe gating networks are also generalized linear. At \r\nthe top level, define linear predictors & as follows: \r\nwhere vi is a weight vector. Then the ith output of \r\nthe top-level gating network is the softmax func- \r\ntion of the [2,11]: \r\nPij = f(uijx), (1) \r\n(2) \r\nT Ei = vj x, \r\n(3) \r\nNote that the gi are positive and sum to one for each \r\nx. The gating networks at the lower level are defined \r\nsimilarly, yielding outputs gjl i  that are obtained by \r\ntaking the softmax function of linear predictors &j  = \r\nV T X  \r\nThe output vector at each nonterminal of the tree is \r\nthe weighted output of the experts below that non- \r\nterminal. That is, the output at the ith nonterminal \r\nin the second layer of the two-level tree is: \r\n$3 . \r\nPI = Cgjlipij \r\nP = C S i P i .  \r\ni \r\nand the output at the top level of the tree is: \r\ni \r\nNote that both the gs and the p  s  depend on the \r\ninput x, thus the total output is a nonlinear function \r\nof the input. \r\n2.1 A PROBABILITY MODEL \r\nThe hierarchy can be given a probabilistic interpre- \r\ntation. We suppose that the mechanism by which \r\ndata are generated by the environment involves a \r\nnested sequence of decisions that terminates in a re- \r\ngressive process that maps x to y.  The decisions \r\nare modeled as multinomial random variables. That \r\nis, for each x, we interpret the values gi(x,vf) as \r\nthe multinomial probabilities associated with the \r\nfirst decision and the gjli(x, vz) as the (conditional) \r\nmultinomial Probabilities associated with the second \r\ndecision. We use a statistical model to model these \r\nprobabilities; in particular, our choice of parameter- \r\nization (cf. Eqs. 2 and 3) corresponds to  a log-lanear \r\nprobability model (see [$]I. A log-linear model is a \r\nspecial case of a GLIM that is commonly used for \r\nsoft multiway classification [ 113. Under the log- \r\nlinear model, we interpret the gating networks as \r\nmodeling the input-dependent, multinomial proba- \r\nbilities of making particular nested sequences of de- \r\ncisions. \r\nOnce a particular sequence of decisions has been \r\nmade, output y is assumed to be generated ac- \r\ncording to the following generalized linear statistical \r\nmodel. First, a linear predictor vV is formed: \r\n722 = \r\nwhere the superscript refers to the true values of \r\nthe parameters. The expected value of y is obtained \r\nby passing the linear predictor through the lank func- \r\ntaon f: \r\nThe output y is then chosen from a probability den- \r\nsity P, with mean py2 and dispersion parameter \r\n&. We denote the density of y as: \r\n0 \r\nd3 = f(dj) .  \r\nP(Y Ix, fe3 ), \r\nwhere the parameter vector 0i9i includes the weights \r\nU$ and the dispersion parameter + f J .  We assume \r\nthe density P to be a member of the exponen- \r\ntial family of densities [ l l ] .  The interpretation of \r\nthe dispersion parameter depends on the particular \r\nchoice of density. For example, in the case of the n- \r\ndimensional Gaussian, the dispersion parameter is \r\nthe covariance matrix . \r\nGiven these assumptions, the total probability of \r\ngenerating y from x is the mixture of the proba- \r\nbilities of generating y from each of the component \r\ndensities, where the mixture components are multi- \r\nnomial probabilities: \r\ni 1 \r\n(4) \r\nNot all exponential family densities have a dispersion \r\nparameter; in particular, the Bernoulli density has no \r\ndispersion parameter. \r\n1340 \r\nNote that 8 includes the expert network parame- \r\nters 8ij0 as well as the gating network parameters vp \r\nand v!. Note also that we can utilize Eq. 4 without \r\nthe superscripts to  refer to  the probability model de- \r\nfined by a particular HME architecture, irrespective \r\nof any reference to a true model. \r\n2.2 POSTERIOR PROBABILITIES \r\nIn developing the learning algorithms to  be pre- \r\nsented in the remainder of the paper, it will prove \r\nuseful to  define posterior probabilities associated \r\nwith the nodes of the tree. The terms posterior \r\nand prior have meaning in this context during the \r\ntraining of the system. We refer to  the probabili- \r\nties g 2  and g j l i  as p r i o r  probabilities, because they \r\nare computed basad oniy on the input x, without \r\nknowledge of the corresponding target output y .  A \r\np o s t e r i o r  probability is defined once both the input \r\nand the target output are known. Using Bayes rule, \r\nwe define the posterior probabilities a t  the nodes of \r\nthe tree as follows \r\nand \r\nwhere we have drolpped the dependence on the input \r\nand the parameters to  simplify the notation. \r\nWe will also find it useful to  define the joint pos- \r\nterior probability hij, the product of hj and hjli. \r\nThis quantity is the probability that expert network \r\n(i,j) can be considered to  have generated the data, \r\nbased on knowledge of both the input and the out- \r\nput. Once again, we emphasize that all of these \r\nquantities are conditional on the input x. \r\nIn deeper trees, the posterior probability associated \r\nwith an ');
INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(297, 548, 'Experiments with a New Boosting Algorithm', 'Machine Learning: Proceedings of the Thirteenth International Conference, 1996.\r\nExperiments with a New Boosting Algorithm\r\nYoav Freund Robert E. Schapire\r\nAT&T Laboratories\r\n600 Mountain Avenue\r\nMurray Hill, NJ 07974-0636\r\nfyoav, schapireg@research.att.com\r\nAbstract. In an earlier paper, we introduced a new boosting\r\nalgorithm called AdaBoost which, theoretically, can be used to\r\nsignificantly reduce the error of any learning algorithm that con-\r\nsistently generates classifiers whose performance is a little better\r\nthan random guessing. We also introduced the related notion of a\r\npseudo-losswhich is a method for forcing a learning algorithm\r\nof multi-label concepts to concentrateon the labels that are hardest\r\nto discriminate. In this paper, we describe experiments we carried\r\nout to assess how well AdaBoost with and without pseudo-loss,\r\nperforms on real learning problems.\r\nWe performed two sets of experiments. Thefirst set compared\r\nboosting to Breimans baggingmethod when used to aggregate\r\nvarious classifiers (including decision trees and single attribute-\r\nvalue tests). We compared the performance of the two methods\r\non a collection of machine-learning benchmarks. In the second\r\nset of experiments, we studied in more detail the performance of\r\nboosting using a nearest-neighbor classifier on an OCR problem.\r\n1 INTRODUCTION\r\nBoosting is a general method for improving the perfor-\r\nmance of any learning algorithm. In theory, boostingcan be\r\nused to significantly reduce the error of any weak learning\r\nalgorithm that consistently generates classifiers which need\r\nonly be a little bit better than random guessing. Despite\r\nthe potential benefits of boosting promised by the theoret-\r\nical results, the true practical value of boosting can only\r\nbe assessed by testing the method on real machine learning\r\nproblems. In this paper, we present such an experimental\r\nassessment of a new boosting algorithm called AdaBoost.\r\nBoosting works by repeatedly running a given weak1\r\nlearning algorithm on various distributions over the train-\r\ning data, and then combining the classifiers produced by\r\nthe weak learner into a single composite classifier. The\r\nfirst provably effective boosting algorithms were presented\r\nby Schapire [20] and Freund [9]. More recently, we de-\r\nscribed and analyzed AdaBoost, and we argued that this\r\nnew boosting algorithm has certain properties which make\r\nit more practical and easier to implement than its prede-\r\ncessors [10]. This algorithm, which we used in all our\r\nexperiments, is described in detail in Section 2.\r\nHome page: http://www.research.att.com/orgs/ssr/people/uid.\r\nExpected to change to http://www.research.att.com/uid some-\r\ntime in the near future (for uid \0 fyoav, schapireg).\r\n1We use the term weak learning algorithm, even though, in\r\npractice, boosting might be combined with a quite strong learning\r\nalgorithm such as C4.5.\r\nThis paper describes two distinct sets of experiments.\r\nIn the first set of experiments, described in Section 3, we\r\ncompared boosting to bagging, a method described by\r\nBreiman [1] which works in the same general fashion (i.e.,\r\nby repeatedly rerunning a given weak learning algorithm,\r\nand combining the computed classifiers), but which con-\r\nstructs each distribution in a simplermanner. (Details given\r\nbelow.) We compared boosting with bagging because both\r\nmethods work by combining many classifiers. This com-\r\nparison allows us to separate out the effect of modifying\r\nthe distribution on each round (which is done differently by\r\neach algorithm) from the effect of votingmultipleclassifiers\r\n(which is done the same by each).\r\nIn our experiments, we compared boosting to bagging\r\nusing a number of different weak learning algorithms of\r\nvarying levels of sophistication. These include: (1) an\r\nalgorithm that searches for very simple prediction rules\r\nwhich test on a single attribute (similar to Holtes very sim-\r\nple classification rules [14]); (2) an algorithm that searches\r\nfor a single good decision rule that tests on a conjunction\r\nof attribute tests (similar in flavor to the rule-formation\r\npart of Cohens RIPPER algorithm [3] and Furnkranz and\r\nWidmers IREP algorithm [11]); and (3) Quinlans C4.5\r\ndecision-tree algorithm [18]. We tested these algorithmson\r\na collection of 27 benchmark learning problems taken from\r\nthe UCI repository.\r\nThe main conclusion of our experiments is that boost-\r\ning performs significantly and uniformly better than bag-\r\nging when the weak learning algorithm generates fairly\r\nsimple classifiers (algorithms (1) and (2) above). When\r\ncombined with C4.5, boosting still seems to outperform\r\nbagging slightly, but the results are less compelling.\r\nWe also found that boosting can be used with very sim-\r\nple rules (algorithm (1)) to construct classifiers that are quite\r\ngood relative, say, toC4.5. Kearns and Mansour [16] argue\r\nthat C4.5 can itself be viewed as a kind of boosting algo-\r\nrithm, so a comparison of AdaBoost and C4.5 can be seen\r\nas a comparison of twocompeting boostingalgorithms. See\r\nDietterich, Kearns and Mansours paper [4] for more detail\r\non this point.\r\nIn the second set of experiments, we test the perfor-\r\nmance of boosting on a nearest neighbor classifier for hand-\r\nwritten digit recognition. In this case the weak learning\r\nalgorithm is very simple, and this lets us gain some insight\r\ninto the interaction between the boosting algorithm and the\r\nnearest neighbor classifier. We show that the boosting al-\r\ngorithm is an effective way for finding a small subset of\r\nprototypes that performs almost as well as the complete set.\r\nWe also show that it compares favorably to the standard\r\nmethod of Condensed Nearest Neighbor [13] in terms of its\r\ntest error.\r\nThere seem to be two separate reasons for the improve-\r\nment in performance that is achieved by boosting. The first\r\nand better understood effect of boosting is that it generates a\r\nhypothesis whose error on the training set is small by com-\r\nbining many hypotheses whose error may be large (but still\r\nbetter than randomguessing). It seems that boostingmay be\r\nhelpful on learning problems having either of the following\r\ntwo properties. The first property, which holds for many\r\nreal-world problems, is that the observed examples tend to\r\nhave varying degrees of hardness. For such problems, the\r\nboosting algorithm tends to generate distributions that con-\r\ncentrate on the harder examples, thus challenging the weak\r\nlearning algorithm to perform well on these harder parts of\r\nthe sample space. The second property is that the learning\r\nalgorithm be sensitive to changes in the training examples\r\nso that significantly different hypotheses are generated for\r\ndifferent training sets. In this sense, boosting is similar to\r\nBreimans bagging [1] which performs best when the weak\r\nlearner exhibits such unstable behavior. However, unlike\r\nbagging, boosting tries actively to force the weak learning\r\nalgorithm to change its hypotheses by changing the distri-\r\nbution over the training examples as a function of the errors\r\nmade by previously generated hypotheses.\r\nThe second effect of boosting has to dowith variance re-\r\nduction. Intuitively, taking a weighted majority over many\r\nhypotheses, all of which were trained on different samples\r\ntaken out of the same training set, has the effect of re-\r\nducing the random variability of the combined hypothesis.\r\nThus, like bagging, boostingmay have the effect of produc-\r\ning a combined hypothesis whose variance is significantly\r\nlower than those produced by the weak learner. However,\r\nunlike bagging, boosting may also reduce the bias of the\r\nlearning algorithm, as discussed above. (See Kong and Di-\r\netterich [17] for further discussion of the bias and variance\r\nreducing effects of voting multiple hypotheses, as well as\r\nBreimans [2] very recent work comparing boosting and\r\nbagging in terms of their effects on bias and variance.) In\r\nour first set of experiments, we compare boosting and bag-\r\nging, and try to use that comparison to separate between the\r\nbias and variance reducing effects of boosting.\r\nPrevious work. Drucker, Schapire and Simard [8, 7]\r\nperformed the first experiments using a boosting algorithm.\r\nThey used Schapires [20] original boosting algorithmcom-\r\nbined with a neural net for an OCR problem. Follow-\r\nup comparisons to other ensemble methods were done by\r\nDrucker et al. [6]. More recently, Drucker and Cortes [5]\r\nused AdaBoost with a decision-tree algorithm for an OCR\r\ntask. Jackson and Craven [15] used AdaBoost to learn\r\nclassifiers represented by sparse perceptrons, and tested the\r\nalgorithm on a set of benchmarks. Finally, Quinlan [19]\r\nrecently conducted an independent comparison of boosting\r\nand bagging combined with C4.5 on a collection of UCI\r\nbenchmarks.\r\nAlgorithm AdaBoost.M1\r\nInput: sequenceofm examples h\0x1\0 y1\0    \0 \0xm\0 ymi\r\nwith labels y\r\ni\r\n\0 Y  f1\0    \0 kg\r\nweak learning algorithm WeakLearn\r\ninteger T specifying number of iterations\r\nInitialize D1\0i  1m for all i.\r\nDo for t  1\0 2\0    \0 T :\r\n1. Call WeakLearn, providing it with the distribution D\r\nt\r\n.\r\n2. Get back a hypothesis h\r\nt\r\n: X  Y .\r\n3. Calculate the error of h\r\nt\r\n: \r\nt\r\n\r\nX\r\ni:h\r\nt\r\n\0x\r\ni\r\n\0y\r\ni\r\nD\r\nt\r\n\0i.\r\nIf \r\nt\r\n 12, then set T  t 1 and abort loop.\r\n4. Set \r\nt\r\n \r\nt\r\n\01  \r\nt\r\n.\r\n5. Update distribution D\r\nt\r\n:\r\nD\r\nt1\0i \r\nD\r\nt\r\n\0i\r\nZ\r\nt\r\n\r\n\0\r\n\r\nt\r\nif h\r\nt\r\n\0x\r\ni\r\n  y\r\ni\r\n1 otherwise\r\nwhere Z\r\nt\r\nis a normalization constant (chosen so that D\r\nt1\r\nwill be a distribution).\r\nOutput the final hypothesis:\r\nh\r\n\0n\r\n\0x  arg max\r\nyY\r\nX\r\nt:h\r\nt\r\n\0xy\r\nlog\r\n1\r\n\r\nt\r\n\r\nFigure 1: The algorithm AdaBoost.M1.\r\n2 THE BOOSTING ALGORITHM\r\nIn this section, we describe our boosting algorithm, called\r\nAdaBoost. See our earlier paper [10] formore details about\r\nthe algorithm and its theoretical properties.\r\nWe describe two versions of the algorithm which we\r\ndenote AdaBoost.M1 and AdaBoost.M2. The two ver-\r\nsion'),
(298, 549, 'Constraint Networks', 'CONSTRAINT NETWORKS\r\n1\r\nRina Dechter\r\nInformation and Computer Science\r\nUniversity of California\r\nIrvine, CA 92717-3425\r\n1 Introduction\r\nConstraint-based reasoning is a paradigm for formulating knowledge as a set of constraints\r\nwithout specifying the method by which these constraints are to be satised. A variety\r\nof techniques have been developed for nding partial or complete solutions for dierent\r\nkinds of constraint expressions. These have been successfully applied to diverse tasks\r\nsuch as design, diagnosis, truth maintenance, scheduling, spatiotemporal reasoning, logic\r\nprogramming and user interface. Constraint networks are graphical representations used\r\nto guide strategies for solving constraint satisfaction problems (CSPs).\r\n1.1 Basic denitions\r\nA constraint network (CN) consists of a nite set of variables X = fX\r\n1\r\n; . . . ;X\r\nn\r\ng, each as-\r\nsociated with a domain of discrete values, D\r\n1\r\n; . . . ;D\r\nn\r\nand a set of constraints, fC\r\n1\r\n; . . . ; C\r\nt\r\ng.\r\nEach of the constraints is expressed as a relation, dened on some subset of variables, whose\r\ntuples are all the simultaneous value assignments to the members of this variable subset\r\nthat, as far as this constraint alone is concerned, are legal\r\n2\r\nFormally, a constraint C\r\ni\r\nhas\r\ntwo parts: (1) the subset of variables S\r\ni\r\n= fX\r\ni\r\n1\r\n; . . . ;X\r\ni\r\nj(i)\r\ng, on which it is dened, called\r\na constraint-subset, and (2) a relation, rel\r\ni\r\ndened over S\r\ni\r\n: rel\r\ni\r\n D\r\ni\r\n1\r\n     D\r\ni\r\nj(i)\r\n.\r\nBecause many properties of a CN depend on the structure of the constraint subsets, the\r\nscheme of a CN is dened as the set of subsets on which constraints are dened, namely,\r\nscheme(CN) = fS\r\n1\r\n; S\r\n2\r\n; . . . ; S\r\nt\r\ng; S\r\ni\r\n X. The projection of a relation  on a subset of\r\nvariables U = U\r\n1\r\n; . . . ; U\r\nl\r\nis given by \r\nU\r\n() = fx\r\nu\r\n= (x\r\nu1\r\n; . . . ; x\r\nul\r\n) j 9 x 2 ; x is an\r\nextension of x\r\nu\r\ng.\r\n1\r\nPublished in the Encyclopedia of Articial Intelligence, second edition, Wiley and Sons, pp 276-285,\r\n1992\r\n2\r\nThis does not mean that the actual representation of any constraint is necessarily in the form of its\r\ndening relation, but that the relation can, in principle, be generated using the constraints specication\r\nwithout the need to consult other constraints in the network.\r\n1\r\n1 2 3\r\n4\r\n5\r\nD1\r\nD2 4\r\n(a) (b)\r\nD\r\n12 (snail, aron), (steer, earn))\r\nC\r\n3\r\n5\r\nD\r\n= (hoses, laser, sheet, snail, steer)\r\n= (hike, aron, keet, earn, same)= D\r\n= (run, sun, let, yes, eat, ten)\r\n= (no, be, us, it)\r\n= ((hoses, same), (laser,same), (sheet, earn),\r\nFigure 1: A crossword puzzle and its CN representation.\r\nAn assignment of a unique domain value to each member of some subset of variables is\r\ncalled an instantiation. An instantiation is said to satisfy a given constraint C\r\ni\r\nif the\r\npartial assignment specied by the instantiation does not violate C\r\ni\r\n(i.e., it belongs to the\r\nprojection of rel\r\ni\r\non the common variables). An instantiation is said to be legal or locally\r\nconsistent if it satises all the (relevant) constraints of the network.\r\nA legal instantiation of all the variables of a constraint network is called a solution of the\r\nnetwork, and the set of all solutions is a relation, , dened on the set of all variables. This\r\nrelation is said to be represented by the constraint network. Formally,\r\n = f(X\r\n1\r\n= x\r\n1\r\n; . . . ;X\r\nn\r\n= x\r\nn\r\n) j 8 S\r\ni\r\n2 scheme; \r\nS\r\ni\r\n  rel\r\ni\r\ng\r\nExample 1: Figure 1a presents a simplied version of a crossword puzzle (see constraint\r\nsatisfaction). The variables are X\r\n1\r\n(1, horizontal), X\r\n2\r\n(2, vertical), X\r\n3\r\n(3, vertical), X\r\n4\r\n(4,\r\nhorizontal), and X\r\n5\r\n(5, horizontal). The scheme of this problem is fX\r\n1\r\nX\r\n2\r\n;X\r\n1\r\nX\r\n3\r\n;X\r\n4\r\nX\r\n2\r\n;X\r\n4\r\nX\r\n3\r\n;X\r\n5\r\nX\r\n2\r\ng.\r\nThe domains and some constraints are specied in Figure 1b. A tuple in the relation associ-\r\nated with this puzzle is the solution: (X\r\n1\r\n= sheet;X\r\n2\r\n= earn;X\r\n3\r\n= ten;X\r\n4\r\n= aron;X\r\n5\r\n=\r\nno).\r\nTypical tasks dened in connection with constraint networks are to determine whether a\r\nsolution exists, to nd one or all of the solutions, to determine whether an instantiation\r\nof some subset of the variables is a partial solution (i.e., is part of a global solution), etc.\r\nThese tasks are collectively called constraint satisfaction problems.\r\nTechniques used in processing constraint networks can be classied into three categories.\r\nThe rst category consists of search techniques for systematic exploration of the space of\r\nall solutions. The most common algorithm in this class is backtracking which traverses the\r\n2\r\nsearch space in a depth-rst fashion. The second category is consistency algorithms for\r\ntransforming a CN into more explicit representation. These are used primarily in a prepro-\r\ncessing phase, to improve the performance of the subsequent backtracking search, but can\r\nbe incorporated into the search procedure itself. Third are the structure-driven algorithms,\r\nwhich exploit the topological features of the network to guide the search. Structure-driven\r\nalgorithms can support both the consistency algorithms as well as the backtracking search.\r\nThis survey concentrates on techniques of the third kind, namely, structure-based algo-\r\nrithms. These together with backtracking and consistency algorithms (see constraint satis-\r\nfaction) give a complete picture of the available techniques. A brief summary of backtrack-\r\ning and consistency enforcing procedures is presented next.\r\n2 Backtracking and Consistency-Enforcing Strate-\r\ngies\r\nThe standard solution procedure for solving constraint satisfaction problems is backtrack-\r\ning search. The algorithm typically considers the variables in some order and, starting\r\nwith the rst, assigns a provisional value to each successive variable in turn as long as the\r\nassigned values are consistent with those assigned in the past. When, in the process, a\r\nvariable is encountered such that none of its domain values are consistent with previous\r\nassignments (a situation referred to as a dead-end), backtracking takes place. That is, the\r\nvalue assigned to the immediately preceding variable is replaced, and the search continues\r\nin a systematic way until either a solution is found or until it may be concluded that no\r\nsuch solution exists.\r\nImproving backtracking eciency amounts to reducing the size of its expanded search\r\nspace. This depends on the way the constraints are represented, (i.e., on the extent of\r\ntheir explicitness), the order of variables instantiation, and, when one solution suces, on\r\nthe order in which values are assigned to each variable.\r\nUsing these factors to improve the performance of backtracking algorithms, researchers\r\nhave developed procedures of two types: those that are employed in advance of perform-\r\ning the search, and those that are used dynamically during search. The former include\r\na variety of consistency-enforcing algorithms. [Montanari 74, Mackworth & Freuder 84a,\r\nFreuder 85] These transform a given constraint network into an equivalent, yet more ex-\r\nplicit, network by deducing new constraints to be added on to the network.\r\nIntuitively, a consistency-enforcing algorithm will make any partial solution of a small sub-\r\nnetwork extensible to some surrounding network. For example, the most basic consistency\r\nalgorithm, called arc-consistency or two-consistency (also known as constraint propagation\r\n3\r\nand constraint relaxation), ensures that any legal value in the domain of a single variable\r\nhas a legal match in any other selected variable. Path-consistency (or three-consistency)\r\nalgorithms ensure that any consistent solution to a two-variable subnetwork is extensible\r\nto any third variable, and, in general, i-consistency algorithms guarantee that any locally\r\nconsistent instantiation of i\0 1 variables is extensible to any i\r\nth\r\nvariable.\r\nDeciding the level of consistency that should be enforced on the network is not a clear-\r\ncut choice. Generally speaking, backtracking will benet from representations that are as\r\nexplicit as possible, having higher consistency level. However, the complexity of enforcing\r\ni-consistency is exponential in i. As a result, there is a trade-o between the eort spent\r\non preprocessing and that spent on search (backtracking.) Experimental analyses of this\r\ntrade-o have been published [Dechter & Meiri 89, Dechter 90, Haralick & Elliott 80].\r\nVariable orderings decisions have also received much consideration, and several heuristics\r\nhave been proposed [Freuder 82, Dechter & Pearl 89], all following the intuition that tightly\r\nconstrained variables should come rst. Strategies for dynamically improving the pruning\r\npower of backtracking can be conveniently classied as look-ahead schemes and look-back\r\nschemes. Look-ahead schemes are invoked whenever the algorithm is preparing to assign\r\na value to the next variable. Some of the functions that such schemes perform are:\r\n1. Calculate and record the way in which the current instantiations restrict future vari-\r\nables. This process has been referred to as constraint propagation. Examples include\r\nWaltzs algorithm [Waltz 75] and forward checking [Haralick & Elliott 80].\r\n2. Decide which variable to instantiate next (when the order is not predetermined).\r\nGenerally, it is advantageous to rst instantiate variables that maximally constrain\r\nthe rest of the search space. Therefore, the variable participating in the highest\r\nnumber of constraints is usually selected. [Freuder 82, Purdom 83, Stone & Stone 86]\r\n3. Decide which value to assign to the next variable (when there is more than one\r\ncandidate). Generally, for nding one solution, an attempt is made to assign a value\r\nthat maximizes the number of options available for future assignments\r\n[Haralick & Elliott 80, Dechter & Pearl 87].\r\nLook-back schemes are invoked when the algorithm encounters a dead-end and prepares\r\nfor the backtracking step. These schemes perform two functions:\r\n1. Decide how far to backtr'),
(299, 550, 'The structure and function of complex networks', 'arXiv:cond-mat/0303516v1  [cond-mat.stat-mech]  25 Mar 2003\nThe structure and function of complex networks\nM. E. J. Newman\nDepartment of Physics, University of Michigan, Ann Arbor, MI 48109, U.S.A. and\nSanta Fe Institute, 1399 Hyde Park Road, Santa Fe, NM 87501, U.S.A.\nInspired by empirical studies of networked systems such as the Internet, social networks, and bio-\nlogical networks, researchers have in recent years developed a variety of techniques and models to\nhelp us understand or predict the behavior of these systems. Here we review developments in this\neld, including such concepts as the small-world eect, degree distributions, clustering, network\ncorrelations, random graph models, models of network growth and preferential attachment, and\ndynamical processes taking place on networks.\nContents\nAcknowledgments 1\nI. Introduction 2\nA. Types of networks 3\nB. Other resources 4\nC. Outline of the review 4\nII. Networks in the real world 4\nA. Social networks 5\nB. Information networks 6\nC. Technological networks 8\nD. Biological networks 8\nIII. Properties of networks 9\nA. The small-world eect 9\nB. Transitivity or clustering 11\nC. Degree distributions 12\n1. Scale-free networks 13\n2. Maximum degree 14\nD. Network resilience 15\nE. Mixing patterns 16\nF. Degree correlations 17\nG. Community structure 17\nH. Network navigation 19\nI. Other network properties 19\nIV. Random graphs 20\nA. Poisson random graphs 20\nB. Generalized random graphs 22\n1. The conguration model 22\n2. Example: power-law degree distribution 23\n3. Directed graphs 24\n4. Bipartite graphs 24\n5. Degree correlations 25\nV. Exponential random graphs and Markov graphs 26\nVI. The small-world model 27\nA. Clustering coecient 28\nB. Degree distribution 28\nC. Average path length 29\nVII. Models of network growth 30\nA. Prices model 30\nB. The model of Barab asi and Albert 31\nC. Generalizations of the Barab asiAlbert model 34\nD. Other growth models 35\nE. Vertex copying models 37\nVIII. Processes taking place on networks 37\nA. Percolation theory and network resilience 38\nB. Epidemiological processes 40\n1. The SIR model 40\n2. The SIS model 42\nC. Search on networks 43\n1. Exhaustive network search 43\n2. Guided network search 44\n3. Network navigation 45\nD. Phase transitions on networks 46\nE. Other processes on networks 47\nIX. Summary and directions for future research 47\nReferences 48\nAcknowledgments\nForusefulfeedbackonearlyversionsofthisarticle, theauthor wouldparticularlyliketothankLadaAdamic,MichelleGirvan,\nPetter Holme, Randy LeVeque, Sidney Redner, Ricard Sol e, Steve Strogatz, Alexei V azquez, and an anonymous referee. For\nother helpful conversations and comments about networks thanks go to Lada Adamic, L aszl o Barab asi, Stefan Bornholdt,\nDuncan Callaway, Peter Dodds, Jennifer Dunne, Rick Durrett, Stephanie Forrest, Michelle Girvan, Jon Kleinberg, James\nMoody, Cris Moore, Martina Morris, Juyong Park, Richard Rothenberg, Larry Ruzzo, Matthew Salganik, Len Sander, Steve\nStrogatz, Alessandro Vespignani, Chris Warren, Duncan Watts, and Barry Wellman. For providing data used in calculations\nandgures,thanks gotoLadaAdamic,L aszl oBarab asi,JerryDavis,JenniferDunne, Ram onFerreriCancho, PaulGinsparg,\nJerryGrossman, Oleg Khovayko, Hawoong Jeong, DavidLipman, Neo Martinez, Stephen Muth, RichardRothenberg, Ricard\nSol e, GrigoriyStarchenko, Duncan Watts, Georey West, and Janet Wiener. Figure 2a was kindlyprovided by Neo Martinez\nand Richard Williams and Fig. 8 by James Moody. This work was supported in part by the US National Science Foundation\nunder grants DMS0109086 and DMS0234188 and by the James S. McDonnell Foundation and the Santa Fe Institute.2 The structure and function of complex networks\nI. INTRODUCTION\nA network is a set of items, which we will call vertices\nor sometimes nodes, with connections between them,\ncalled edges (Fig. 1). Systems taking the form of net-\nworks(also calledgraphsin much of the mathematical\nliterature)aboundintheworld. ExamplesincludetheIn-\nternet, the World Wide Web, socialnetworks of acquain-\ntance or other connections between individuals, organi-\nzational networks and networks of business relations be-\ntween companies, neural networks, metabolic networks,\nfood webs, distribution networks such as blood vessels\nor postal delivery routes, networks of citations between\npapers, andmanyothers (Fig. 2). This paper reviewsre-\ncent (and some not-so-recent) work on the structure and\nfunction of networked systems such as these.\nThe study of networks, in the form of mathematical\ngraph theory, is one of the fundamental pillars of dis-\ncrete mathematics. Eulers celebrated 1735 solution of\nthe K onigsberg bridge problem is often cited as the rst\ntrueproofinthetheoryofnetworks,andduringthetwen-\ntieth centurygraphtheoryhas developed into a substan-\ntial body of knowledge.\nNetworks have also been studied extensively in the so-\ncialsciences. Typicalnetworkstudiesinsociologyinvolve\nthe circulation of questionnaires, asking respondents to\ndetail their interactions with others. One can then use\nthe responses to reconstruct a network in which vertices\nrepresent individuals and edges the interactions between\nthem. Typical social network studies address issues of\ncentrality(whichindividualsarebestconnectedtoothers\nor have most inuence) and connectivity (whether and\nhow individuals are connected to one another through\nthe network).\nRecentyearshoweverhavewitnessedasubstantialnew\nmovement in network research, with the focus shifting\naway from the analysis of single small graphs and the\nproperties of individual vertices or edges within such\ngraphs to consideration of large-scale statistical proper-\ntiesofgraphs. Thisnewapproachhasbeendrivenlargely\nby the availabilityof computers and communication net-\nworks that allow us to gather and analyze data on a\nscale far larger than previously possible. Where stud-\nies used to look at networks of maybe tens or in extreme\ncaseshundredsofvertices,itisnotuncommonnowtosee\nnetworks with millions or even billions of vertices. This\nchange of scale forces upon us a corresponding change in\nedge\nvertex\nFIG. 1 A small example network with eight vertices and ten\nedges.\nouranalyticapproach. Manyofthequestionsthatmight\npreviously have been asked in studies of small networks\nare simply not useful in much larger networks. A social\nnetworkanalystmight haveasked, Which vertexin this\nnetwork would prove most crucial to the networks con-\nnectivity if it were removed? But such a question has\nlittle meaning in most networks of a million verticesno\nsinglevertexinsuchanetworkwillhavemucheectatall\nwhen removed. On the other hand, onecould reasonably\naska question like, What percentageof vertices need to\nbe removed to substantially aect network connectivity\nin some given way? and this type of statistical question\nhas real meaning even in a very large network.\nHowever, there is another reason why our approach\nto the study of networks has changed in recent years, a\nreason whose importance should not be underestimated,\nalthough it often is. For networks of tens or hundreds\nof vertices, it is a relatively straightforward matter to\ndrawapictureofthenetworkwithactualpointsandlines\n(Fig. 2) and to answer specic questions about network\nstructurebyexaminingthispicture. Thishasbeenoneof\nthe primary methods of network analysts since the eld\nbegan. The human eye is an analytic tool of remarkable\npower,andeyeballingpicturesofnetworksis anexcellent\nway to gain an understanding of their structure. With\na network of a million or a billion vertices however, this\napproach is useless. One simply cannot draw a mean-\ningful picture of a million vertices, even with modern 3D\ncomputer rendering tools, and therefore direct analysis\nby eye is hopeless. The recent development of statistical\nmethods for quantifying large networks is to a large ex-\ntentanattempttondsomethingtoplaythepartplayed\nby the eye in the network analysis of the twentieth cen-\ntury. Statistical methods answerthe question, How can\nI tell what this network looks like, when I cant actually\nlook at it?\nThe body of theory that is the primary focus of this\nreview aims to do three things. First, it aims to nd sta-\ntisticalproperties,suchaspathlengthsanddegreedistri-\nbutions, that characterize the structure and behavior of\nnetworked systems, and to suggest appropriate ways to\nmeasuretheseproperties. Second, itaims to createmod-\nels of networksthatcan helpus to understand the mean-\ningofthesepropertieshowtheycametobeastheyare,\nand how they interact with one another. Third, it aims\nto predict what the behavior of networked systems will\nbeonthebasisofmeasuredstructuralpropertiesandthe\nlocal rules governing individual vertices. How for exam-\nplewillnetworkstructureaecttracontheInternet,or\ntheperformanceofaWebsearchengine, orthedynamics\nof social or biological systems? As we will see, the scien-\ntic community has, by drawing on ideas from a broad\nvarietyof disciplines, made an excellent start on the rst\ntwo of these aims, the characterization and modeling of\nnetwork structure. Studies of the eects of structure on\nsystem behavior on the other hand are still in their in-\nfancy. It remains to be seen what the crucial theoretical\ndevelopments will be in this area.I Introduction 3\nFIG. 2 Three examples of the kinds of networks that are the topic of this review. (a) A food web of predator\0prey interactio ns\nbetween species in a freshwater lake [272]. Picture courtesy of Neo Martinez and Richard Williams. (b) The network of\ncollaborations between scientists at a private research institution [171]. (c) A network of sexual contacts between individuals\nin the study by Potterat et al. [342].\nA. Types of networks\nA set of vertices joined by edges is only the simplest\ntype of network; there are many ways in which networks\nmay be more complex than this (Fig. 3). For instance,\nthere may be more than one dierent type of vertex in a\nnetwork, or more than one dierent type of edge. And\nvertices or edges may have a variety of properties, nu-\nmerical'),
(300, 551, 'A Survey on Sensor Networks', '\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'),
(301, 552, 'Network Information Flow', '1204 IEEE TRANSACTIONS ON INFORMATION THEORY , VOL. 46, NO. 4, JULY 2000\nNetwork Information Flow\nRudolf Ahlswede, Ning Cai, Shuo-Yen Robert Li, Senior Member, IEEE, and\nRaymond W. Yeung, Senior Member, IEEE\nAbstractWe introduce a new class of problems called network\ninformation flow which is inspired by computer network applica-\ntions. Consider a point-to-point communication network on which\na number of information sources are to be mulitcast to certain sets\nof destinations. We assume that the information sources are mu-\ntually independent. The problem is to characterize the admissible\ncoding rate region. This model subsumes all previously studied\nmodels along the same line. In this paper, we study the problem\nwith one information source, and we have obtained a simple char-\nacterization of the admissible coding rate region. Our result can be\nregarded as the Max-flow Min-cut Theorem for network informa-\ntion flow. Contrary to ones intuition, our work reveals that it is in\ngeneral not optimal to regard the information to be multicast as a\nfluid which can simply be routed or replicated. Rather, by em-\nploying coding at the nodes, which we refer to as network coding,\nbandwidth can in general be saved. This finding may have signifi-\ncant impact on future design of switching systems.\nIndex TermsDiversity coding, multicast, network coding,\nswitching, multiterminal source coding.\nI. INTRODUCTION\nL\nET be the set of nodes of a point-to-point communica-\ntion network. Such a network is represented by a directed\ngraph , where is the set of edges, such that in-\nformation can be sent noiselessly from node to node for all\n. An example of this type of networks is the Internet\nbackbone, where with proper data link protocols information\ncan be sent between nodes essentially free of noise.\nLet be mutually independent information\nsources. The information rate (in bits per unit time) of is\ndenoted by , and let . Let\nand be arbitrary mappings. The source\nis generated at node , and it is multicast to node for\nall . The mappings and the vector specify a set\nof multicast requirements.\nIn this model, the graph may represent a physical network,\nwhile the set of multicast requirements may represent the aggre-\nManuscript received February 25, 1998; revised March 6, 2000. This work\nwas supported in part under Grants CUHK95E/480 and CUHK332/96E from\nthe Research Grant Council of the Hong Kong Special Administrative Region,\nChina. The material in this paper was presented in part at the IEEE International\nSymposium on Information Theory, MIT, Cambridge, MA, August 1621, 1998\nR. Ahlswede is with Fakultt fr Mathematik, Universitt Bielefeld, 33501\nBielefeld, Germany (e-mail: hollmann@Mathematik.uni-Bielefeld.de).\nN. Cai was with Fakultt fr Mathematik, Universitt Bielefeld, 33501\nBielefeld, Germany. He is now with the Department of Information Engi-\nneering, The Chinese University of Hong Kong, N.T., Hong Kong (e-mail:\nncai@ie.cuhk.edu.hk).\nS.-Y . R. Li and R. W. Yeung are with the Department of Information En-\ngineering, The Chinese University of Hong Kong, N.T., Hong Kong (e-mail:\nsyli@ie.cuhk.hk; whyeung@ie.cuhk.hk).\nCommunicated by R. L. Cruz, Associate Editor for Communication Net-\nworks.\nPublisher Item Identifier S 0018-9448(00)05297-4.\ngated traffic pattern the network needs to support. In other sit-\nuations, the graph may represent a subnetwork in a physical\nnetwork, while the set of multicast requirements may pertain to\na specific application on this subnetwork, e.g., a video-confer-\nence call.\nIn existing computer networks, each node functions as a\nswitch in the sense that it either relays information from an\ninput link to an output link, or it replicates information received\nfrom an input link and sends it to a certain set of output links.\nFrom the information-theoretic point of view, there is no\nreason to restrict the function of a node to that of a switch.\nRather, a node can function as an encoder in the sense that\nit receives information from all the input links, encodes, and\nsends information to all the output links. From this point of\nview, a switch is a special case of an encoder. In the sequel, we\nwill refer to coding at a node in a network as network coding.\nLet be a nonnegative real number associated with the\nedge , and let . For a fixed set of mul-\nticast requirements, a vector is admissible if and only if there\nexists a coding scheme satisfying the set of multicast require-\nments such that the coding rate from node to node (i.e., the\naverage number of bits sent from node to node per unit time)\nis less than or equal to for all . (At this point we\nleave the details of a coding scheme open because it is extremely\ndifficult to define the most general form of a coding scheme. A\nclass of coding schemes called -codes will be studied in Sec-\ntion III.) In graph theory, is called the capacity of the edge\n. Our goal is to characterize the admissible coding rate re-\ngion , i.e., the set of all admissible , for any graph and\nmulticast requirements and .\nThe model we have described includes both multilevel diver-\nsity coding (without distortion) [12], [8], [13] and distributed\nsource coding [14] as special cases. As an illustration, let us\nshow how the multilevel diversity coding system in Fig. 1 can\nbe formulated as a special case of our model. In this system,\nthere are two sources, and . Decoder 1 reconstructs\nonly, while all other decoders reconstruct both and . Let\nbe the coding rate of Encoder , . In our model, the\nsystem is represented by the graph in Fig. 2. In this graph,\nnode 1 represents the source, nodes 2, 3, and 4 represent the in-\nputs of Encoders 1, 2, and 3, respectively, nodes 5, 6, and 7 rep-\nresent the outputs of Encoders 1, 2, and 3, respectively, while\nnodes 8, 9, 10, and 11 represent the inputs of Decoders 1, 2, 3,\nand 4, respectively. The mappings and are specified as\nand\n00189448/00$10.00  2000 IEEEAHLSWEDE et al.: NETWORK INFORMATION FLOW 1205\nFig. 1. A multilevel diversity coding system.\nFig. 2. The graph representing the coding system in Fig. 1.\nand represents the information rates of and .\nNow all the edges in except for corre-\nspond to straight connections in Fig. 1, so there is no constraint\non the coding rate in these edges. Therefore, in order to deter-\nmining , the set of all admissible for the graph (with the\nset of multicast requirements specified by and , we set\nfor all edges in except for to\nobtain the admissible coding rate region of the problem in Fig. 1.\nA major finding in this paper is that, contrary to ones in-\ntuition, it is in general not optimal to consider the information\nto be multicast in a network as a fluid which can simply be\nrouted or replicated at the intermediate nodes. Rather, network\ncoding has to be employed to achieve optimality. This fact is il-\nlustrated by examples in the next section.\nIn the rest of the paper, we focus our discussion on problems\nwith , which we collectively refer to as the single-source\nproblem. For problems with , we refer to them collec-\ntively as the multisource problem. The rest of the paper is orga-\nnized as follows. In Section II, we propose a Max-flow Min-cut\ntheorem which characterizes the admissible coding rate region\nof the single-source problem. In Section III, we formally state\nthe main result in this paper. The proof is presented in Sections\nIV and V . In Section VI, we show that very simple optimal codes\ndo exist for certain networks. In Section VII, we use our results\nfor the single-source problem to solve a special case of the mul-\ntisource problem which has application in video conferencing.\nIn this section, we also show that the multisource problem is\nextremely difficult in general. Concluding remarks are in Sec-\ntion VIII.\nII. A MAX-FLOW MIN-CUT THEOREM\nIn this section, we propose a theorem which characterizes the\nadmissible coding rate region for the single-source problem. For\nthis problem, we let , and .In\nFig. 3. A single-level diversity coding system.\nFig. 4. The graph representing the coding system in Fig. 3.\nother words, the information source is generated at node\nand is multicast to nodes . We will call the source\nand the sinks of the graph . For a specific , the\nproblem will be referred to as the one-source -sink problem.\nLet us first define some notations and terminology which will\nbe used in the rest of the paper. Let be a graph with\nsource and sinks . The capacity of an edge\nis given by , and let . The subgraph of\nfrom to , , refers to the graph ,\nwhere\nis on a directed path from to\nis a flow in from to if for all\nsuch that for all except for and\ni.e., the total flow into node is equal to the total flow out of\nnode . is referred to as the value of in the edge .\nThe value of is defined as\nwhich is equal to\nis a max-flow from to in if is a flow from to\nwhose value is greater than or equal to any other flow from to1206 IEEE TRANSACTIONS ON INFORMATION THEORY , VOL. 46, NO. 4, JULY 2000\n(a) (b) (c)\nFig. 5. A one-source one-sink network.\n. Evidently, a max-flow from to in is also a max-flow\nfrom to in . For a graph with one source and one sink\n(for example, the graph ), the value of a max-flow from the\nsource to the sink is called the capacity of the graph.\nWe begin our discussion by first reviewing a basic result of\ndiversity coding by considering the single-level diversity system\nin Fig. 3. In this system, is the only information source (with\nrate ), and it is reconstructed by all the decoders. Henceforth,\nwe will drop the subscripts of and when there is only\none information source. Let be the coding rate of encoder ,\nand let . In order for a decoder to reconstruct ,\nit is necessary that the sum of the coding rates of the encoders\naccessible by this decoder is at least . Thus the conditions\n(1)\n(2)\n(3)\n(4)\nare necessary for to be admissible. On the other hand, these\nconditions are seen to be sufficient by the work of Singleton [10]\n(also cf. [12]).\nWe now give a graph-theoretic interpretation of the above re-\nsult. The graph corresp');
INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(302, 553, 'Wireless Ad Hoc Networks', ' 1 \r\nThis is a draft version only. \r\n \r\nWireless Ad Hoc Networks \r\n \r\nZygmunt J. Haas, Jing Deng, Ben Liang, Panagiotis Papadimitratos, and S. Sajama \r\nCornell University \r\nSchool of Electrical and Computer Engineering \r\n323 Rhodes Hall \r\nIthaca, NY 14853 \r\nTel: (607) 255-3454, Fax: (607) 255-9072 \r\ne-mail: {haas, jing, liang, papadp, sajama}@ece.cornell.edu \r\nURL: http://www.ece.cornell.edu/~haas/wnl/html \r\n \r\n \r\nAbstract  \r\n \r\nA mobile ad hoc network is a relatively new term for an old technology - a network that does \r\nnot rely on pre-existing infrastructure. Roots of this technology could be traced back to the \r\nearly 1970s with the DARPA PRNet and the SURAN projects. The new twitch is the application \r\nof this technology in the non-military communication environments. Additionally, the research \r\ncommunity has also recently addressed some extended features of this technology, such as \r\nmulticasting and security. Also numerous new solutions to the old problems of routing and \r\nmedium access control have been proposed. This survey attempts to summarize the state-of-\r\nthe-art of the ad hoc networking technology in four areas: routing, medium access control, \r\nmulticasting, and security. Where possible, comparison between the proposed protocols is \r\nalso discussed. \r\n \r\nKeywords: ad hoc networks, MANET, MAC protocols for ad hoc network, routing protocols for \r\nad hoc networks, proactive routing protocols, reactive routing protocols, hybrid routing \r\nprotocols, multicasting for ad hoc networks, security for ad hoc networks,   \r\n \r\n1. Introduction1 \r\n \r\n1.1 The Notion of the Ad Hoc Networks \r\n \r\nA Mobile Ad Hoc Network (MANET) is a network architecture that can be rapidly deployed \r\nwithout relying on pre-existing fixed network infrastructure. The nodes in a MANET can \r\ndynamically join and leave the network, frequently, often without warning, and possibly without \r\ndisruption to other nodes communication.  Finally, the nodes in the network can be highly \r\nmobile, thus rapidly changing the node constellation and the presence or absence of links. \r\nExamples of the use of the MANETs are: \r\n tactical operation - for fast establishment of military communication during the \r\ndeployment of forces in unknown and hostile terrain; \r\n rescue missions - for communication in areas without adequate wireless coverage; \r\n national security - for communication in times of national crisis, where the existing \r\ncommunication infrastructure is non-operational due to a natural disaster or a global \r\nwar; \r\n                                                        \r\n1\r\n Perkins, Charles E., AD HOC NETWORKING, pp.221-225,  2001 Addison Wesley Longman, Inc. \r\nReprinted by permission of Pearson Education, Inc. \r\n 2 \r\n law enforcement - for fast establishment of communication infrastructure during law \r\nenforcement operations; \r\n commercial use - for setting up communication in exhibitions, conferences, or sales \r\npresentations; \r\n education - for operation of wall-free (virtual) classrooms; and \r\n sensor networks - for communication between intelligent sensors (e.g., MEMS2) \r\nmounted on mobile platforms. \r\n \r\nNodes in the MANET exhibit nomadic behavior by freely migrating within some area, \r\ndynamically creating and tearing down associations with other nodes. Groups of nodes that \r\nhave a common goal can create formations (clusters) and migrate together, similarly to military \r\nunits on missions or to guided tours on excursions. Nodes can communicate with each other at \r\nany time and without restrictions, except for connectivity limitations and subject to security \r\nprovisions. Examples of network nodes are pedestrians, soldiers, or unmanned robots.  \r\nExamples of mobile platforms on which the network nodes might reside are cars, trucks, \r\nbuses, tanks, trains, planes, helicopters or ships. \r\n \r\nMANETs are intended to provide a data network that is immediately deployable in arbitrary \r\ncommunication environments and is responsive to changes in network topology.  Because ad-\r\nhoc networks are intended to be deployable anywhere, existing infrastructure may not be \r\npresent.  The mobile nodes are thus likely to be the sole elements of the network.  Differing \r\nmobility patterns and radio propagation conditions that vary with time and position can result in \r\nintermittent and sporadic connectivity between adjacent nodes.  The result is a time-varying \r\nnetwork topology. \r\n \r\nMANETs are distinguished from other ad-hoc networks by rapidly changing network \r\ntopologies, influenced by the network size and node mobility.  Such networks typically have a \r\nlarge span and contain hundreds to thousands of nodes.  The MANET nodes exist on top of \r\ndiverse platforms that exhibit quite different mobility patterns.  Within a MANET, there can be \r\nsignificant variations in nodal speed (from stationary nodes to high-speed aircraft), direction of \r\nmovement, acceleration/deceleration or restrictions on paths (e.g., a car must drive on a road, \r\nbut a tank does not).  A pedestrian is restricted by built objects while airborne platforms can \r\nexist anywhere in some range of altitudes.  In spite of such volatility, the MANET is expected to \r\ndeliver diverse traffic types, ranging from pure voice to integrated voice and image, and even \r\npossibly some limited video. \r\n \r\n \r\n1.2. The Communication Environment and the MANET Model \r\n \r\nThe following are a number of assumptions about the communication parameters, the network \r\narchitecture, and the network traffic in a MANET.  \r\n \r\n Nodes are equipped with portable communication devices. Lightweight batteries may \r\npower these devices.  Limited battery life can impose restrictions on the transmission \r\nrange, communication activity  (both transmitting and receiving) and computational power \r\nof these devices. \r\n Connectivity between nodes is not a transitive relation; i.e., if node A can communicate \r\ndirectly with node B and node B can communicate directly with node C, then node A may \r\n                                                        \r\n2\r\n Micro-Electro-Mechanical-Systems \r\n 3 \r\nnot, necessarily, be able to communicate directly with node C. This leads to the hidden \r\nterminal problem [Tob75]. \r\n A hierarchy in the network routing and mobility management procedures could improve \r\nnetwork performance measures, such as the latency in locating a mobile. However, a \r\nphysical hierarchy may lead to areas of congestion and is very vulnerable to frequent \r\ntopological reconfigurations. \r\n We assume that nodes are identified by fixed IDs (based on IP [Pos81] addresses, for \r\nexample). \r\n All the network nodes have equal capabilities.  This means that all nodes are equipped with \r\nidentical communication devices and are capable of performing functions from a common \r\nset of networking services.  However, all nodes do not necessarily perform the same \r\nfunctions at the same time.  In particular, nodes may be assigned specific functions in the \r\nnetwork, and these roles may change over time. \r\n Although the network should allow communication between any two nodes, it is envisioned \r\nthat a large portion of the traffic will be between geographically close nodes. This \r\nassumption is clearly justified in a hierarchical organization. For example, it is much more \r\nlikely that communication will take place between two soldiers in the same unit, rather than \r\nbetween two soldiers in two different brigades. \r\n \r\nA MANET is a peer-to-peer network that allows direct communication between any two nodes, \r\nwhen adequate radio propagation conditions exist between these two nodes and subject to \r\ntransmission power limitations of the nodes. If there is no direct link between the source and \r\nthe destination nodes, multi-hop routing is used. In multi-hop routing, a packet is forwarded \r\nfrom one node to another, until it reaches the destination. Of course, appropriate routing \r\nprotocols are necessary to discover routes between the source and the destination, or even to \r\ndetermine the presence or absence of a path to the destination node. Because of the lack of \r\ncentral elements, distributed protocols have to be used. \r\n \r\nThe main challenges in the design and operation of the MANETs, compared to more traditional \r\nwireless networks, stem from the lack of a centralized entity, the potential for rapid node \r\nmovement, and the fact that all communication is carried over the wireless medium.  In \r\nstandard cellular wireless networks, there are a number of centralized entities (e.g., the base-\r\nstations, the Mobile Switching Centers (MSCs), the Home Location Register (HLR), and the \r\nVisitor Location Register (VLR)). In ad-hoc networks, there is no preexisting infrastructure, and \r\nthese centralized entities do not exist. The centralized entities in the cellular networks perform \r\nthe function of coordination. The lack of these entities in the MANETs requires distributed \r\nalgorithms to perform these functions. In particular, the traditional algorithms for mobility \r\nmanagement, which rely on a centralized HLR/VLR, and the medium access control schemes, \r\nwhich rely on the base-station/MSC support, become inappropriate. \r\n \r\nAll communications between all network entities in ad-hoc networks are carried over the \r\nwireless medium. Due to the radio communications being vulnerable to propagation \r\nimpairments, connectivity between network nodes is not guaranteed. In fact, intermittent and \r\nsporadic connectivity may be quite common. Additionally, as the wireless bandwidth is limited, \r\nits use should be minimized. Finally, as some of the mobile devices are expected to be hand-\r\nheld with limited power sources, the required transmission power should be minimized as well. \r\nTherefore, the transmission radius of each mobile is limited, and channels assigned to mobiles \r\nare typically spatially reused. Consequently, since the transmission radius is much smaller \r\nthan the network span, communication between two nodes of'),
(303, 554, 'Support-Vector Networks', 'SUPPOR TECTOR NETW ORKS Corinna Cortes and Vladimir V apnik A T Labsesearc h USA Abstract The supp orte ctor network is a new learning mac hine for t w oroup classiation problems The mac hine conceptually implemen ts the follo wing idea input v ectors are noninearly mapp ed to a v ery highimension feature space In this feature space a linear decision surface is constructed Sp ecial prop erties of the decision surface ensures high generalization abilit y of the learning mac hine The idea b ehind the supp ort v ector net w ork w as previously implemen ted for the restricted case where the training data can b e separated without errors W e here extend this result to noneparable training data High generalization abilit y of supp ort ector net w orks utilizing p olynomial input transformations is demonstrated W e also compare the p erformance of the supp ort v ector net w ork to v arious classical learning algorithms that all to ok part in a b enc hmark study of Optical Character Recognition Keyw ords P attern recognition eien t learning algorithms neural net w orks ra dial basis function classirs p olynomial classirs In tro duction More than y ears ago R A Fisher suggested the st algorithm for pattern recog nition He considered a mo del of t w o normal distributed p opulations N m and N m of n dimensional v ectors x with mean v ectors m and m and co ariance matrices and and sho w ed that the optimal a y esian solution is a quadratic decision function F sq x sign x m T x m x m T x m ln j j j j In the case where the quadratic decision function degenerates to a linear function F lin x sign m m T x m T m m T m T o estimate the quadratic decision function one has to determine n n free parameters T o estimate the linear function only n free parameters ha v e to b e determined In the Da ytime phone Eail corinnaesearc httom Da ytime phone Eail vladesearc httomoutput from the 4 hidden units\nweights of the 4 hidden units\ndot-products\nweights of the 5 hidden units\ndot-products\ndot-product\nperceptron output \na  \n1\n, ... ,a  \n1\ninput vector, x\n5\n5\nweights of the output unit,\nz  , ... , z output from the 5 hidden units:\nFigure A simple feedorw ard p erceptron with input units la y ers of hidden units and output unit The gra yhading of the v ector en tries rects their n umeric v alue case where the n um b er of observ ations is small a y less than n estimating o n parameters is not reliable Fisher therefore recommended ev en in the case of to use the linear discriminator function with of the form where is some constan t Fisher also recommended a linear decision function for the case where the t w o distributions are not normal Algorithms for pattern recognition w ere therefore from the v ery b eginning asso ciated with the construction of linear decision surfaces In Rosen blatt explored a diren t kind of learning mac hines p erceptrons or neural net w orks The p erceptron consists of connected neurons where eac h neu ron implemen ts a separating h yp erplane so the p erceptron as a whole implemen ts a piecewise linear separating surface See Figure The problem of ding an algorithm that minimizes the error on a set of v ectors b y adjusting all the w eigh ts of the net w ork w as not found in Rosen blatt time and Rosen blatt suggested a sc heme where only the w eigh ts of the output unit are adaptiv e According to the ed setting of the other w eigh ts the input v ectors are noninearly transformed in to the feature space Z of the last la y er of units In this space a linear decision function is constructed I x sign X i i z i x b y adjusting the w eigh ts i from the i h hidden unit to the output unit so as to mini mize some error measure o v er the training data As a result of Rosen blatt approac h The optimal co eien t for w as found in the sixtiesconstruction of decision rules w as again asso ciated with the construction of linear h y p erplanes in some space An algorithm that allo ws for all w eigh ts of the neural net w ork to adapt in order lo cally to minimize the error on a set of v ectors b elonging to a pattern recognition problem w as found in when the bac kropagation algorithm w as disco v ered The solution in v olv es a sligh t mo diation of the mathematical mo del of neurons Therefore neural net w orks implemen t ieceise linear yp e decision func tions In this article w e construct a new t yp e of learning mac hines the soalled supp ort v ector net w ork The supp ort ector net w ork implemen ts the follo wing idea it maps the input v ectors in to some high dimensional feature space Z through some noninear mapping c hosen a priori In this space a linear decision surface is constructed with sp ecial prop erties that ensure high generalization abilit y of the net w ork Example T o obtain a decision surface corresp onding to a p olynomial of degree t w o one can create a feature space Z whic h has N n n co ordinates of the form z x z n x n n co ordinates z n x z n x n n co ordinates z n x x z N x n x n n n co ordinates where x x x n The h yp erplane is then constructed in this space Tw o problems arise in the ab o v e approac h one conceptual and one tec hnical The conceptual problem is ho w to d a separating h yp erplane that will generalize w ell the dimensionalit y of the feature space will b e h uge and not all h yp erplanes that separate the training data will necessarily generalize w ell The tec hnical problem is ho w com putationally to treat suc h highimensional spaces to construct p olynomial of degree or in a dimensional space it ma y b e necessary to construct h yp erplanes in a billion dimensional feature space The conceptual part of this problem w as solv ed in for the case of optimal hyp erplanes for separable classes An optimal h yp erplane is here deed as the linear decision function with maximal margin b et w een the v ectors of the t w o classes see Figure It w as observ ed that to construct suc h optimal h yp erplanes one only has to tak e in to accoun t a small amoun t of the training data the so called supp ort ve ctors whic h determine this margin It w as sho wn that if the training v ectors are separated without errors b y an optimal h yp erplane the exp ectation v alue of the probabilit y of committing an error on a test example is b ounded b y the ratio b et w een the exp ectation Recall Fisher concerns ab out small amoun ts of data and the quadratic discriminan t functionoptimal margin\noptimal hyperplane\nFigure An example of a separable problem in a dimensional space The supp ort v ectors mark ed with grey squares dee the margin of largest separation b et w een the t w o classes v alue of the n um b er of supp ort v ectors and the n um b er of training v ectors E r rror E um b er of supp ort v ectors n um b er of training v ectors Note that this b ound do es not explicitly con tain the dimensionalit y of the space of separation It follo ws from this b ound that if the optimal h yp erplane can b e constructed from a small n um b er of supp ort v ectors relativ e to the training set size the generalization abilit y will b e high ev en in an inite dimensional space In Section w e will demonstrate that the ratio for a real life problems can b e as lo w as and the optimal h yp erplane generalizes w ell in a billion dimensional feature space Let w z b b e the optimal h yp erplane in feature space W e will sho w that the w eigh ts w for the optimal h yp erplane in the feature space can b e written as some linear com bination of supp ort v ectors w X supp ort v ectors i z i The linear decision function I z in the feature space will accordingly b e of the form I z sign X supp ort v ectors i z i z b A where z i z is the dotro duct b et w een supp ort v ectors z i and v ector z in feature space The decision function can therefore b e describ ed as a t w o la y er net w ork Figurenon-linear transformation\n1\nw\ni\nw\nj\nw\nN\nw\nz\ni\nsupport vectors\nin feature space\ninput vector in feature space\nx input vector,\nclassification\nFigure Classiation b y a supp ort ector net w ork of an unkno wn pattern is concep tually done b y st transforming the pattern in to some highimensional feature space An optimal h yp erplane constructed in this feature space determines the output The similarit y to a t w oa y er p erceptron can b e seen b y comparison to Figurea 1\na i\na j\nk\na s\nu\nj\nu\ni\nu\n1\nu\ns\nk\nu = K(     ,   )\nx\nx\nx x\ninput vector,\nvectors,\nsupport\nLagrange multipliers\nclassification\nk\ncomparison\nFigure Classiation of an unkno wn pattern b y a supp ort ector net w ork The pattern is in input space compared to supp ort v ectors The resulting v alues are non linearly transformed A linear function of these transformed v alues determine the output of the classir Ho w ev er ev en if the optimal h yp erplane generalizes w ell the tec hnical problem of ho w to treat the high dimensional feature space remains In it w as sho wn that the order of op erations for constructing a decision function can b e in terc hanged instead of making a noninear transformation of the input v ectors follo w ed b y dotro ducts with supp ort v ectors in feature space one can st compare t w o v ectors in input space y e taking their dotro duct or some distance measure and then mak e a noninear transformation of the v alue of the result See Figure This enables for the construction of ric h classes of decision surfaces for example p olynomial decision surfaces of arbitrarily degree W e will call this t yp e of learning mac hine for supp ort ectors net w ork The tec hnique of supp ort ector net w orks w as st dev elop ed for the restricted case of separating training data without errors In this article w e extend the approac h of supp ort ector net w orks to co v er when separation without error on the training v ectors With this name w e emphasize ho w crucial the idea of expanding the solution on supp ort v ectors is for these learning mac hines In the supp ort ectors learning algorithm e the complexit y of the construction do es not dep end on the dimensiona'),
(304, 555, 'Statistical Mechanics Of Complex Networks', 'STATISTICAL MECHANICS OF COMPLEX NETWORKS\nA Dissertation\nSubmitted to the Graduate School\nof the University of Notre Dame\nin Partial Ful\0llment of the Requirements\nfor the Degree of\nDoctor of Philosophy\nby\nR\0eka Zsuzs\0anna Albert, B. S., M. S.\nAlbert-L\0aszl\0o Barab\0asi, Director\nDepartment of Physics\nNotre Dame, Indiana\nApril 2001STATISTICAL MECHANICS OF COMPLEX NETWORKS\nAbstract\nby\nR\0eka Zsuzs\0anna Albert\nThe emergence of order in natural systems is a constant source of inspiration\nfor both physical and biological sciences. While the spatial order characterizing for\nexample the crystals has been the basis of many advances in contemporary physics,\nmost complex systems in nature do not o\0er such high degree of order. Many of\nthese systems form complex networks whose nodes are the elements of the system\nand edges represent the interactions between them.\nTraditionally complex networks have been described by the random graph the-\nory founded in 1959 by Paul Erd} os and Alfr\0ed R\0enyi. One of the de\0ning features\nof random graphs is that they are statistically homogeneous, and their degree dis-\ntribution (characterizing the spread in the number of edges starting from a node) is\na Poisson distribution. In contrast, recent empirical studies, including the work of\nour group, indicate that the topology of real networks is much richer than that of\nrandom graphs. In particular, the degree distribution of real networks is a power-\nlaw, indicating a heterogeneous topology in which the majority of the nodes have a\nsmall degree, but there is a signi\0cant fraction of highly connected nodes that play\nan important role in the connectivity of the network.\nThe scale-free topology of real networks has very important consequences on\ntheir functioning. For example, we have discovered that scale-free networks are\nextremely resilient to the random disruption of their nodes. On the other hand, theR\0eka Zsuzs\0anna Albert\nselective removal of the nodes with highest degree induces a rapid breakdown of the\nnetwork to isolated subparts that cannot communicate with each other.\nThe non-trivial scaling of the degree distribution of real networks is also an\nindication of their assembly and evolution. Indeed, our modeling studies have shown\nus that there are general principles governing the evolution of networks. Most\nnetworks start from a small seed and grow by the addition of new nodes which attach\nto the nodes already in the system. This process obeys preferential attachment: the\nnew nodes are more likely to connect to nodes with already high degree. We have\nproposed a simple model based on these two principles wich was able to reproduce\nthe power-law degree distribution of real networks. Perhaps even more importantly,\nthis model paved the way to a new paradigm of network modeling, trying to capture\nthe evolution of networks, not just their static topology.CONTENTS\nTABLES ........ ........... ........... ........ v\nFIGURES ........ ........... ........... ........ vi\nACKNOWLEDGEMENTS ......... ........... ........ xiv\nCHAPTER1:INTRODUCTION ...... ........... ........ 1\n1.1 Smallworlds.. ........... ........... ........ 3\n1.2 Clustering ... ........... ........... ........ 4\n1.3 Degreedistribution ......... ........... ........ 4\nCHAPTER 2: THE TOPOLOGY OF REAL NETWORKS: EMPIRICAL\nRESULTS ...... ........... ........... ........ 7\n2.1 World-WideWeb .......... ........... ........ 7\n2.2 Internet .... ........... ........... ........ 12\n2.3 Actorcollaborationnetwork .... ........... ........ 14\n2.4 Collaborationbetweenscientists .. ........... ........ 15\n2.5 Metabolicnetworks ......... ........... ........ 15\n2.6 Ecologicalnetworks ......... ........... ........ 18\n2.7 Citationnetworks .......... ........... ........ 18\n2.8 Phone-callnetwork ......... ........... ........ 19\n2.9 Otherdatabases........... ........... ........ 19\nCHAPTER3:RANDOMGRAPHTHEORY .......... ........ 23\n3.1 The Erd} os-R\0enyimodel ...... ........... ........ 24\n3.2 Subgraphs . . . ........... ........... ........ 28\n3.3 Graphevolution........... ........... ........ 31\n3.4 Degreedistribution ......... ........... ........ 33\n3.5 Connectednessanddiameter.... ........... ........ 36\n3.6 Clusteringcoe\0cient ........ ........... ........ 38\niiCHAPTER4:PERCOLATIONTHEORY . ........... ........ 41\n4.1 Quantitiesofinterestinpercolationtheory ....... ........ 41\n4.2 Generalresults ........... ........... ........ 44\n4.2.1 The subcritical phase (p<p\nc\n) .......... ........ 44\n4.2.2 The supercritical phase (p>p\nc\n) ......... ........ 45\n4.3 Exactsolutions: percolationonaCayleytree...... ........ 46\n4.3.1 Percolationthreshold .... ........... ........ 47\n4.3.2 Percolation probability . . . ........... ........ 48\n4.3.3 Meanclustersize ...... ........... ........ 49\n4.3.4 Clustersizedistribution . . ........... ........ 50\n4.4 Scalinginthecriticalregion .... ........... ........ 50\n4.5 Clusterstructure .......... ........... ........ 52\n4.6 In\0nite dimensional percolation . . ........... ........ 53\n4.7 Parallelsbetweenrandomgraphtheoryandpercolation ........ 54\nCHAPTER5:GENERALIZEDRANDOMGRAPHS...... ........ 58\n5.1 Thresholdsinascale-freerandomgraph ........ ........ 60\n5.2 Generatingfunctionformalism... ........... ........ 61\n5.2.1 Componentsizesandphasetransitions ..... ........ 62\n5.2.2 Averagepathlength .... ........... ........ 65\n5.3 Randomgraphswithpower-lawdegreedistribution . . ........ 66\n5.4 Bipartitegraphsandtheclusteringcoe\0cient ..... ........ 69\nCHAPTER6:SMALL-WORLDNETWORKS ......... ........ 72\n6.1 The Watts-Strogatz (WS) model . ........... ........ 73\n6.2 Propertiesofsmall-worldnetworks ........... ........ 74\n6.2.1 Averagepathlength .... ........... ........ 76\n6.2.2 Clusteringcoe\0cient .... ........... ........ 80\n6.2.3 Degreedistribution ..... ........... ........ 81\nCHAPTER 7: DYNAMIC MODELING OF SCALE-FREE NETWORKS . . . 84\n7.1 Thescale-freemodel ........ ........... ........ 86\n7.2 Continuumtheory ......... ........... ........ 88\n7.3 Limitingcasesofthescale-freemodel .......... ........ 89\n7.3.1 ModelA ........... ........... ........ 91\n7.3.2 ModelB ........... ........... ........ 93\n7.4 Propertiesofthescale-freemodel . ........... ........ 94\n7.4.1 Averagepathlength .... ........... ........ 95\n7.4.2 Nodedegreecorrelations . . ........... ........ 97\n7.4.3 Clusteringcoe\0cient .... ........... ........ 99\nCHAPTER8:EVOLVINGNETWORKS . ........... ........101\n8.1 Preferentialattachment....... ........... ........102\n8.1.1 Measuring \0( k)forrealnetworks ........ ........102\niii8.1.2 Nonlinearpreferentialattachment ........ ........103\n8.1.3 Initial attractiveness .... ........... ........106\n8.2 Growth .... ........... ........... ........107\n8.2.1 Empiricalresults ...... ........... ........108\n8.2.2 Analyticalresults ...... ........... ........108\n8.3 Localevents .. ........... ........... ........110\n8.3.1 Internaledgesandrewiring ........... ........110\n8.3.2 Internaledgesandedgeremoval ......... ........113\n8.4 Growthconstraints ......... ........... ........117\n8.4.1 Agingandcost ....... ........... ........117\n8.4.2 Gradualaging ........ ........... ........118\n8.5 Competitioninevolvingnetworks . ........... ........120\n8.5.1 Fitnessmodel ........ ........... ........120\n8.5.2 Edgeinheritance ...... ........... ........123\nCHAPTER 9: ERROR AND ATTACK TOLERANCE OF NETWORKS . . . 125\n9.1 Numericalresults .......... ........... ........126\n9.1.1 Randomnetwork,randomnoderemoval .... ........127\n9.1.2 Scale-freenetwork,randomnoderemoval .... ........129\n9.1.3 Intentionalattack ..... ........... ........129\n9.2 Errortolerance: analyticalresults . ........... ........130\n9.2.1 Randomgraphs ....... ........... ........132\n9.2.2 Scale-freenetworks ..... ........... ........133\n9.3 Attacktolerance: analyticalresults ........... ........134\n9.4 Generatingfunctionapproach ... ........... ........137\n9.5 Therobustnessofrealnetworks . . ........... ........141\n9.5.1 Communicationnetworks.. ........... ........141\n9.5.2 Cellularnetworks ...... ........... ........144\n9.5.3 Ecologicalnetworks ..... ........... ........145\nivTABLES\n2.1 Average path length and clustering coe\0cient for several real net-\nworks. For each network we indicated the number of nodes, the av-\nerage degreehki, the average path length  and clustering coe\0cient\nC. For a comparison we have included the average path length \nrand\nand clustering coe\0cient C\nrand\nof a random graph with the same size\nandaveragedegree. ......... ........... ........ 20\n2.2 Exponents of the degree distribution of several real networks. We\nalso indicate the size of the network and its average degreehki.The\nnetworks with undirected edges are marked with a star. ........ 21\n5.1 Comparison between the average path length of real networks with\npower-law degree distribution and the prediction of random graph\ntheory (3.19) and that of Newman, Strogatz and Watts (5.27). I am\nalso indicating the size of the networks, their average degree and the\ncuto\0 length \0 as extracted from the data. One can see that while\nEq. (5.27) predicts a smaller average path length for these networks\nthan Eq. (3.19), in most cases the actual average path length is larger\nthanbothrandompredictions.... ........... ........ 69\n8.1 Evolving network models containing more general assumptions than\nthe scale-free model or additional mechanisms of network evolution.\nIn each case I listed the most important new concept or mechanism\nthat is considered in the model, and the interval in which the exponent\nofthedegreedistributioncanvary. .......... ........124\nvFIGURES\n2.1 The distribution of outgoing (a) and incoming edges on the nd.edu\ndomain containing 325; 729 documents and 1; 469; 680 directed edges.\nThe dotted lines in (a) and (b) represent the analytical \0ts we used\nas input distributions in constructing the topological model of the\nWWW, the tail of the distributions following P(k) \0 k\n\n,with\n\nout\n=2:45 and \nin\n=2:1. (c) Average path length  as a function\nof t'),
(305, 556, 'The capacity of wireless networks', '388 IEEE TRANSACTIONS ON INFORMATION THEORY , VOL. 46, NO. 2, MARCH 2000\nThe Capacity of Wireless Networks\nPiyush Gupta, Student Member, IEEE, and P. R. Kumar, Fellow, IEEE\nAbstractWhen identical randomly located nodes, each ca-\npable of transmitting at bits per second and using a fixed range,\nform a wireless network, the throughput obtainable by each\nnode for a randomly chosen destination is\nbits per\nsecond under a noninterference protocol.\nIf the nodes are optimally placed in a disk of unit area, traffic\npatterns are optimally assigned, and each transmissions range is\noptimally chosen, the bitdistance product that can be transported\nby the network per second is bit-meters per second.\nThus even under optimal circumstances, the throughput is only\n bits per second for each node for a destination nonva-\nnishingly far away.\nSimilar results also hold under an alternate physical model\nwhere a required signal-to-interference ratio is specified for\nsuccessful receptions.\nFundamentally, it is the need for every node all over the domain\nto share whatever portion of the channel it is utilizing with nodes\nin its local neighborhood that is the reason for the constriction in\ncapacity.\nSplitting the channel into several subchannels does not change\nany of the results.\nSome implications may be worth considering by designers. Since\nthe throughput furnished to each user diminishes to zero as the\nnumber of users is increased, perhaps networks connecting smaller\nnumbers of users, or featuring connections mostly with nearby\nneighbors, may be more likely to be find acceptance.\nIndex TermsAd hoc networks, capacity, multihop radio net-\nworks, throughput, wireless networks.\nI. INTRODUCTION\nW\nIRELESS networks consist of a number of nodes which\ncommunicate with each other over a wireless channel.\nSome wireless networks have a wired backbone with only the\nlast hop being wireless. Examples are cellular voice and data\nnetworks and mobile IP. In others, all links are wireless. One\nManuscript received December 3, 1998; revised July 1, 1999. This mate-\nrial is based on work supported in part by the Air Force Office of Scientific\nResearch under Contract AF-DC-5-36128, by EPRI and the U.S. Army Re-\nsearch Office under Subcontract to Cornell University Contracts WO8333-04\nand 35352-6086, by the U.S. Army Research Office under Contract DAAH\n04-95-1-0090, the Office of Naval Research under Contract N00014-99-1-0696,\nand the Joint Services Electronics Program under Contract N00014-96-1-0129.\nAny opinions, findings, and conclusions are those of the authors and do not nec-\nessarily reflect the views of the above agencies.\nP. Gupta is with the Department of Electrical and Computer Engineering, and\nthe Coordinated Science Laboratory, University of Illinois, Urbana, IL 61801\nUSA.\nP. R. Kumar is with the University of Illinois at Urbana-Champaign, Coordi-\nnated Science Laboratory, Urbana, IL 61801 USA.\nCommunicated by V . Anantharam, Associate Editor for Communication Net-\nworks.\nPublisher Item Identifier S 0018-9448(00)01358-4.\nexample of such networks is multihop radio networks or ad\nhoc networks. Another possibly futuristic example, see [1], may\nbe collections of smart homes where computers, microwave\novens, door locks, water sprinklers, and other information ap-\npliances are interconnected by a wireless network.\nIt is to these types of all wireless networks that this paper\nis addressed. Such networks consist of a group of nodes which\ncommunicate with each other over a wireless channel without\nany centralized control; see Fig. 1. Nodes may cooperate in\nrouting each others data packets. Lack of any centralized con-\ntrol and possible node mobility give rise to many issues at the\nnetwork, medium access, and physical layers, which have no\ncounterparts in the wired networks like Internet, or in cellular\nnetworks.\nAt the network layer, the main problem is that of routing,\nwhich is exacerbated by the time-varying network topology,\npower constraints, and the characteristics of the wireless\nchannel; see Ramanathan and Steenstrup [2] for an overview.\nThe choice of medium access scheme is also difficult in ad hoc\nnetworks due to the time-varying network topology and the lack\nof centralized control. Use of TDMA or dynamic assignment of\nfrequency bands is complex since there is no centralized control\nas in cellular networks, FDMA is inefficient in dense networks,\nCDMA is difficult to implement due to node mobility and\nthe consequent need to keep track of the frequency-hopping\npatterns and/or spreading codes for nodes in the time-varying\nneighborhood, and random access appears to be the current\nfavorite. The access problem when many nodes transmit to\nthe same receiver has been much studied in the literature ever\nsince the genesis of the ALOHA network, and bounds on the\nthroughput of successful collision-free transmissions as well\nas transmission protocols have been devised; see Gallager [3].\nSharing channels in networks does lead to some new problems\nassociated with hidden terminals and exposed terminals.\nThe protocols MACA and its extension MACAW, see Karn [4]\nand Bhargavan et al. [5] respectively, use a series of handshake\nsignals to resolve these problems to a certain extent. This has\nbeen standardized in the IEEE 802.11 protocol, see [6]. At the\nphysical layer, an important issue is that of power control. The\ntransmission power of nodes needs to be regulated so that it\nis high enough to reach the intended receiver while causing\nminimal interference at other nodes. Iterative power control\nalgorithms have been devised, see Bambos, Chen, and Pottie\n[7] and Ulukus and Yates [8].\nIn this paper we analyze the capacity of wireless networks.\nWe scale space and suppose that nodes are located in a region\nof area 1 m\n2\n. Each node can transmit at bits per second over\na common wireless channel. We shall see that it is immaterial\n00189448/00$10.00  2000 IEEEGUPTA AND KUMAR: THE CAPACITY OF WIRELESS NETWORKS 389\nFig. 1. An ad hoc wireless network.\nto our results\n1\nif the channel is broken up into several subchan-\nnels of capacity bits per second, as long as\nPackets are sent from node to node in a mul-\ntihop fashion until they reach their final destination. They can\nbe buffered at intermediate nodes while awaiting transmission.\nDue to spatial separation, several nodes can make wireless\ntransmissions simultaneously, provided there is no destructive\ninterference of a transmission by others. We will describe in\nthe sequel under what conditions a wireless transmission over\na subchannel is received successfully by its intended recipient.\nWe will consider two types of networks, Arbitrary Networks,\nwhere the node locations, destinations of sources, and traffic\ndemands, are all arbitrary, and Random Networks, where the\nnodes and their destinations are randomly chosen.\nA. Arbitrary Networks: Arbitrarily Located Nodes and Traffic\nPatterns\nIn the arbitrary setting we suppose that nodes are arbitrarily\nlocated in a disk of unit area in the plane. Each node has an arbi-\ntrarily chosen destination to which it wishes to send traffic at an\narbitrary rate; thus the traffic pattern is arbitrary. Each node can\nchoose an arbitrary range or power level for each transmission.\nWe need to describe when a transmission is received success-\nfully by its intended recipient. We will allow for two possible\nmodels for successful reception of a transmission over one hop,\ncalled the Protocol Model and the Physical Model, described\nbelow. Let denote the location of a node; we will also use\nto refer to the node itself.\n1) The Protocol Model: Suppose node transmits over the\nth subchannel to a node Then this transmission is success-\nfully received by node if\n(1)\nfor every other node simultaneously transmitting over the\nsame subchannel.\nThe quantity models situations where a guard zone\nis specified by the protocol to prevent a neighboring node from\n1\nWe are grateful to Kimberly King for asking us to be more explicit about the\nprospects for routing through multiple technologies.\ntransmitting on the same subchannel at the same time. It also\nallows for imprecision in the achieved range of transmissions.\nAnother model which is more related to physical layer con-\nsiderations is\n2) The Physical Model: Let be the subset of\nnodes simultaneously transmitting at some time instant over a\ncertain subchannel. Let be the power level chosen by node\nfor Then the transmission from a node , ,\nis successfully received by a node if\n(2)\nThis models a situation where a minimum signal-to-interference\nratio (SIR) of is necessary for successful receptions, the am-\nbient noise power level is , and signal power decays with dis-\ntance as We will suppose that , which is the usual\nmodel outside a small neighborhood of the transmitter.\n3) The Transport Capacity of Arbitrary Networks: Given\nany set of successful transmissions taking place over time and\nspace, let us say that the network transports one bit-meter when\none bit has been transported a distance of one meter toward\nits destination. (We do not give multiple credit for the same\nbit carried from one source to several different destinations as\nin the multicast or broadcast cases). This sum of products of\nbits and the distances over which they are carried is a valuable\nindicator of a networks transport capacity. (It should be noted\nthat when the area of the domain is square meters rather\nthan the normalized 1 m\n2\n, then all the transport capacity results\npresented below should be scaled by Our main results\nare the following. Recall Knuths notation:\ndenotes that as well as .\nMain Result 1.: The transport capacity of an Arbitrary Net-\nwork under the Protocol Model is bit-meters per\nsecond if the nodes are optimally placed, the traffic pattern is op-\ntimally chosen, and if the range of each transmission is chosen\noptimally.\nSpecifically, an upper bound is bit-meters per\nsecond for every Arbitrary Network for all spatial and temporal390 IEEE TRANSACTIONS ON INFORMATION THEORY , VOL. 46, NO. 2');
INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(306, 557, 'Resilient Overlay Networks', 'Resilient Overlay Networks\nDavid Andersen, Hari Balakrishnan, Frans Kaashoek, and Robert Morris\nMIT Laboratory for Computer Science\nron@nms.lcs.mit.edu\nhttp://nms.lcs.mit.edu/ron/\nAbstract\nA Resilient Overlay Network (RON) is an architecture that allows\ndistributed Internet applications to detect and recover from path\noutages and periods of degraded performance within several sec-\nonds, improving over todays wide-area routing protocols that take\nat least several minutes to recover. A RON is an application-layer\noverlay on top of the existing Internet routing substrate. The RON\nnodes monitor the functioning and quality of the Internet paths\namong themselves, and use this information to decide whether to\nroute packets directly over the Internet or by way of other RON\nnodes, optimizing application-specic routing metrics.\nResults from two sets of measurements of a working RON de-\nployed at sites scattered across the Internet demonstrate the benets\nof our architecture. For instance, over a 64-hour sampling period in\nMarch 2001 across a twelve-node RON, there were 32 signicant\noutages, each lasting over thirty minutes, over the 132 measured\npaths. RONs routing mechanism was able to detect, recover, and\nroute around all of them, in less than twenty seconds on average,\nshowing that its methods for fault detection and recovery work well\nat discovering alternate paths in the Internet. Furthermore, RON\nwas able to improve the loss rate, latency, or throughput perceived\nby data transfers; for example, about 5% of the transfers doubled\ntheir TCP throughput and 5% of our transfers saw their loss prob-\nability reduced by 0.05. We found that forwarding packets via at\nmost one intermediate RON node is sufcient to overcome faults\nand improve performance in most cases. These improvements, par-\nticularly in the area of fault detection and recovery, demonstrate the\nbenets of moving some of the control over routing into the hands\nof end-systems.\n1. Introduction\nThe Internet is organized as independently operating au-\ntonomous systems (ASs) that peer together. In this architecture,\ndetailed routing information is maintained only within a single AS\nThis research was sponsored by the Defense Advanced Research\nProjects Agency (DARPA) and the Space and Naval Warfare Sys-\ntems Center, San Diego, under contract N66001-00-1-8933.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for prot or commercial advantage and that copies\nbear this notice and the full citation on the rst page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior specic\npermission and/or a fee.\n18th ACM Symp. on Operating Systems Principles (SOSP) October 2001,\nBanff, Canada.\nCopyright 2001 ACM\nand its constituent networks, usually operated by some network ser-\nvice provider. The information shared with other providers and\nASs is heavily ltered and summarized using the Border Gateway\nProtocol (BGP-4) running at the border routers between ASs [21],\nwhich allows the Internet to scale to millions of networks.\nThis wide-area routing scalability comes at the cost of re-\nduced fault-tolerance of end-to-end communication between Inter-\nnet hosts. This cost arises because BGP hides many topological\ndetails in the interests of scalability and policy enforcement, has\nlittle information about trafc conditions, and damps routing up-\ndates when potential problems arise to prevent large-scale oscil-\nlations. As a result, BGPs fault recovery mechanisms sometimes\ntake many minutes before routes converge to a consistent form [12],\nand there are times when path outages even lead to signicant dis-\nruptions in communication lasting tens of minutes or more [3, 18,\n19]. The result is that todays Internet is vulnerable to router and\nlink faults, conguration errors, and malicehardly a week goes\nby without some serious problem affecting the connectivity pro-\nvided by one or more Internet Service Providers (ISPs) [15].\nResilient Overlay Networks (RONs) are a remedy for some of\nthese problems. Distributed applications layer a resilient overlay\nnetwork over the underlying Internet routing substrate. The nodes\ncomprising a RON reside in a variety of routing domains, and co-\noperate with each other to forward data on behalf of any pair of\ncommunicating nodes in the RON. Because ASs are independently\nadministrated and congured, and routing domains rarely share in-\nterior links, they generally fail independently of each other. As\na result, if the underlying topology has physical path redundancy,\nRON can often nd paths between its nodes, even when wide-area\nrouting Internet protocols like BGP-4 cannot.\nThe main goal of RON is to enable a group of nodes to commu-\nnicate with each other in the face of problems with the underlying\nInternet paths connecting them. RON detects problems by aggres-\nsively probing and monitoring the paths connecting its nodes. If\nthe underlying Internet path is the best one, that path is used and no\nother RON node is involved in the forwarding path. If the Internet\npath is not the best one, the RON will forward the packet by way of\nother RON nodes. In practice, we have found that RON can route\naround most failures by using only one intermediate hop.\nRON nodes exchange information about the quality of the paths\namong themselves via a routing protocol and build forwarding ta-\nbles based on a variety of path metrics, including latency, packet\nloss rate, and available throughput. Each RON node obtains the\npath metrics using a combination of active probing experiments\nand passive observations of on-going data transfers. In our imple-\nmentation, each RON is explicitly designed to be limited in size\nbetween two and fty nodesto facilitate aggressive path main-\ntenance via probing without excessive bandwidth overhead. ThisCCI\nAros\nUtah\nCMU\nTo vu.nl\nLulea.se\nMIT\nMA-Cable\nCisco\nCornell\nNYU\nNC-Cable\nOR-DSL\nCA-T1\nPDI\nMazu\nFigure 1: The current sixteen-node RON deployment. Five sites\nare at universities in the USA, two are European universities\n(not shown), three are broadband home Internet hosts con-\nnected by Cable or DSL, one is located at a US ISP, and ve are\nat corporations in the USA.\nallows RON to recover from problems in the underlying Internet in\nseveral seconds rather than several minutes.\nThe second goal of RON is to integrate routing and path selec-\ntion with distributed applications more tightly than is traditionally\ndone. This integration includes the ability to consult application-\nspecic metrics in selecting paths, and the ability to incorporate\napplication-specic notions of what network conditions constitute a\nfault. As a result, RONs can be used in a variety of ways. A mul-\ntimedia conferencing program may link directly against the RON\nlibrary, transparently forming an overlay between all participants\nin the conference, and using loss rates, delay jitter, or application-\nobserved throughput as metrics on which to choose paths. An ad-\nministrator may wish to use a RON-based router application to\nform an overlay network between multiple LANs as an Overlay\nVPN. This idea can be extended further to develop an Overlay\nISP, formed by linking (via RON) points of presence in different\ntraditional ISPs after buying bandwidth from them. Using RONs\nrouting machinery, an Overlay ISP can provide more resilient and\nfailure-resistant Internet service to its customers.\nThe third goal of RON is to provide a framework for the imple-\nmentation of expressive routing policies, which govern the choice\nof paths in the network. For example, RON facilitates classifying\npackets into categories that could implement notions of acceptable\nuse, or enforce forwarding rate controls.\nThis paper describes the design and implementation of RON,\nand presents several experiments that evaluate whether RON is a\ngood idea. To conduct this evaluation and demonstrate the ben-\nets of RON, we have deployed a working sixteen-node RON at\nsites sprinkled across the Internet (see Figure 1). The RON client\nwe experiment with is a resilient IP forwarder, which allows us to\ncompare connections between pairs of nodes running over a RON\nagainst running straight over the Internet.\nWe have collected a few weeks worth of experimental results of\npath outages and performance failures and present a detailed analy-\nsis of two separate datasets:\n with twelve nodes measured in\nMarch 2001 and\n with sixteen nodes measured in May 2001.\nIn both datasets, we found that RON was able to route around be-\ntween 60% and 100% of all signicant outages. Our implementa-\ntion takes 18 seconds, on average, to detect and route around a path\nfailure and is able to do so in the face of an active denial-of-service\nattack on a path. We also found that these benets of quick fault de-\ntection and successful recovery are realized on the public Internet\nand do not depend on the existence of non-commercial or private\nnetworks (such as the Internet2 backbone that interconnects many\neducational institutions); our ability to determine this was enabled\nby RONs policy routing feature that allows the expression and im-\nplementation of sophisticated policies that determine how paths are\nselected for packets.\nWe also found that RON successfully routed around performance\nfailures: in\n , the loss probability improved by at least 0.05\nin 5% of the samples, end-to-end communication latency reduced\nby 40ms in 11% of the samples, and TCP throughput doubled in\n5% of all samples. In addition, we found cases when RONs loss,\nlatency, and throughput-optimizing path selection mechanisms all\nchose different paths between the same two nodes, suggesting that\napplication-specic path selection techniques are likely to be use-\nful in practice. A noteworthy nding from the experiments and\nanalysis is that in most cases, forwarding packets via at most one\nintermediate RON node is sufcient both for recovering from fail-\nures a'),
(307, 558, 'An introduction to variational methods for graphical models', ''),
(308, 559, 'Learning in graphical models', 'Statistical Science\r\n2004, Vol. 19, No. 1, 140155\r\nDOI 10.1214/088342304000000026\r\n Institute of Mathematical Statistics, 2004\r\nGraphical Models\r\nMichael I. Jordan\r\nAbstract. Statistical applications in fields such as bioinformatics, informa-\r\ntion retrieval, speech processing, image processing and communications of-\r\nten involve large-scale models in which thousands or millions of random\r\nvariables are linked in complex ways. Graphical models provide a gen-\r\neral methodology for approaching these problems, and indeed many of the\r\nmodels developed by researchers in these applied fields are instances of the\r\ngeneral graphical model formalism. We review some of the basic ideas under-\r\nlying graphical models, including the algorithmic ideas that allow graphical\r\nmodels to be deployed in large-scale data analysis problems. We also present\r\nexamples of graphical models in bioinformatics, error-control coding and\r\nlanguage processing.\r\nKey words and phrases: Probabilistic graphical models, junction tree\r\nalgorithm, sum-product algorithm, Markov chain Monte Carlo, variational\r\ninference, bioinformatics, error-control coding.\r\n1. INTRODUCTION\r\nThe fields of statistics and computer science have\r\ngenerally followed separate paths for the past several\r\ndecades, each field providing useful services to the\r\nother, but with the core concerns of the two fields rarely\r\nappearing to intersect. In recent years, however, it has\r\nbecome increasingly evident that the long-term goals\r\nof the two fields are closely aligned. Statisticians are\r\nincreasingly concerned with the computational aspects,\r\nboth theoretical and practical, of models and infer-\r\nence procedures. Computer scientists are increasingly\r\nconcerned with systems that interact with the external\r\nworld and interpret uncertain data in terms of under-\r\nlying probabilistic models. One area in which these\r\ntrends are most evident is that of probabilistic graph-\r\nical models.\r\nA graphical model is a family of probability distribu-\r\ntions defined in terms of a directed or undirected graph.\r\nThe nodes in the graph are identified with random vari-\r\nables, and joint probability distributions are defined by\r\ntaking products over functions defined on connected\r\nsubsets of nodes. By exploiting the graphtheoretic\r\nMichael I. Jordan is Professor, Computer Science\r\nDivision and Department of Statistics, University of\r\nCalifornia, Berkeley, California 94720-3860, USA\r\n(e-mail: jordan@stat.berkeley.edu).\r\nrepresentation, the formalism provides general algo-\r\nrithms for computing marginal and conditional prob-\r\nabilities of interest. Moreover, the formalism provides\r\ncontrol over the computational complexity associated\r\nwith these operations.\r\nThe graphical model formalism is agnostic to the\r\ndistinction between frequentist and Bayesian statis-\r\ntics. However, by providing general machinery for\r\nmanipulating joint probability distributions and, in par-\r\nticular, by making hierarchical latent variable models\r\neasy to represent and manipulate, the formalism has\r\nproved to be particularly popular within the Bayesian\r\nparadigm. Viewing Bayesian statistics as the sys-\r\ntematic application of probability theory to statis-\r\ntics and viewing graphical models as a systematic\r\napplication of graphtheoretic algorithms to probabil-\r\nity theory, it should not be surprising that many authors\r\nhave viewed graphical models as a general Bayesian\r\ninference engine (Cowell, Dawid, Lauritzen and\r\nSpiegelhalter, 1999).\r\nWhat is perhaps most distinctive about the graph-\r\nical model approach is its naturalness in formulating\r\nprobabilistic models of complex phenomena in applied\r\nfields, while maintaining control over the computa-\r\ntional cost associated with these models. Accordingly,\r\nin this article our principal focus is on the presentation\r\nof graphical models that have proved useful in applied\r\ndomains and on ways in which the formalism encour-\r\nages the exploration of extensions of classical methods.\r\n140\r\nGRAPHICAL MODELS 141\r\nBefore turning to these examples, however, we begin\r\nwith an overview of basic concepts.\r\n2. REPRESENTATION\r\nThe two most common forms of graphical model are\r\ndirected graphical models and undirected graphical\r\nmodels, based on directed acylic graphs and undirected\r\ngraphs, respectively.\r\nLet us begin with the directed case. Let G(V,E)\r\nbe a directed acyclic graph, where V are the nodes\r\nand E are the edges of the graph. Let {Xv :v  V}\r\nbe a collection of random variables indexed by the\r\nnodes of the graph. To each node v  V, let v denote\r\nthe subset of indices of its parents. We allow sets of\r\nindices to appear wherever a single index appears;\r\nthus Xv denotes the vector of random variables\r\nindexed by the parents of v. Given a collection of\r\nkernels {k(xv|xv) :v  V} that sum (in the discrete\r\ncase) or integrate (in the continuous case) to 1 (with\r\nrespect to xv), we define a joint probability distribution\r\n(a probability mass function or probability density as\r\nappropriate) as\r\np(xV) =\r\n\r\nvV\r\nk(xv|xv).(1)\r\nIt is easy to verify that this joint probability distribution\r\nhas {k(xv|xv)} as its conditionals; thus, henceforth,\r\nwe write k(xv|xv) = p(xv|xv).\r\nNote that we have made no distinction between data\r\nand parameters, and indeed it is natural to include\r\nparameters among the nodes in the graph.\r\nA plate is a useful device for capturing replication\r\nin graphical models, including the factorial and nested\r\nstructures that occur in experimental designs. A simple\r\nexample of a plate is shown in Figure 1, which can be\r\nviewed as a graphical model representation of the de\r\nFinetti exchangeability theorem.\r\nDirected graphical models are familiar as represen-\r\ntations of hierarchical Bayesian models. An example is\r\ngiven in Figure 2.\r\nThe graph provides an appealing visual representa-\r\ntion of a joint probability distribution, but it also pro-\r\nvides a great deal more. First, whatever the functional\r\nforms of the kernels p(xv|xv), the factorization in (1)\r\nimplies a set of conditional independence statements\r\namong the variables Xv , and the entire set of condi-\r\ntional independence statements can be obtained from\r\na polynomial time reachability algorithm based on the\r\ngraph (Pearl, 1988). Second, as we discuss in the fol-\r\nlowing section, the graphical structure can be exploited\r\nby algorithms for probabilistic inference.\r\nLet us now consider the undirected case. Given an\r\nundirected graph G(V,E), we again let {Xv :v  V} be\r\na collection of random variables indexed by the nodes\r\nof the graph and let C denote a collection of cliques\r\nof the graph (i.e., fully connected subsets of nodes).\r\nAssociated with each clique C  C, let C(xC) denote\r\na nonnegative potential function. We define the joint\r\nprobability p(xV) by taking the product over these\r\npotential functions and normalizing,\r\np(xV) = 1\r\nZ\r\n\r\nCC\r\nC(xC),(2)\r\nwhere Z is a normalization factor obtained by inte-\r\ngrating or summing the product with respect to xV .\r\nSee Figure 3 for an example of an undirected graph-\r\nical model.\r\nFIG. 1. The diagram in (a) is shorthand for the graphical model in (b). This model asserts that the variables Zn are conditionally\r\nindependent and identically distributed given  , and can be viewed as a graphical model representation of the de Finetti theorem. Note\r\nthat shading, here and elsewhere in the paper, denotes conditioning.\r\n142 M. I. JORDAN\r\nFIG. 2. An example of a hierarchical Bayesian model represented\r\nas a directed graphical model. This is the errors-in-covariates\r\nlogistic regression model of Richardson, Leblond, Jaussent and\r\nGreen (2002). The core of this model is a logistic regression of Yi\r\non Xi . The covariate Xi is not observed (in general ), but noisy\r\nmeasurements Ui of Xi are available, as are additional observed\r\ncovariates Ci . The density model for Xi is taken to be a mixture\r\nmodel, where K is the number of components, W are the mixing\r\nproportions, Zi are the allocations and  parameterizes the mixture\r\ncomponents.\r\nUndirected graphs are often used in problems in ar-\r\neas such as spatial statistics, statistical natural language\r\nprocessing and communication networksproblems\r\nin which there is little causal structure to guide the con-\r\nstruction of a directed graph. However, there is no need\r\nFIG. 3. An example of an undirected graphical model. Proba-\r\nbility distributions associated with this graph can be factorized as\r\np(xV ) = 1Z (x1, x2)(x1, x3)(x2, x4)(x3, x5)(x2, x5, x6).\r\nto restrict undirected models to such domains; in par-\r\nticular, it is possible to include parameters among the\r\nnodes of an undirected graph to yield an alternative\r\ngeneral tool for Bayesian modeling. It is also possi-\r\nble to work with hybrids that include both directed and\r\nundirected edges (Lauritzen, 1996).\r\nIn general, directed graphs and undirected graphs\r\nmake different assertions of conditional independence.\r\nThus, there are families of probability distributions that\r\nare captured by a directed graph and are not captured\r\nby any undirected graph, and vice versa (Pearl, 1988).\r\nThe representations shown in (1) and (2) can be\r\noverly coarse for some purposes. In particular, in the\r\nundirected formalism the cliques C may be quite large,\r\nand it is often useful to consider potential functions\r\nthat are themselves factorized in ways that need not\r\nbe equated with conditional independencies. Thus, in\r\ngeneral, we consider a set of factors {fi(xCi ) : i  I}\r\nfor some index set I, where Ci is the subset of nodes\r\nassociated with the ith factor. Note in particular that\r\nthe same subset can be repeated multiple times (i.e.,\r\nwe allow Ci = Cj for i = j ). We define a joint\r\nprobability by taking the product across these factors:\r\np(xV) = 1\r\nZ\r\n\r\niI\r\nfi\r\n(\r\nxCi\r\n)\r\n.(3)\r\nAs shown in Figure 4, this definition is associated with\r\na graphical representationthe factor graph\r\n(Kschischang, Frey and Loeliger, 2001). A factor graph\r\nis a bipartite graph in which the random '),
(309, 560, 'Sketchpad: A man-machine graphical communication system', 'Technical Report\r\nNumber 574\r\nComputer Laboratory\r\nUCAM-CL-TR-574\r\nISSN 1476-2986\r\nSketchpad: A man-machine graphical\r\ncommunication system\r\nIvan Edward Sutherland\r\nSeptember 2003\r\nNew preface by Alan Blackwell and\r\nKerry Rodden.\r\n15 JJ Thomson Avenue\r\nCambridge CB3 0FD\r\nUnited Kingdom\r\nphone +44 1223 763500\r\nhttp://www.cl.cam.ac.uk/\r\nc 2003 Ivan Edward Sutherland\r\nThis technical report is based on a dissertation submitted January\r\n1963 by the author for the degree of Doctor of Philosophy to the\r\nMassachusetts Institute of Technology.\r\nTechnical reports published by the University of Cambridge\r\nComputer Laboratory are freely available via the Internet:\r\nhttp://www.cl.cam.ac.uk/TechReports/\r\nSeries editor: Markus Kuhn\r\nISSN 1476-2986\r\nPreface to this Electronic Edition\r\nAlan Blackwell and Kerry Rodden\r\nUniversity of Cambridge Computer Laboratory\r\nIvan Sutherlands Sketchpad is one of the most influential computer pro-\r\ngrams ever written by an individual, as recognized in his citation for the Tur-\r\ning award in 1988. The Sketchpad program itself had limited distribution \r\nexecutable versions were limited to a customized machine at the MIT Lincoln\r\nLaboratory  so its influence has been via the ideas that it introduced rather\r\nthan in its execution. Sutherlands dissertation describing Sketchpad was a\r\ncritical channel by which those ideas were propagated, along with a movie of\r\nthe program in use, and a widely-cited conference publication [10]. Copies of\r\nthe dissertation were distributed relatively widely, but it was never published\r\ncommercially. It is still available in the form of a technical report from MIT,\r\nbut we believe it deserves wider readership  hence this electronic archival\r\npublication.\r\nAfter 40 years, ideas introduced in Sketchpad still influence how every\r\ncomputer user thinks about computing. It made fundamental contributions in\r\nthe area of humancomputer interaction, being one of the first graphical user\r\ninterfaces. It exploited the light-pen, predecessor of the mouse, allowing the\r\nuser to point at and interact with objects displayed on the screen. This antic-\r\nipated many of the interaction conventions of direct manipulation, including\r\nclicking a button to select a visible object, and dragging to modify it. Smiths\r\nPygmalion [9], heavily influenced by Sketchpad, made a more explicit argu-\r\nment for the cognitive benefits of this kind of direct interaction and feedback,\r\ncoining the term icon, and making it clear that graphical images could rep-\r\nresent abstract entities of a programming language. Smith was a member of\r\nthe team that developed the Xerox Star workstation on these principles; in a\r\nretrospective article [4] they acknowledge that Sketchpad influenced Stars\r\nuser interface as a whole as well as its graphics applications, providing a di-\r\nrect link to the commercialization of the Macintosh and Windows interfaces\r\nand widely recognized benefits of direct manipulation [8].\r\nSketchpad encountered a critical challenge that remains central to human-\r\ncomputer interaction. Sutherlands original aim was to make computers ac-\r\ncessible to new classes of user (artists and draughtsmen among others), while\r\nretaining the powers of abstraction that are critical to programmers. In con-\r\ntrast, direct manipulation interfaces have since succeeded by reducing the lev-\r\nels of abstraction exposed to the user. Ongoing research in end-user program-\r\n4ming continues to struggle with the question of how to reduce the cognitive\r\nchallenges of abstract manipulation [1]. Nevertheless, Sutherlands attempt to\r\nremove the division between users and programmers was not the only sys-\r\ntem that, in failing to do so, provided the imaginative leap to a new program-\r\nming paradigm. Nygaard and Dahls Simula [7] was the first conventional\r\nprogramming language incorporating the principles of object orientation, but\r\nSketchpads implementation of class and instance-based inheritance (though\r\nnot called objects) predated Simula by several years.\r\nThere appears to have been a common influence through the work of Dou-\r\nglas T. Ross, who is mentioned in the acknowledgements of this dissertation\r\nand also cited in the MIT Lincoln Laboratory technical report based on it. Ross\r\nsat on the Algol 68 committee with C. A. R. Hoare in the mid-1960s, where his\r\nprevious work on a record-like data structure (called a plex) influenced Hoares\r\nown ideas on abstract data types [3], later credited by Nygaard and Dahl as\r\nthe origin of the class definition mechanisms in Simula [7].\r\nAlan Kays seminal Dynabook project, which led both to the Xerox Star\r\nand to the explosion of interest in object oriented programming through his\r\nlanguage Smalltalk, was directly influenced by Sketchpad. Kay has written of\r\nthe fact that the genesis of Smalltalk lay in the coincidental appearance on his\r\ndesk of both a distribution tape of Simula and a copy of Sutherlands Sketch-\r\npad thesis [5]. Kay recognized that the two systems were based on the same\r\nunderlying type concepts (apparently derived via two different routes from\r\nRosss plex), and that these could form the basis of a more widely usable pro-\r\ngramming system. In comparing these two routes of influence, Simula was a\r\nfar larger project than Sketchpad, rightly recognized as the first object-oriented\r\nprogramming language, but we hope that the special emphasis of Sketchpad\r\non supporting abstraction in the user interface itself may yet become viable as\r\na result of ongoing research efforts [2,6].\r\nAs with many early publications of computer science, this dissertation is\r\nalso interesting for the way in which it explores important concepts that are\r\nnow considered familiar, but which at the time demanded continual small dis-\r\ncoveries by every researcher. The first-person account of the history of the\r\nproject in Chapter 2 reads almost like an excerpt from an autobiography, as\r\nSutherland describes how he had to follow the stumbling trail towards gen-\r\nerality, through the different versions of Sketchpad. His rather charming pro-\r\nposal that dynamic data structures should be described using the terminology\r\nhen and chickens has been a sad loss when compared to the far more prosaic\r\nterminology of linked lists and garbage collection. The struggles of develop-\r\ning custom hardware while also exploring far-reaching abstractions are also\r\nfar removed from current research experiences.\r\nChapter 9 provides an immediate illustration of how far computer graph-\r\nics has moved on in the 40 years since Sketchpads development. For example,\r\nSutherland says that if the almost identical but slightly different frames that\r\nare required for making a motion picture cartoon could be produced semi-\r\nautomatically, the entire Sketchpad system could justify itself economically\r\nPersonal communication with C. A. R. Hoare.\r\n5in another way. Now, of course, we are used to seeing entire feature films\r\ncreated from computer graphics. Also, in choosing manipulation of facial fea-\r\ntures as an example, Sutherland has anticipated the sometimes controversial\r\nfacilities available in modern photograph editing tools.\r\nSutherlands clear writing makes all of these issues a fresh source of en-\r\njoyment to the contemporary reader, and we hope that it will reach a new\r\naudience with the assistance of this electronic edition, continuing the great in-\r\nfluence that Sketchpad has had on both users and programmers.\r\nThis Edition\r\nOur aim in preparing this edition has been to create an archival copy of the\r\nSketchpad dissertation, suitable for electronic access and scholarly reference.\r\nAlthough created in consultation with current international research efforts in\r\nelectronic archival, it is clear that there are, as yet, no common conventions for\r\nelectronic archive formats. Our priorities have been that this edition should be\r\naccessible for download using current technology (i.e. in a relatively small file\r\nsize), that it should be suitable for electronic search and indexing, that it should\r\nbe easy to read both on paper and on the screen, and that it should be faithful\r\nto the original document. These have not been easy criteria to meet, and our\r\nchosen solution (LATEX to PostScript to PDF) has several disadvantages, but is\r\nthe best overall solution we could find.\r\nThere are some editorial choices that should be explained. We thought it\r\nimportant to indicate original page numbering, so that citations of the original\r\ndissertation could be traced, but wished to avoid the decreased readability\r\nthat would have resulted from simply reproducing the original double-spaced\r\ntypescript. We therefore chose not to preserve page breaks and line breaks,\r\ninstead marking the positions of the original page breaks (with the  symbol)\r\nthroughout the main body of the text, giving the original page number in the\r\nmargin (next to the  symbol). For figures, the original page number is noted\r\nin the caption. We also chose not to correct any errors we found in the original\r\ndocument, in order to provide the textual equivalent of a facsimile edition.\r\nThese include a few spelling errors (to Ivans embarrassment), and also the\r\nrather idiosyncratic fact that the original dissertation had two pages 106.\r\nAn exact facsimile copy of the original dissertation (where the pages have\r\nsimply been scanned, not transcribed) can be purchased in hardcopy or PDF\r\nfrom the Digital Library of MIT Theses at http://theses.mit.edu/. Each\r\nindividual page can also be viewed as a GIF image, free of charge, which may\r\nbe a useful reference for readers wishing to check the layout of the original.\r\nWe are very grateful to Ivan Sutherland, who has encouraged this project,\r\nand who personally proof-read the original scanned text. We are also grateful\r\nto Malcolm Sabin, who kindly loaned us his original copy of the dissertation\r\nfor over a year. This work has been supported by the Engineering and Physical\r\nSciences Research Council, UK.\r\nSepte'),
(310, 561, 'Decimation of triangle meshes', 'Decimation of Triangle Meshes\r\nWilliam J. Schroeder\r\nJonathan A. Zarge\r\nWilliam E. Lorensen\r\nGeneral Electric Company\r\nSchenectady, NY\r\n1.0 INTRODUCTION\r\nThe polygon remains a popular graphics primitive for\r\ncomputer graphics application. Besides having a simple\r\nrepresentation, computer rendering of polygons is widely\r\nsupported by commercial graphics hardware and software.\r\nHowever, because the polygon is linear, often thousands\r\nor millions of primitives are required to capture the details\r\nof complex geometry. Models of this size are generally\r\nnot practical since rendering speeds and memory require-\r\nments are proportional to the number of polygons. Conse-\r\nquently applications that generate large polygonal meshes\r\noften use domain-specific knowledge to reduce model\r\nsize. There remain algorithms, however, where domain-\r\nspecific reduction techniques are not generally available\r\nor appropriate.\r\nOne algorithm that generates many polygons is march-\r\ning cubes. Marching cubes is a brute force surface con-\r\nstruction algorithm that extracts isodensity surfaces from\r\nvolume data, producing from one to five triangles within\r\nvoxels that contain the surface. Although originally devel-\r\noped for medical applications, marching cubes has found\r\nmore frequent use in scientific visualization where the size\r\nof the volume data sets are much smaller than those found\r\nin medical applications. A large computational fluid\r\ndynamics volume could have a finite difference grid size\r\nof order 100 by 100 by 100, while a typical medical com-\r\nputed tomography or magnetic resonance scanner pro-\r\nduces over 100 slices at a resolution of 256 by 256 or 512\r\nby 512 pixels each. Industrial computed tomography, used\r\nfor inspection and analysis, has even greater resolution,\r\nvarying from 512 by 512 to 1024 by 1024 pixels. For these\r\nsampled data sets, isosurface extraction using marching\r\ncubes can produce from 500k to 2,000k triangles. Even\r\ntodays graphics workstations have trouble storing and\r\nrendering models of this size.\r\nOther sampling devices can produce large polygonal\r\nmodels: range cameras, digital elevation data, and satellite\r\ndata. The sampling resolution of these devices is also\r\nimproving, resulting in model sizes that rival those\r\nobtained from medical scanners.\r\nThis paper describes an application independent algo-\r\nrithm that uses local operations on geometry and topology\r\nto reduce the number of triangles in a triangle mesh.\r\nAlthough our implementation is for the triangle mesh, it\r\ncan be directly applied to the more general polygon mesh.\r\nAfter describing other work related to model creation\r\nfrom sampled data, we describe the triangle decimation\r\nprocess and its implementation. Results from two differ-\r\nent geometric modeling applications illustrate the\r\nstrengths of the algorithm.\r\n2.0 THE DECIMATION ALGORITHM\r\nThe goal of the decimation algorithm is to reduce the\r\ntotal number of triangles in a triangle mesh, preserving\r\nthe original topology and a good approximation to the\r\noriginal geometry.\r\n2.1 OVERVIEW\r\nThe decimation algorithm is simple. Multiple passes are\r\nmade over all vertices in the mesh. During a pass, each\r\nvertex is a candidate for removal and, if it meets the spec-\r\nified decimation criteria, the vertex and all triangles that\r\nuse the vertex are deleted. The resulting hole in the mesh\r\nis patched by forming a local triangulation. The vertex\r\nremoval process repeats, with possible adjustment of the\r\ndecimation criteria, until some termination condition is\r\nmet. Usually the termination criterion is specified as a\r\npercent reduction of the original mesh (or equivalent), or\r\nas some maximum decimation value. The three steps of\r\nthe algorithm are:\r\n1. characterize the local vertex geometry and topology,\r\n2. evaluate the decimation criteria, and\r\n3. triangulate the resulting hole.\r\n2.2  CHARACTERIZING LOCAL\r\nGEOMETRY / TOPOLOGY\r\nThe first step of the decimation algorithm characterizes\r\nthe local geometry and topology for a given vertex. The\r\noutcome of this process determines whether the vertex is\r\na potential candidate for deletion, and if it is, which crite-\r\nria to use.\r\nEach vertex may be assigned one of five possible clas-\r\nsifications: simple, complex, boundary, interior edge, or\r\ncorner vertex. Examples of each type are shown in the\r\nfigure below.\r\nA simple vertex is surrounded by a complete cycle of\r\nSimple Complex Boundary Interior\r\nEdge\r\nCorner\r\ntriangles, and each edge that uses the vertex is used by\r\nexactly two triangles. If the edge is not used by two trian-\r\ngles, or if the vertex is used by a triangle not in the cycle of\r\ntriangles, then the vertex is complex. These are non-mani-\r\nfold cases.\r\nA vertex that is on the boundary of a mesh, i.e., within a\r\nsemi-cycle of triangles, is a boundary vertex.\r\nA simple vertex can be further classified as an interior\r\nedge or corner vertex. These classifications are based on the\r\nlocal mesh geometry. If the dihedral angle between two\r\nadjacent triangles is greater than a specified feature angle,\r\nthen a feature edge exists. When a vertex is used by two fea-\r\nture edges, the vertex is an interior edge vertex. If one or\r\nthree or more feature edges use the vertex, the vertex is clas-\r\nsified a corner vertex.\r\nComplex vertices are not deleted from the mesh. All other\r\nvertices become candidates for deletion.\r\n2.3 EVALUATING THE DECIMATION\r\nCRITERIA\r\nThe characterization step produces an ordered loop of verti-\r\nces and triangles that use the candidate vertex. The evalua-\r\ntion step determines whether the triangles forming the loop\r\ncan be deleted and replaced by another triangulation exclu-\r\nsive of the original vertex. Although the fundamental deci-\r\nmation criterion we use is based on vertex distance to plane\r\nor vertex distance to edge, others can be applied.\r\nSimple vertices use the distance to plane criterion (see\r\nfigure below). If the vertex is within the specified distance to\r\nthe average plane it may be deleted. Otherwise it is retained.\r\nBoundary and interior edge vertices use the distance to\r\nedge criterion (figure below). In this case, the algorithm\r\ndetermines the distance to the line defined by the two verti-\r\nces creating the boundary or feature edge. If the distance to\r\nthe line is less than d, the vertex can be deleted.\r\nIt is not always desirable to retain feature edges. For\r\nexample, meshes may contain areas of relatively small trian-\r\ngles with large feature angles, contributing relatively little to\r\nthe geometric approximation. Or, the small triangles may be\r\nthe result of noise in the original mesh. In these situations,\r\ncorner vertices, which are usually not deleted, and interior\r\nedge vertices, which are evaluated using the distance to\r\nedge criterion, may be evaluated using the distance to plane\r\ncriterion. We call this edge preservation, a user specifiable\r\nparameter.\r\nIf a vertex can be eliminated, the loop created by remov-\r\ning the triangles using the vertex must be triangulated. For\r\ninterior edge vertices, the original loop must be split into\r\ntwo halves, with the split line connecting the vertices form-\r\ning the feature edge. If the loop can be split in this way, i.e.,\r\nso that resulting two loops do not overlap, then the loop is\r\nsplit and each piece is triangulated separately.\r\n2.4 TRIANGULATION\r\nDeleting a vertex and its associated triangles creates one\r\n(simple or boundary vertex) or two loops (interior edge ver-\r\ntex). Within each loop a triangulation must be created\r\nwhose triangles are non-intersecting and non-degenerate. In\r\naddition, it is desirable to create triangles with good aspect\r\nratio and that approximate the original loop as closely as\r\npossible.\r\nIn general it is not possible to use a two-dimensional\r\nalgorithm to construct the triangulation, since the loop is\r\nusually non-planar. In addition, there are two important\r\ncharacteristics of the loop that can be used to advantage.\r\nFirst, if a loop cannot be triangulated, the vertex generating\r\nthe loop need not be removed. Second, since every loop is\r\nstar-shaped, triangulation schemes based on recursive loop\r\nsplitting are effective. The next section describes one such\r\nscheme.\r\nOnce the triangulation is complete, the original vertex and\r\nits cycle of triangles are deleted. From the Euler relation it\r\nfollows that removal of a simple, corner, or interior edge\r\nvertex reduces the mesh by precisely two triangles. If a\r\nboundary vertex is deleted then the mesh is reduced by pre-\r\ncisely one triangle.\r\n3.0 IMPLEMENTATION\r\n3.1 DATA STRUCTURES\r\nThe data structure must contain at least two pieces of infor-\r\nmation: the geometry, or coordinates, of each vertex, and\r\nthe definition of each triangle in terms of its three vertices.\r\nIn addition, because ordered lists of triangles surrounding a\r\nvertex are frequently required, it is desirable to maintain a\r\nlist of the triangles that use each vertex.\r\nAlthough data structures such as Weilers radial edge or\r\nBaumgarts winged-edge data structure can represent this\r\ninformation, our implementation uses a space-efficient ver-\r\ntex-triangle hierarchical ring structure. This data structure\r\ncontains hierarchical pointers from the triangles down to the\r\nvertices, and pointers from the vertices back up to the trian-\r\ngles using the vertex. Taken together these pointers form a\r\nring relationship. Our implementation uses three lists: a list\r\nof vertex coordinates, a list of triangle definitions, and\r\nanother list of lists of triangles using each vertex. Edge defi-\r\nnitions are not explicit, instead edges are implicitly defined\r\nas ordered vertex pairs in the triangle definition.\r\n3.2  TRIANGULATION\r\nAlthough other triangulation schemes can be used, we chose\r\na recursive loop splitting procedure. Each loop to be trian-\r\ngulated is divided into two halves. The division is along a\r\nline (i.e., the split line) defined from two non-neighboring\r\nvertices in the loop. Each new loop is divided again, until\r\nonly three vertices');
INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(311, 562, 'Progressive Meshes', 'Progressive Meshes\r\nHugues Hoppe\r\nMicrosoft Research\r\nABSTRACT\r\nHighly detailed geometric models are rapidly becoming common-\r\nplace in computer graphics. These models, often represented as\r\ncomplex triangle meshes, challenge rendering performance, trans-\r\nmission bandwidth, and storage capacities. This paper introduces\r\nthe progressive mesh (PM) representation, a new scheme for storing\r\nand transmitting arbitrary triangle meshes. This efficient, loss-\r\nless, continuous-resolution representation addresses several practi-\r\ncal problems in graphics: smooth geomorphing of level-of-detail\r\napproximations, progressive transmission, mesh compression, and\r\nselective refinement.\r\nIn addition, we present a new mesh simplification procedure for\r\nconstructing a PM representation from an arbitrary mesh. The goal\r\nof this optimization procedure is to preserve not just the geometry\r\nof the original mesh, but more importantly its overall appearance\r\nas defined by its discrete and scalar appearance attributes such as\r\nmaterial identifiers, color values, normals, and texture coordinates.\r\nWe demonstrate construction of the PM representation and its ap-\r\nplications using several practical models.\r\nCR Categories and Subject Descriptors: I.3.5 [Computer Graphics]:\r\nComputational Geometry and Object Modeling - surfaces and object repre-\r\nsentations.\r\nAdditional Keywords: mesh simplification, level of detail, shape interpo-\r\nlation, progressive transmission, geometry compression.\r\n1 INTRODUCTION\r\nHighly detailed geometric models are necessary to satisfy a grow-\r\ning expectation for realism in computer graphics. Within traditional\r\nmodeling systems, detailed models are created by applying ver-\r\nsatile modeling operations (such as extrusion, constructive solid\r\ngeometry, and freeform deformations) to a vast array of geometric\r\nprimitives. For efficient display, these models must usually be tes-\r\nsellated into polygonal approximationsmeshes. Detailed meshes\r\nare also obtained by scanning physical objects using range scanning\r\nsystems [5]. In either case, the resulting complex meshes are ex-\r\npensive to store, transmit, and render, thus motivating a number of\r\npractical problems:\r\nEmail: hhoppe@microsoft.com\r\nWeb: http://www.research.microsoft.com/research/graphics/hoppe/\r\n\0 Mesh simplification: The meshes created by modeling and scan-\r\nning systems are seldom optimized for rendering efficiency, and\r\ncan frequently be replaced by nearly indistinguishable approx-\r\nimations with far fewer faces. At present, this process often\r\nrequires significant user intervention. Mesh simplification tools\r\ncan hope to automate this painstaking task, and permit the port-\r\ning of a single model to platforms of varying performance.\r\n\0 Level-of-detail (LOD) approximation: To further improve ren-\r\ndering performance, it is common to define several versions of a\r\nmodel at various levels of detail [3, 8]. A detailed mesh is used\r\nwhen the object is close to the viewer, and coarser approxima-\r\ntions are substituted as the object recedes. Since instantaneous\r\nswitching between LOD meshes may lead to perceptible pop-\r\nping, one would like to construct smooth visual transitions,\r\ngeomorphs, between meshes at different resolutions.\r\n\0 Progressive transmission: When a mesh is transmitted over a\r\ncommunication line, one would like to show progressively better\r\napproximations to the model as data is incrementally received.\r\nOne approach is to transmit successive LOD approximations,\r\nbut this requires additional transmission time.\r\n\0 Mesh compression: The problem of minimizing the storage\r\nspace for a model can be addressed in two orthogonal ways.\r\nOne is to use mesh simplification to reduce the number of faces.\r\nThe other is mesh compression: minimizing the space taken to\r\nstore a particular mesh.\r\n\0 Selective refinement: Each mesh in a LOD representation cap-\r\ntures the model at a uniform (view-independent) level of detail.\r\nSometimes it is desirable to adapt the level of refinement in se-\r\nlected regions. For instance, as a user flies over a terrain, the\r\nterrain mesh need be fully detailed only near the viewer, and\r\nonly within the field of view.\r\nIn addressing these problems, this paper makes two major con-\r\ntributions. First, it introduces the progressive mesh (PM) repre-\r\nsentation. In PM form, an arbitrary mesh \0M is stored as a much\r\ncoarser mesh M0 together with a sequence of n detail records that\r\nindicate how to incrementally refine M0 exactly back into the orig-\r\ninal mesh \0M = Mn. Each of these records stores the information\r\nassociated with a vertex split, an elementary mesh transformation\r\nthat adds an additional vertex to the mesh. The PM representation\r\nof \0M thus defines a continuous sequence of meshes M0\0 M1\0    \0 Mn\r\nof increasing accuracy, from which LOD approximations of any de-\r\nsired complexity can be efficiently retrieved. Moreover, geomorphs\r\ncan be efficiently constructed between any two such meshes. In\r\naddition, we show that the PM representation naturally supports\r\nprogressive transmission, offers a concise encoding of \0M itself, and\r\npermits selective refinement. In short, progressive meshes offer an\r\nefficient, lossless, continuous-resolution representation.\r\nThe other contribution of this paper is a new simplification pro-\r\ncedure for constructing a PM representation from a given mesh\r\n\0M. Unlike previous simplification methods, our procedure seeks\r\nto preserve not just the geometry of the mesh surface, but more\r\nimportantly its overall appearance, as defined by the discrete and\r\nscalar attributes associated with its surface.\r\n2 MESHES IN COMPUTER GRAPHICS\r\nModels in computer graphics are often represented using triangle\r\nmeshes.1 Geometrically, a triangle mesh is a piecewise linear sur-\r\nface consisting of triangular faces pasted together along their edges.\r\nAs described in [9], the mesh geometry can be denoted by a tuple\r\n(K\0 V), where K is a simplicial complex specifying the connectivity\r\nof the mesh simplices (the adjacency of the vertices, edges, and\r\nfaces), and V = fv1\0    \0vmg is the set of vertex positions defining\r\nthe shape of the mesh in R3. More precisely (cf. [9]), we construct\r\na parametric domain jKj  Rm by identifying each vertex of K with\r\na canonical basis vector of Rm, and define the mesh as the image\r\nV(jKj) where V : Rm  R3 is a linear map.\r\nOften, surface appearance attributes other than geometry are also\r\nassociated with the mesh. These attributes can be categorized into\r\ntwo types: discrete attributes and scalar attributes.\r\nDiscrete attributes are usually associated with faces of the mesh.\r\nA common discrete attribute, the material identifier, determines\r\nthe shader function used in rendering a face of the mesh [18]. For\r\ninstance, a trivial shader function may involve simple look-up within\r\na specified texture map.\r\nMany scalar attributes are often associated with a mesh, including\r\ndiffuse color (r\0 g\0 b), normal (nx\0 ny\0 nz), and texture coordinates\r\n(u\0 v). More generally, these attributes specify the local parameters\r\nof shader functions defined on the mesh faces. In simple cases, these\r\nscalar attributes are associated with vertices of the mesh. However,\r\nto represent discontinuities in the scalar fields, and because adjacent\r\nfaces may have different shading functions, it is common to associate\r\nscalar attributes not with vertices, but with corners of the mesh [1].\r\nA corner is defined as a (vertex,face) tuple. Scalar attributes at a\r\ncorner (v\0 f ) specify the shading parameters for face f at vertex v.\r\nFor example, along a crease (a curve on the surface across which\r\nthe normal field is not continuous), each vertex has two distinct\r\nnormals, one associated with the corners on each side of the crease.\r\nWe express a mesh as a tuple M = (K\0 V\0D\0 S) where V specifies\r\nits geometry, D is the set of discrete attributes df associated with\r\nthe faces f = fj\0 k\0 lg  K, and S is the set of scalar attributes s(v\0f )\r\nassociated with the corners (v\0 f ) of K.\r\nThe attributes D and S give rise to discontinuities in the visual\r\nappearance of the mesh. An edge fvj\0 vkg of the mesh is said to be\r\nsharp if either (1) it is a boundary edge, or (2) its two adjacent faces\r\nfl and fr have different discrete attributes (i.e. dfl = dfr ), or (3) its\r\nadjacent corners have different scalar attributes (i.e. s(vj\0fl) = s(vj\0fr)\r\nor s(vk\0fl) = s(vk\0fr)). Together, the set of sharp edges define a set\r\nof discontinuity curves over the mesh (e.g. the yellow curves in\r\nFigure 12).\r\n3 PROGRESSIVE MESH REPRESENTATION\r\n3.1 Overview\r\nHoppe et al. [9] describe a method, mesh optimization, that can\r\nbe used to approximate an initial mesh \0M by a much simpler one.\r\nTheir optimization algorithm, reviewed in Section 4.1, traverses the\r\nspace of possible meshes by successively applying a set of 3 mesh\r\ntransformations: edge collapse, edge split, and edge swap.\r\nWe have discovered that in fact a single one of those transforma-\r\ntions, edge collapse, is sufficient for effectively simplifying meshes.\r\nAs shown in Figure 1, an edge collapse transformation ecol(fvs\0 vtg)\r\n1We assume in this paper that more general meshes, such as those con-\r\ntaining n-sided faces and faces with holes, are first converted into triangle\r\nmeshes by triangulation. The PM representation could be generalized to\r\nhandle the more general meshes directly, at the expense of more complex\r\ndata structures.\r\nvt\r\nv\r\ns\r\nvl vr\r\nvl vr\r\nv\r\ns\r\necol\r\nvsplit\r\nFigure 1: Illustration of the edge collapse transformation.\r\nv1\r\nv2\r\nv3\r\nv4\r\nv5\r\nv6\r\nv7\r\nv1\r\nv2\r\nv3\r\nv4\r\nv5\r\nv6\r\nv1\r\nv2\r\nv3\r\nMi+1 Mi\r\necol i\r\nM0\r\necol 0\r\nm0=3\r\ns0=2\r\nsi=4\r\n(i=3)\r\nv1\r\nv2\r\nv3\r\nv4\r\nv5\r\nv6\r\nv7\r\nMf\r\nv1\r\nv2\r\nv3\r\nMc\r\nAc\r\n(a) (b)\r\nFigure 2: (a) Sequence of edge collapses; (b) Resulting vertex\r\ncorrespondence.\r\nunifies 2 adjacent vertices vs and vt into a single vertex vs. The ver-\r\ntex vt and the two adjacent faces fvs\0 vt\0 vlg and fvt\0 vs\0 vrg vanish\r\nin the process. A position vs is specified for t'),
(312, 563, 'Surface Simplification Using Quadric Error Metrics', 'Surface Simplication Using Quadric Error Metrics\nMichael Garland\n\0 Paul S. Heckbert\n\nCarnegie Mellon University\nAbstract\nMany applications in computer graphics require complex, highly\ndetailed models. However, the level of detail actually necessary\nmay vary considerably. To control processing time, it is often desir-\nable to use approximations in place of excessively detailed models.\nWe have developed a surface simplication algorithm which can\nrapidly produce high quality approximations of polygonal models.\nThe algorithm uses iterative contractions of vertex pairs to simplify\nmodels and maintains surface error approximations using quadric\nmatrices. By contracting arbitrary vertex pairs (not just edges), our\nalgorithm is able to join unconnected regions of models. This can\nfacilitate much better approximations, both visually and with re-\nspect to geometric error. In order to allow topological joining, our\nsystem also supports non-manifold surface models.\nCR Categories: I.3.5 [Computer Graphics]: Computational Ge-\nometry and Object Modelingsurface and object representations\nKeywords: surface simplication, multiresolution modeling, pair\ncontraction, level of detail, non-manifold\n1 Introduction\nMany computer graphics applications require complex, highly de-\ntailed models to maintain a convincing level of realism. Conse-\nquently, models are often created or acquired at a very high reso-\nlution to accommodate this need for detail. However, the full com-\nplexity of such models is not always required, and since the compu-\ntational cost of using a model is directly related to its complexity, it\nis useful to have simpler versions of complex models. Naturally, we\nwould like to automatically produce these simplied models. Re-\ncent work on surface simplication algorithms has focused on this\ngoal.\nAs with most other work in this area, we will focus on the simpli-\ncation of polygonal models. We will assume that the model con-\nsists of triangles only. This implies no loss of generality, since every\npolygon in the original model can be triangulated as part of a pre-\nprocessing phase. To achieve more reliable results, when corners of\ntwo faces intersect at a point, the faces should be dened as sharing\na single vertex rather than using two separate vertices which happen\nto be coincident in space.\n\0 garland@cs.cmu.edu; http://www.cs.cmu.edu/\0 garland/\n\nph@cs.cmu.edu; http://www.cs.cmu.edu/\0 ph/\nWe have developed an algorithm which produces simplied ver-\nsions of such polygonal models. Our algorithm is based on the iter-\native contraction of vertex pairs (a generalization of edge contrac-\ntion). As the algorithm proceeds, a geometric error approximation\nis maintained at each vertex of the current model. This error approx-\nimation is represented using quadric matrices. The primary advan-\ntages of our algorithm are:\n\0 Efciency: The algorithm is able to simplify complex models\nquite rapidly. For example, our implementation can create a\n100 face approximation of a 70,000 face model in 15 seconds.\nThe error approximation is also very compact, requiring only\n10 oating point numbers per vertex.\n\0 Quality: The approximations produced by our algorithm\nmaintain high delity to the original model. The primary fea-\ntures of the model are preserved even after signicant simpli-\ncation.\n\0 Generality: Unlike most other surface simplication algo-\nrithms, ours is able to join unconnected regions of the model\ntogether, a process which we term aggregation. Provided that\nmaintaining object topology is not an important concern, this\ncan facilitate better approximations of models with many dis-\nconnected components. This also requires our algorithm to\nsupport non-manifold\n1\nmodels.\n2 Background and Related Work\nThe goal of polygonal surface simplication is to take a polygonal\nmodel as input and generate a simplied model (i.e., an approxima-\ntion of the original) as output. We assume that the input model (M\nn\n)\nhas been triangulated. The target approximation (M\ng\n) will satisfy\nsome given target criterion which is typically either a desired face\ncount or a maximum tolerable error. We are interested in surface\nsimplication algorithms that can be used in rendering systems for\nmultiresolution modeling  the generation of models with appro-\npriate levels of detail for the current context.\nWe do not assume that the topology of the model must be main-\ntained. In certain application areas, medical imaging for example,\nmaintaining the object topology can be essential. However, in appli-\ncation areas such as rendering, topology is less important than over-\nall appearance. Our algorithm is capable of both closing topological\nholes as well as joining unconnected regions.\nMany prior simplication algorithms have either implicitly or ex-\nplicitly assumed that their input surfaces were, and ought to remain,\nmanifold surfaces. Let us stress that we do not make this assump-\ntion. In fact, the process of aggregation will regularly create non-\nmanifold regions.\n2.1 Surface Simplication\nIn recent years, the problem of surface simplication, and the more\ngeneral problem of multiresolution modeling, has received increas-\n1\nA manifold is a surface for which the innitesimal neighborhood of ev-\nery point is topologically equivalent to a disk (or half-disk for a manifold\nwith boundary).Before After\ncontract\nv\n1\nv\n2\nv\nFigure 1: Edge contraction. The highlighted edge is contracted\ninto a single point. The shaded triangles become degenerate and are\nremoved during the contraction.\ning attention. Several different algorithms have been formulated for\nsimplifying surfaces. Those algorithms which are most relevant to\nour work can be broadly categorized into 3 classes:\nVertex Decimation. Schroeder et al. [9] describe an algorithm\nwhichwewouldterm vertex decimation. Their method iteratively\nselects a vertex for removal, removes all adjacent faces, and retri-\nangulates the resulting hole. Soucy and Laurendeau [10] described\na more sophisticated, but essentially similar algorithm. While they\nprovide reasonable efciency and quality, these methods are not re-\nally suited for our purpose. Both methods use vertex classication\nand retriangulation schemes which are inherently limited to mani-\nfold surfaces, and they carefully maintain the topology of the model.\nWhile these are important features in some domains, they are restric-\ntions for multiresolution rendering systems.\nVertex Clustering. The algorithm described by Rossignac and\nBorrel [8] is one of the few capable of processing arbitrary polygo-\nnal input. A bounding box is placed around the original model and\ndivided into a grid. Within each cell, the cells vertices are clustered\ntogether into a single vertex, and the model faces are updated ac-\ncordingly. This process can be very fast, and can make drastic topo-\nlogical alterations to the model. However, while the size of the grid\ncells does provide a geometric error bound, the quality of the out-\nput is often quite low. In addition, it is difcult to construct an ap-\nproximation with a specic face count, since the number of faces\nis only indirectly determined by the specied grid dimensions. The\nexact approximation produced is also dependent on the exact posi-\ntion and orientation of the original model with respect to the sur-\nrounding grid. This uniform method can easily be generalized to use\nan adaptive grid structure, such as an octree [6]. This can improve\nthe simplication results, but it still does not support the quality and\ncontrol that we desire.\nIterative Edge Contraction. Several algorithms have been\npublished that simplify models by iteratively contracting edges (see\nFigure 1). The essential difference between these algorithms lies in\nhow they choose an edge to contract. Some notable examples of\nsuch algorithms are those of Hoppe [4, 3], Ronfard and Rossignac\n[7], and Gu eziec [2]. These algorithms all seem to have been de-\nsigned for use on manifold surfaces, although edge contractions can\nbe utilized on non-manifold surfaces. By performing successive\nedge contractions, they can close holes in the object but they can-\nnot join unconnected regions.\nIf it is critical that the approximate model lie within some dis-\ntance of the original model and that its topology remain unchanged,\nthe simplication envelopes technique of Cohen et al. [1] can be\nused in conjunction with one of the above simplication algorithms.\nAs long as any modication made to the model is restricted to lie\nwithin the envelopes, a global error guarantee can be maintained.\nHowever, while this provides strong error limits, the method is in-\nherently limited to orientable manifold surfaces and carefully pre-\ncontract\nBefore After\nv\n1\nv\n2 v\nFigure 2: Non-edge contraction. When non-edge pairs are con-\ntracted, unconnected sections of the model are joined. The dashed\nline indicates the two vertices being contracted together.\nserves model topology. Again, these are often limitations for the\npurposes of simplication for rendering.\nNone of these previously developed algorithms provide the com-\nbination of efciency, quality, and generality that we desire. Vertex\ndecimation algorithms are unsuitable for our needs; they are careful\nto maintain model topology and usually assume manifold geometry.\nVertex clustering algorithms are very general and can be very fast.\nHowever, they provide poor control over their results and these re-\nsults can be of rather low quality. Edge contraction algorithms can\nnot support aggregation.\nWe have developed an algorithm which supports both aggrega-\ntion and high quality approximations. It possesses much of the gen-\nerality of vertex clustering as well as the quality and control of itera-\ntive contraction algorithms. It also allows faster simplication than\nsome higher quality methods [3].\n3 Decimation via Pair Contraction\nOur simplication algorithm is based on the iterative contraction of\nvertex pairs; a generalization of the iterative edge contraction tech-\nnique used in previous work. A pair contra'),
(313, 564, 'Tcl and the Tk Toolkit', 'Tcl and the Tk Toolkit\r\nJohn K. Ousterhout\r\nComputer Science Division\r\nDepartment of Electrical Engineering and Computer Sciences\r\nUniversity of California\r\nBerkeley, CA 94720\r\nCopyright  1993 Addison-Wesley Publishing Company, Inc.\r\nAll rights reserved. Duplication of this draft is permitted by individuals for personal\r\nuse only. Any other form of duplication or reproduction requires prior written permis-\r\nsion of the author or publisher. This statement must be easily visible on the first page\r\nof any reproduced copies. The publisher does not offer warranties in regard to this\r\ndraft.\r\nNote to readers:\r\nThis manuscript is a partial draft of a book to be published in early 1994 by Addison-\r\nWesley (ISBN 0-201-63337-X). Addison-Wesley has given me permission to make\r\ndrafts of the book available to the Tcl community to help meet the need for introduc-\r\ntory documentation on Tcl and Tk until the book becomes available. Please observe\r\nthe restrictions set forth in the copyright notice above: youre welcome to make a\r\ncopy for yourself or a friend but any sort of large-scale reproduction or reproduction\r\nfor profit requires advance permission from Addison-Wesley.\r\nI would be happy to receive any comments you might have on this draft; send them to\r\nme via electronic mail at ouster@cs.berkeley.edu. Im particularly interested\r\nin hearing about things that you found difficult to learn or that werent adequately\r\nexplained in this document, but Im also interested in hearing about inaccuracies,\r\ntypos, or any other constructive criticism you might have.\r\n2DRAFT (8/12/93): Distribution Restricted\r\n1DRAFT (8/12/93): Distribution Restricted\r\nChapter 1 Introduction 1\r\n1.1 Introduction 1\r\n1.2 Organization of the book 3\r\n1.3 Notation 4\r\nChapter 2 An Overview of Tcl and Tk 5\r\n2.1 Getting started 5\r\n2.2 Hello world with Tk 7\r\n2.3 Script files 9\r\n2.4 Variables and substitutions 10\r\n2.5 Control structures 11\r\n2.6 Event bindings 13\r\n2.7 Subprocesses 15\r\n2.8 Additional features of Tcl and Tk 18\r\n2.9 Extensions and applications 18\r\n2.9.1 Expect 19\r\n2.9.2 Extended Tcl 19\r\n2.9.3 XF 20\r\n2.9.4 Distributed programming 20\r\n2.9.5 Ak 22\r\nChapter 3 Tcl Language Syntax 25\r\n3.1 Scripts, commands, and words 25\r\n3.2 Evaluating a command 26\r\n3.3 Variable substitution 28\r\n3.4 Command substitution 29\r\n3.5 Backslash substitution 30\r\n3.6 Quoting with double-quotes 30\r\n3.7 Quoting with braces 32\r\n3.8 Comments 33\r\n3.9 Normal and exceptional returns 33\r\n3.10 More on substitutions 34\r\n2DRAFT (8/12/93): Distribution Restricted\r\nChapter 4 Variables 37\r\n4.1 Simple variables and the set command 37\r\n4.2 Arrays 38\r\n4.3 Variable substitution 39\r\n4.4 Removing variables: unset 40\r\n4.5 Multi-dimensional arrays 41\r\n4.6 The incr and append commands 41\r\n4.7 Preview of other variable facilities 42\r\nChapter 5 Expressions 43\r\n5.1 Numeric operands 43\r\n5.2 Operators and precedence 44\r\n5.2.1 Arithmetic operators 44\r\n5.2.2 Relational operators 46\r\n5.2.3 Logical operators 46\r\n5.2.4 Bitwise operators 46\r\n5.2.5 Choice operator 46\r\n5.3 Math functions 47\r\n5.4 Substitutions 47\r\n5.5 String manipulation 49\r\n5.6 Types and conversions 49\r\n5.7 Precision 50\r\nChapter 6 Lists 51\r\n6.1 Basic list structure and the lindex command 51\r\n6.2 Creating lists: concat, list, and llength 53\r\n6.3 Modifying lists: linsert, lreplace, lrange, and lappend 54\r\n6.4 Searching lists: lsearch 56\r\n6.5 Sorting lists: lsort 56\r\n6.6 Converting between strings and lists: split and join 57\r\n6.7 Lists and commands 58\r\n3DRAFT (8/12/93): Distribution Restricted\r\nChapter 7 Control Flow 61\r\n7.1 The if command 61\r\n7.2 Looping commands: while, for, and foreach 63\r\n7.3 Loop control: break and continue 65\r\n7.4 The switch command 65\r\n7.5 Eval 67\r\n7.6 Executing from files: source 68\r\nChapter 8 Procedures 69\r\n8.1 Procedure basics: proc and return 69\r\n8.2 Local and global variables 71\r\n8.3 Defaults and variable numbers of arguments 72\r\n8.4 Call by reference: upvar 73\r\n8.5 Creating new control structures: uplevel 74\r\nChapter 9 Errors and Exceptions 77\r\n9.1 What happens after an error? 77\r\n9.2 Generating errors from Tcl scripts 79\r\n9.3 Trapping errors with catch 80\r\n9.4 Exceptions in general 81\r\nChapter 10 String Manipulation 85\r\n10.1 Glob-style pattern matching 85\r\n10.2 Pattern matching with regular expressions 88\r\n10.3 Using regular expressions for substitutions 90\r\n10.4 Generating strings with format 91\r\n10.5 Parsing strings with scan 93\r\n10.6 Extracting characters: string index and string range 94\r\n10.7 Searching and comparison 94\r\n10.8 Length, case conversion, and trimming 95\r\n4DRAFT (8/12/93): Distribution Restricted\r\nChapter 11 Accessing Files 97\r\n11.1 File names 97\r\n11.2 Basic file I/O 99\r\n11.3 Output buffering 101\r\n11.4 Random access to files 101\r\n11.5 The current working directory 102\r\n11.6 Manipulating file names: glob and file 102\r\n11.7 File information commands 105\r\n11.8 Errors in system calls 107\r\nChapter 12 Processes 109\r\n12.1 Invoking subprocesses with exec 109\r\n12.2 I/O to and from a command pipeline 112\r\n12.3 Process ids 113\r\n12.4 Environment variables 113\r\n12.5 Terminating the Tcl process with exit 113\r\nChapter 13 Managing Tcl Internals 115\r\n13.1 Querying the elements of an array 115\r\n13.2 The info command 117\r\n13.2.1 Information about variables 117\r\n13.2.2 Information about procedures 120\r\n13.2.3 Information about commands 121\r\n13.2.4 Tclversion and library 122\r\n13.3 Timing command execution 122\r\n13.4 Tracing operations on variables 123\r\n13.5 Renaming and deleting commands 125\r\n13.6 Unknown commands 126\r\n13.7 Auto-loading 128\r\nChapter 14 History 131\r\n14.1 The history list 131\r\n5DRAFT (8/12/93): Distribution Restricted\r\n14.2 Specifying events 133\r\n14.3 Re-executing commands from the history list 133\r\n14.4 Shortcuts implemented by unknown 134\r\n14.5 Current event number: history nextid 134\r\n6DRAFT (8/12/93): Distribution Restricted\r\n1\r\nCopyright  1993 Addison-Wesley Publishing Company, Inc.\r\nAll rights reserved. Duplication of this draft is permitted by individuals for personal use only. Any\r\nother form of duplication or reproduction requires prior written permission of the author or pub-\r\nlisher. This statement must be easily visible on the first page of any reproduced copies. The publisher\r\ndoes not offer warranties in regard to this draft.\r\nChapter 1\r\nIntroduction\r\n1.1 Introduction\r\nThis book is about two packages called Tcl and Tk. Together they provide a programming\r\nsystem for developing and using graphical user interface (GUI) applications. Tcl stands\r\nfor tool command language and is pronounced tickle; is a simple scripting language\r\nfor controlling and extending applications. It provides generic programming facilities that\r\nare useful for a variety of applications, such as variables and loops and procedures. Fur-\r\nthermore, Tcl is embeddable: its interpreter is implemented as a library of C procedures\r\nthat can easily be incorporated into applications, and each application can extend the core\r\nTcl features with additional commands specific to that application.\r\nOne of the most useful extensions to Tcl is Tk. It is a toolkit for the X Window Sys-\r\ntem, and its name is pronounced tee-kay. Tk extends the core Tcl facilities with addi-\r\ntional commands for building user interfaces, so that you can construct Motif user\r\ninterfaces by writing Tcl scripts instead of C code. Like Tcl, Tk is implemented as a library\r\nof C procedures so it too can be used in many different applications. Individual applica-\r\ntions can also extend the base Tk features with new user-interface widgets and geometry\r\nmanagers written in C.\r\nTogether, Tcl and Tk provide four benefits to application developers and users. First,\r\nTcl makes it easy for any application to have a powerful scripting language. All that an\r\napplication needs to do is to implement a few new Tcl commands that provide the basic\r\nfeatures of that application. Then the application can be linked with the Tcl interpreter to\r\nproduce a full-function scripting language that includes both the commands provided by\r\nTcl (called the Tcl core) and those implemented by the application (see Figure 1.1).\r\nFIGURE  1\r\nTABLE  1\r\n2 Introduction\r\nDRAFT (8/12/93): Distribution Restricted\r\nFor example, an application for reading electronic bulletin boards might contain C\r\ncode that implements one Tcl command to query a bulletin board for new messages and\r\nanother Tcl command to retrieve a given message. Once these commands exist, Tcl scripts\r\ncan be written to cycle through the new messages from all the bulletin boards and display\r\nthem one at a time, or keep a record in disk files of which messages have been read and\r\nwhich havent, or search one or more bulletin boards for messages on a particular topic.\r\nThe bulletin board application would not have to implement any of these additional func-\r\ntions in C; they could all be written as Tcl scripts, and users of the application could write\r\nadditional Tcl scripts to add more functions to the application.\r\nThe second benefit of Tcl and Tk is rapid development. For example, many interest-\r\ning windowing applications can be written entirely as Tcl scripts with no C code at all,\r\nusing a windowing shell called wish. This allows you to program at a much higher level\r\nthan you would in C or C++, and many of the details that C programmers must address are\r\nhidden from you. Compared to toolkits where you program entirely in C, such as Xt/\r\nMotif, there is much less to learn in order to use Tcl and Tk and much less code to write.\r\nNew Tcl/Tk users can often create interesting user interfaces after just a few hours of\r\nlearning, and many people have reported ten-fold reductions in code size and development\r\ntime when they switched from other toolkits to Tcl and Tk.\r\nAnother reason for rapid development with Tcl and Tk is that Tcl is an interpreted lan-\r\nguage. When you use a Tcl application such as wish you can generate and execute new\r\nscripts on-the-fly without recompiling or restarting the application. This allows you to test\r\nout '),
(314, 565, 'Plenoptic Modeling: An Image-Based Rendering System', 'Proceedings of SIGGRAPH 95 (Los Angeles, California, August 6-11, 1995)\r\ninitial problems of camera calibration, two-dimensional image reg-\r\nistration, and photometric\r\ntion of three-dimensional\r\nproblems such as robot nav\r\nunderstanding have natura\r\ngraphics, the progression\r\nthe latin root plenus, meaning complete or full, and optic pertaining\r\nany point in space, at any\r\nhey used this function to\r\n of low-level vision. The\r\nnt energy that can be per-\r\ner rather than the point of\r\n can be considered\r\ne or two dimensions\r\n structure of the\r\n an observer.\r\n functional description by\r\nthe plenoptic function is\r\nealized eye which we are\r\n. From there we can select\r\nPlenoptic M\r\nAn Image-Based R\r\nCB 3175 Sitterson Hall, Chape\r\n (919) 962-1797 mcmillan@\r\n (919) 962-1886 gb@cs.uncs have progressed toward the determina-\r\n models. Likewise, in computer vision,\r\nigation, image discrimination, and image\r\nlly led in the same direction. In computer\r\n toward image-based rendering systems\r\nto vision) to the pencil of rays visible from \r\ntime, and over any range of wavelengths. T\r\ndevelop a taxonomy for evaluating models\r\nplenoptic function describes all of the radia\r\nceived from the point of view of the observ\r\nview of the source. They postulate\r\n all the basic visual measurements\r\nto characterize local change along on\r\nof a single function that describes the\r\ninformation in the light impinging on\r\nAdelson and Bergen further formalized this\r\nproviding a parameter space over which \r\nvalid, as shown in Figure 1. Imagine an id\r\nfree to place at any point in space (Vx, Vy, Vz)\r\nl Hill, NC 27599\r\ncs.unc.edu http://www.cs.unc.edu/~mcmillan\r\n.edu http://www.cs.unc.edu/~gbABSTRACT\r\nImage-based rendering is a powerful new approach for generating\r\nreal-time photorealistic computer graphics. It can provide convinc-\r\ning animations without an explicit geometric representation. We use\r\nthe plenoptic function of Adelson and Bergen to provide a concise\r\nproblem statement for image-based rendering paradigms, such as\r\nmorphing and view interpolation. The plenoptic function is a param-\r\neterized function for describing everything that is visible from a\r\ngiven point in space. We present an image-based rendering system\r\nbased on sampling, reconstructing, and resampling the plenoptic\r\nfunction. In addition, we introduce a novel visible surface algorithm\r\nand a geometric invariant for cylindrical projections that is equiva-\r\nlent to the epipolar constraint defined for planar projections.\r\nCR Descriptors: I.3.3 [Computer Graphics]: Picture/Image Gen-\r\neration display algorithms, viewing algorithms; I.3.7 [Computer\r\nGraphics]: Three-Dimensional Graphics and Realism hidden line/\r\nsurface removal; I.4.3 [Image Processing]: Enhancement regis-\r\ntration; I.4.7 [Image Processing]: Feature Measurement\r\nprojections; I.4.8 [Image Processing]: Scene Analysis.\r\n1. INTRODUCTION\r\nIn recent years there has been increased interest, within the computer\r\ngraphics community, in image-based rendering systems. These sys-\r\ntems are fundamentally different from traditional geometry-based\r\nrendering systems. In image-based systems the underlying data rep-\r\nresentation (i.e model) is composed of a set of photometric\r\nobservations, whereas geometry-based systems use either mathe-\r\nmatical descriptions of the boundary regions separating scene\r\nelements (B-rep) or discretely sampled space functions (volumetric).\r\nThe evolution of image-based rendering systems can be traced\r\nthrough at least three different research fields. In photogrammetry the\r\nLeonard McMillan\r\nDepartment of Co\r\nUniversity of North Cawas initially motivated by the desire to increase the visual realism of\r\nthe approximate geometric descriptions by mapping images onto\r\ntheir surface (texture mapping) [7], [12]. Next, images were used to\r\napproximate global illumination effects (environment mapping) [5],\r\nand, most recently, we have seen systems where the images them-\r\nselves constitute the significant aspects of the scenes description [8].\r\nAnother reason for considering image-based rendering systems\r\nin computer graphics is that acquisition of realistic surface models is\r\na difficult problem. While geometry-based rendering technology has\r\nmade significant strides towards achieving photorealism, creating\r\naccurate models is still nearly as difficult as it was ten years ago. Tech-\r\nnological advances in three-dimensional scanning provide some\r\npromise in model building. However, they also verify our worst sus-\r\npicions the geometry of the real-world is exceedingly complex.\r\nIronically, the primary subjective measure of image quality used by\r\nproponents of geometric rendering systems is the degree with which\r\nthe resulting images are indistinguishable from photographs.\r\nOne liability of image-based rendering systems is the lack of a\r\nconsistent framework within which to judge the validity of the\r\nresults. Fundamentally, this arises from the absence of a clear prob-\r\nlem definition. Geometry-based rendering, on the other hand, has a\r\nsolid foundation; it uses analytic and projective geometry to describe\r\nthe worlds shape and physics to describe the worlds surface prop-\r\nerties and the lights interaction with those surfaces.\r\nThis paper presents a consistent framework for the evaluation\r\nof image-based rendering systems, and gives a concise problem def-\r\ninition. We then evaluate previous image-based rendering methods\r\nwithin this new framework. Finally, we present our own image-based\r\nrendering methodology and results from our prototype implementa-\r\ntion.\r\n2. THE PLENOPTIC FUNCTION\r\nAdelson and Bergen [1] assigned the name plenoptic function (from\r\nodeling:\r\nendering System\r\n and Gary Bishop\r\nmputer Science\r\nrolina at Chapel Hillany of the viewable rays by choosing an azimuth and elevation angle\r\nProceedings of SIGGRAPH 95 (Los Angeles, California, August 6-11, 1995)(,) as well as a band of wavelengths, , which we wish to consider.\r\nFIGURE 1. The plenoptic function describes all of the\r\nimage information visible from a particular viewing\r\nposition.\r\nIn the case of a dynamic scene, we can additionally choose the time,\r\nt, at which we wish to evaluate the function. This results in the fol-\r\nlowing form for the plenoptic function:\r\n(1)\r\nIn computer graphics terminology, the plenoptic function\r\ndescribes the set of all possible environment maps for a given scene.\r\nFor the purposes of visualization, one can consider the plenoptic\r\nfunction as a scene representation. In order to generate a view from\r\na given point in a particular direction we would need to merely plug\r\nin appropriate values for (Vx, Vy, Vz) and select from a range of (,)\r\nfor some constant t.\r\nWe define a complete sample of the plenoptic function as a full\r\nspherical map for a given viewpoint and time value, and an incom-\r\nplete sample as some solid angle subset of this spherical map.\r\nWithin this framework we can state the following problem def-\r\ninition for image-based rendering. Given a set of discrete samples\r\n(complete or incomplete) from the plenoptic function, the goal of\r\nimage-based rendering is to generate a continuous representation of\r\nthat function. This problem statement provides for many avenues of\r\nexploration, such as how to optimally select sample points and how\r\nto best reconstruct a continuous function from these samples.\r\n3. PREVIOUS WORK\r\n3.1 Movie-Maps\r\nThe Movie-Map system by Lippman [17] is one of the earliest\r\nattempts at constructing an image-based rendering system. In Movie-\r\nMaps, incomplete plenoptic samples are stored on interactive video\r\nlaser disks. They are accessed randomly, primarily by a change in\r\nviewpoint; however, the system can also accommodate panning, tilt-\r\ning, or zooming about a fixed viewing position. We can characterize\r\nLippmans plenoptic reconstruction technique as a nearest-neighbor\r\ninterpolation because, when given a set of input parameters (Vx, Vy,\r\nVz, , , t), the Movie-Map system can select the nearest partial sam-\r\nple. The Movie-Map form of image-based rendering can also be\r\ninterpreted as a table-based evaluation of the plenoptic function. This\r\ninterpretation reflects the database structure common to most image-\r\nbased systems.\r\n3.2 Image Morphing\r\nImage morphing is a very popular image-based rendering technique\r\n[4], [28]. Generally, morphing is considered to occur between two\r\nimages. We can think of these images as endpoints along some path\r\nthrough time and/or space. In this interpretation, morphing becomes\r\na method for reconstructing partial samples of the continuous ple-\r\nnoptic function along this path. In addition to photometric data,\r\nmorphing uses additional information describing the image flow\r\nfield. This information is usually hand crafted by an animator. At first\r\n\r\n\r\n(Vx, Vy, Vz)\r\np P    Vx Vy Vz t, , , , , ,( )=glance, this type of augmentation might seem to place it outside of\r\nthe plenoptic functions domain. However, several authors in the field\r\nof computer vision have shown that this type of image flow infor-\r\nmation is equivalent to changes in the local intensity due to\r\ninfinitesimal perturbations of the plenoptic functions independent\r\nvariables [20], [13]. This local derivative behavior can be related to\r\nthe intensity gradient via applications of the chain rule. In fact, mor-\r\nphing makes an even stronger assumption that the flow information\r\nis constant along the entire path, thus amounting to a locally linear\r\napproximation. Also, a blending function is often used to combine\r\nboth reference images after being partially flowed from their initial\r\nconfigurations to a given point on the path. This blending function\r\nis usually some linear combination of the two images based on what\r\npercentage of the paths length has been traversed. Thus, morphing\r\nis a plenoptic reconstruction method which interpolates between\r\nsamples and uses local derivative information to construct approxi-\r\nmations.\r\n3.3 View Interpolation\r\nChens and W');
INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(315, 566, 'The eyes have it: A task by data type taxonomy for information visualizations', 'The Eyes Have It: \r\nA Task by Data Type Taxonomy for Information Visualizations \r\nBen Shneiderman \r\nDepartment of Computer Science, \r\nHuman-Computer Interaction Laboratory, and Institute for Systems Research \r\nUniversity of Maryland \r\nCollege Park, Maryland 20742 USA \r\nben @ cs.umd.edu \r\nAbstract \r\nA useful starting point for designing advanced graphical \r\nuser interjaces is the Visual lnformation-Seeking Mantra: \r\noverview first, zoom and filter, then details on demand. \r\nBut this is only a starting point in trying to understand the \r\nrich and varied set of information visualizations that have \r\nbeen proposed in recent years. This paper offers a task by \r\ndata type taxonomy with seven data types (one-, two-, \r\nthree-dimensional datu, temporal and multi-dimensional \r\ndata, and tree and network data) and seven tasks (overview, \r\nZoom, filter, details-on-demand, relate, history, and \r\nextracts). \r\nEverything points to the conclusion that \r\nthe phrase the language of art is more \r\nthan a loose metaphor, that even to \r\ndescribe the visible world in images we \r\nneed a developed system of schemata. \r\nE. H. Gombrich Art and Illusion, 1959 (p. 76)  \r\nkeys), are being pushed aside by newer notions of \r\ninformation gathering, seeking, or visualization and data \r\nmining, warehousing, or filtering. While distinctions are \r\nsubtle, the common goals reach from finding a narrow set \r\nof items in a large collection that satisfy a well-understood \r\ninformation need (known-item search) to developing an \r\nunderstanding of unexpected patterns within the collection \r\n(browse) (Marchionini, 1995). \r\nExploring information collections becomes \r\nincreasingly difficult as the volume grows. A page of \r\ninformation is easy to explore, but when the information \r\nbecomes the size of a book, or library, or even larger, it \r\nmay be difficult to locate known items or to browse to \r\ngain an overview, \r\nDesigners are just discovering how to use the rapid and \r\nhigh resolution color displays to present large amounts of \r\ninformation in orderly and user-controlled ways. Perceptual \r\npsychologists, statisticians, and graphic designers (Berlin, \r\n1983; Cleveland, 1993; Tufte, 1983, 1990) offer valuable \r\nguidance about presenting static information, but the \r\nopportunity for dynamic displays takes user interface \r\ndesigners well beyond current wisdom. \r\n1. Introduction 2. Visual Information Seeking Mantra \r\nInformation exploration should be a joyous experience, \r\nbut many commentators talk of information overload and \r\nanxiety (Wurman, 1989). However, there is promising \r\nevidence that the next generation of digital libraries for \r\nstructured databases, textual documents, and multimedia \r\nwill enable convenient exploration of growing information \r\nspaces by a wider range of users. Visual language \r\nresearchers and user-interface designers are inventing \r\npowerful information visualization methods, while \r\noffering smoother integration of technology with task. \r\nThe terminology swirl in this domain is especially \r\ncolorful. The older terms of information retrieval (often \r\napplied to bibliographic and textual document systems) \r\nand database management (often applied to more structured \r\nrelational database systems with orderly attributes and sort \r\nThe success of direct-manipulation interfaces is \r\nindicative of the power of using computers in a more \r\nvisual or graphic manner. A picture is often cited to be \r\nworth a thousand words and, for some (but not all) tasks, \r\nit is clear that a visual presentation-such as a map or \r\nphotograph-is dramatically easier to use than is a textual \r\ndescription or a spoken report. As computer speed and \r\ndisplay resolution increase, information visualization and \r\ngraphical interfaces are likely to have an expanding role. If \r\na map of the United States is displayed, then it should be \r\npossible to point rapidly at one of 1000 cities to get \r\ntourist information. Of course, a foreigner who knows a \r\ncitys name (for example, New Orleans), but not its \r\nlocation, may do better with a scrolling alphabetical list. \r\n336 \r\n0-8186-7469-5/96 $05.00 0 1996 IEEE \r\nVisual displays become even more attractive to provide \r\norientation or context, to enable selection of regions, and \r\nto provide dynamic feedback for identifying changes (for \r\nexample, a weather map). Scientific visualization has the \r\npower to make ,atomic, cosmic, and common three- \r\ndimensional phenomena (such as heat conduction in \r\nengines, airflow aver wings, or ozone holes) visible and \r\ncomprehensible. 14bstract information visualization has \r\nthe power to reveal patterns, clusters, gaps, or outliers in \r\nstatistical data, stock-market trades, computer directories, \r\nor document collections. \r\nOverall, the bandwidth of information presentation is \r\npotentially higher in the visual domain than for media \r\nreaching any of the other senses. Humans have remarkable \r\nperceptual abilities, that are greatly under-utilized in current \r\ndesigns. Users can scan, recognize, and recall images \r\nrapidly, and can detect changes in size, color, shape, \r\nmovement, or texture. They can point to a single pixel, \r\neven in a megapixel display, and can drag one object to \r\nanother to perforrn an action. User interfaces have been \r\nlargely text-oriented, so as visual approaches are explored, \r\nappealing new opportunities are emerging. \r\nThere are many visual design guidelines but the basic \r\nprinciple might be: summarized as the Visual Information \r\nSeeking Mantra: \r\nOverview first, zoom and filter, then details-on-demand \r\nOverview first, zoom and filter, then details-on-demand \r\nOverview first, zoom and filter, then details-on-demand \r\nOverview first, zoom and filter, then details-on-demand \r\nOverview first, zoom and filter, then details-on-demand \r\nOverview first, zoom and filter, then details-on-demand \r\nOverview first, zoom and filter, then details-on-demand \r\nOverview first, zoom and filter, then details-on-demand \r\nOverview first, zoom and filter, then details-on-demand \r\nOverview first, zoom and filter, then details-on-demand \r\nEach line represents one project in which I found \r\nm:yself rediscoveriing this principle and therefore wrote it \r\ndown it as a reminder. It proved to be only a starting point \r\nin trying to characterize the multiple information- \r\nvisualization innovations occurring at university, \r\ngovernment, and industry research labs. \r\n3. Task by Data Type Taxonomy \r\nTo sort out the prototypes and guide researchers to new \r\nopportunities, I propose a type by task taxonomy (TTT) \r\nof information viisualizations. I assume that users are \r\nviewing collections of items, where items have multiple \r\nattributes. In all seven data types (1 - ,  2-, 3-dimensional \r\ndata, temporal and multi-dimensional data, and tree and \r\nnetwork data) the items have attributes and a basic search \r\ntask is to select all items that satisfy values of a set of \r\nattributes. An example task would be finding all divisions \r\nin an organization structure that have a budget greater than \r\n$500,000. \r\nThe data types are on the left side of the TTT \r\ncharacterize the task-domiain information objects and are \r\norganized by the problems users are trying to solve. For \r\nexample, in two-dimensilonal information such as maps, \r\nusers are trying to grasp adjacency or navigate paths, \r\nwhereas in tree-structurecl information users are trying to \r\nunderstand parent/child/sibling relationships. The tasks \r\nacross the top of the TTT are task-domain information \r\nactions that users wish to perform. \r\nThe seven tasks are at a high level of abstraction. More \r\ntasks and refinements of these tasks would be natural next \r\nsteps in expanding this table. The seven tasks are: \r\nOverview: Gain an overview of the entire collection. \r\nZoom : Zoom in on items of interest \r\nFilter: filter out uninteresting items. \r\nDetails-on-demand: Select an item or group and get \r\nRelate: View relations hips among items. \r\nHistory: Keep a history of actions to support undo, \r\nExtract: Allow extraction of sub-collections and of the \r\ndetails when needed. \r\nreplay, and progressive refinement. \r\nquery parameters. \r\nFurther discussion of the tasks follows the descriptions \r\nof the seven data types: \r\n1-dimensional: linear data types include textual \r\ndocuments, program source code, and alphabetical lists of \r\nnames which are all organized in a sequential manner. \r\nEach item in the collection is a line of text containing a \r\nstring of characters. Additional line attributes might be the \r\ndate of last update or authior name. Interface design issues \r\ninclude what fonts, color, size to use and what overview, \r\nscrolling, or selection methods can be used. User problems \r\nmight be to find the number of items, see items having \r\ncertain attributes (show only lines of a document that are \r\nsection titles, lines of a program that were changed from \r\nthe previous version, or people in a list who are older than \r\n21 years), or see an item with all its attributes. \r\nExamples: An early approach to dealing with large 1- \r\ndimensional data sets was the bifocal display which \r\nprovided detailed information in the focus area and less \r\ninformation in the surrounding context area (Spence and \r\nApperley, 1982). In their example, the selected issue of a \r\nscientific journal had detiails about each article, the older \r\nand newer issues of the journal were to the left and right \r\non the bookshelf with decreasing space. Another effort to \r\nvisualize 1-dimensional data showed the attribute values of \r\neach thousands of item in a fixed-sized space using a \r\n337 \r\nscrollbar-like display called value bars (Chimera, 1992). \r\nEven greater compressions were accomplished in compact \r\ndisplays of tens of thousands of lines of program source \r\ncode (SeeSoft, Eick et al., 1992) or textual documents \r\n(Document Lens, Robertson and Mackinlay, 1993; \r\nInformation mural, Jerding and Stasko, 1995). \r\n2-dimensional :  planar or map data include \r\ngeographic maps'),
(316, 567, 'A Fast Algorithm for Particle Simulations', 'JOURNAL OF COMPUTATIONAL PHYSICS 135, 280292 (1997)\nARTICLE NO. CP975706\nA Fast Algorithm for Particle Simulations*\nL. Greengard and V. Rokhlin\nDepartment of Computer Science, Yale University, New Haven, Connecticut 06520\nReceived June 10, 1986; revised February 5, 1987\nWe restrict our attention in this paper to the case where\nthe potential (or force) at a point is a sum of pairwise An algorithm is presented for the rapid evaluation of the potential\nand force elds in systems involving large numbers of particles interactions. More specically, we consider potentials of\nwhose interactions are Coulombic or gravitational in nature. For a\nthe form\nsystem of N particles, an amount of work of the order O(N\n2\n) has\ntraditionally been required to evaluate all pairwise interactions, un-\nF5F\nfar\n1 (F\nnear\n1F\nexternal\n),\nless some approximation or truncation method is used. The algo-\nrithm of the present paper requires an amount of work proportional\nto N to evaluate all interactions to within roundoff error, making it\nwhere F\nnear\n(when present) is a rapidly decaying potential\nconsiderably more practical for large-scale problems encountered in\n(e.g., Van der Waals), F\nexternal\n(when present) is indepen-\nplasma physics, uid dynamics, molecular dynamics, and celestial\ndent of the number of particles, and F\nfar\n, the far-eld\nmechanics. Q 1987 Academic Press\npotential, is Coulombic or gravitational. Such models de-\nscribe classical celestial mechanics and many problems in\nplasma physics and molecular dynamics. In the vortex 1. INTRODUCTION\nmethod for incompressible uid ow calculations [4], an\nThe study of physical systems by means of particle simu-\nimportant and expensive portion of the computation has\nlations is well established in a number of elds and is\nthe same formal structure (the stream function and the\nbecoming increasingly important in others. The most classi-\nvorticity are related by Poissons equation).\ncal example is probably celestial mechanics, but much re-\nIn a system of N particles, the calculation of F\nnear\nre-\ncent work has been done in formulating and studying parti-\nquires an amount of work proportional to N, as does the\ncle models in plasma physics, uid dynamics, and molecular\ncalculation of F\nexternal\n. The decay of the Coulombic or\ndynamics [5].\ngravitational potential, however, is sufciently slow that\nThere are two major classes of simulation methods. Dy-\nall interactions must be accounted for, resulting in CPU\nnamical simulations follow the trajectories of N particles\ntime requirements of the order O(N\n2\n). In this paper a\nover some time interval of interest. Given initial positions\nmethod is presented for the rapid (order O(N)) evaluation\nhx\ni\nj and velocities, the trajectory of each particle is gov-\nof these interactions for all particles.\nerned by Newtons second law of motion,\nThere have been a number of previous efforts aimed\nat reducing the computational complexity of the N-body\nproblem. Particle-in-cell methods [5] have received careful\nm\ni\nd\n2\nx\ni\ndt\n2\n52=\ni\nF for i 5 1, ..., N,\nstudy and are used with much success, most notably in\nplasma physics. Assuming the potential satises Poissons\nequation, a regular mesh is layed out over the computa-\nwhere m\ni\nis the mass of the ith particle and the force is\ntional domain and the method proceeds by:\nobtained from the gradient of a potential function F. When\none is interested in an equilibrium conguration of a set (1) interpolating the source density at mesh points,\nof particles rather than their time-dependent properties,\n(2) using a fast Poisson solver to obtain potential\nan alternative approach is the Monte Carlo method. In\nvalues on the mesh,\nthis case, the potential function F has to be evaluated for\n(3) computing the force from the potential and inter-\na large number of congurations in an attempt to deter-\npolating to the particle positions.\nmine the potential minimum.\nThe complexity of these methods is of the order\nO(N 1 M log M), where M is the number of mesh points.\nReprinted from Volume 73, Number 2, December 1987, pages 325348.\nThe number of mesh points is usually chosen to be propor-\n* The authors were supported in part by the Ofce of Naval Research\nunder Grant N00014-82-K-0184. tional to the number of particles, but with a small constant\n280\n0021-9991/97 $25.00\nCopyright \0 1987 by Academic Press\nAll rights of reproduction in any form reserved.ALGORITHM FOR PARTICLE SIMULATION 281\nof proportionality so that M ! N. Therefore, although the f\nx\n0\n(x, y)52log(ix 2 x\n0\ni)\nasymptotic complexity for the method is O(N log N), the\ncomputational cost in practical calculations is usually ob-\nand\nserved to be proportional to N. Unfortunately, the mesh\nprovides limited resolution, and highly nonuniform source\nE\nx\n0\n(x, y) 5\n(x 2 x\n0\n)\nix 2 x\n0\ni\n2\n,\ndistributions cause a signicant degradation of perfor-\nmance. Further errors are introduced in step (3) by the\nnecessity for numerical differentiation to obtain the force.\nrespectively.\nTo improve the accuracy of particle-in-cell calculations,\nIt is well known that f\nx\n0\nis harmonic in any region not\nshort-range interactions can be handled by direct computa-\ncontaining the point x\n0\n. Moreover, for every harmonic\ntion, while far-eld interactions are obtained from the\nfunction u, there exists an analytic function w: C R C such\nmesh, giving rise to so-called particleparticle/particle\nthat u(x, y) 5 Re(w(x, y)) and w is unique except for an\nmesh (P\n3\nM) methods [5]. For an implementation of these\nadditive constant. In the remainder of the paper we will\nideas in the context of vortex calculations, see [1]. While\nwork with analytic functions, making no distinction be-\nthese algorithms still depend for their efcient perfor-\ntween a point (x, y) [ R\n2\nand a point x 1 iy 5 z [ C.We\nmance on a reasonably uniform distribution of particles,\nnote that\nin theory they do permit arbitrarily high accuracy to be\nobtained. As a rule, when the required precision is rela-\nf\nx\n0\n(x) 5 Re(2log(z 2 z\n0\n)),\ntively low, and the particles are distributed more or less\nuniformly in a rectangular region, P\n3\nM methods perform\nand, following standard practice, we will refer to the ana-\nsatisfactorily. However, when the required precision is high\nlytic function log(z) as the potential due to a charge. As\n(as, for example, in the modeling of highly correlated sys-\nwe develop expressions for the potential due to more com-\ntems), the CPU time requirements of such algorithms tend\nplicated charge distributions, we will continue to use com-\nto become excessive.\nplex notation and will refer to the corresponding analytic\nAppel [2] introduced a gridless method for many-\nfunctions themselves as the potentials. The following\nbody simulation with a computational complexity esti-\nlemma is an immediate consequence of the Cauchy\nmated to be of the order O(N log N). It relies on using a\nRiemann equations.\nmonopole (center-of-mass) approximation for computing\nforces over large distances and sophisticated data struc-\nLEMMA 2.1. If u(x, y) 5 Re(w(x, y)) describes the poten-\ntures to keep track of which particles are sufciently clus-\ntial eld at (x, y), then the corresponding force eld is\ntered to make the approximation valid. For certain types\ngiven by\nof problems, the method achieves a dramatic speedup,\ncompared to the naive O(N\n2\n) approach. It is less efcient\n=u 5 (u\nx\n, u\ny\n) 5 (Re(w9), 2 Im(w9)),\nwhen the distribution of particles is relatively uniform and\nthe required precision is high.\nwhere w9 is the derivativeofw.\nThe algorithm we present uses multipole expansions to\ncompute potentials or forces to whatever precision is re-\nThe following lemma is used in obtaining the multipole\nquired, and the CPU time expended is proportional to N.\nexpansion for the eld due to m charges.\nThe approach we use is similar to the one introduced in\nLEMMA 2.2. Let a point charge of intensity q be located\n[7] for the solution of boundary value problems for the\nat z\n0\n. Then for any z such that uzu . uz\n0\nu,\nLaplace equation. In the following section, we describe the\nnecessary analytical tools, while Section 3 is devoted to a\ndetailed description of the method.\nf\nz\n0\n(z) 5 q log(z 2 z\n0\n) 5 q\nS\nlog(z) 2\nO\ny\nk51\n1\nk\nS\nz\n0\nz\nD\nk\nD\n. (2.1)\n2. PHYSICAL AND MATHEMATICAL PRELIMINARIES\nProof. Note rst that log(z 2 z\n0\n) 2 log(z) 5 log(1 2\nIn this paper, we consider a two-dimensional physical\nz\n0\n/z) and that uz\n0\n/zu , 1. The lemma now follows from\nmodel which consists of a set of N charged particles with\nthe expansion\nthe potential and force obtained as the sum of pairwise\ninteractions from Coulombs law. Suppose that a point\ncharge of unit strength is located at the point (x\n0\n, y\n0\n) 5\nlog(1 2 g) 5 (21)\nO\ny\nk51\nw\nk\nk\n,\nx\n0\n[ R\n2\n. Then, for any x 5 (x, y) [ R\n2\nwith x ? x\n0\n,\nthe potential and electrostatic eld due to this charge are\ndescribed by the expressions which is valid for any w such that uwu , 1.282 GREENGARD AND ROKHLIN\nTHEOREM 2.1. (Multipole expansion). Suppose that m\ncharges of strengths hq\ni\n, i 5 1, ..., mj are located at points\nhz\ni\n, i 5 1, ..., mj, with uz\ni\nu , r. Then for any z [ C with\nuzu . r, the potential f(z) is given by\nf(z) 5 Q log(z) 1\nO\ny\nk51\na\nk\nz\nk\n, (2.2)\nFIG. 1. Well-separated sets in the plane.\nwhere\nQ 5\nO\nm\ni51\nq\ni\n, a\nk\n5\nO\nm\ni51\n2q\ni\nz\nk\ni\nk\n. (2.3)\nthat hy\n1\n, y\n2\n, ..., y\nn\nj is another set of points in C (Fig. 1).\nWe say that the sets hx\ni\nj and hy\ni\nj are well separated if there\nexist points x\n0\n, y\n0\n[ C and a real r . 0 such that\nFurthermore, for any p $ 1,\nux\ni\n2 x\n0\nu , r for all i 5 1, ..., m,\nuy\nj\n2 y\n0\nu , r for all j 5 1, ..., n,\nU\nf(z) 2 Q log(z) 2\nO\np\nk51\na\nk\nz\nk\nU\n# a\nU\nr\nz\nU\np11\n#\nS\nA\nc 2 1\nDS\n1\nc\nD\np\n,\nux\n0\n2 y\n0\nu . 3r.\n(2.4)\nIn order to obtain the potential (or force) at the points\nhy\nj\nj due to the charges at the points hx\ni\nj directly, we\nwhere\ncould compute\nc 5\nU\nz\nr\nU\n, A 5\nO\nm\ni51\nuq\ni\nu, and a 5\nA\n1 2 ur/zu\n. (2.5)\nO\nm\ni51\nf\nx\ni\n(y\nj\n) for all j 5 1, ..., n. (2.7)\nProof. The form of the multipole expansion (2.2) is an\nT'),
(317, 568, 'A data locality optimizing algorithm', 'A Data Locality Optimizing Algorithm\r\nMichael E. Wolf and Monica S. Lam\r\nComputer Systems Laboratory\r\nStanford University, CA 94305\r\nAbstract\r\nThis paper proposes an algorithm that improves the local-\r\nity of a loop nest by transforming the code via interchange,\r\nreversal, skewing and tiling. The loop transformation al-\r\ngorithm is based on two concepts: a mathematical for-\r\nmulation of reuse and locality, and a loop transformation\r\ntheory that unifies the various transforms as unimodular\r\nmatrix transformations.\r\nThe algorithm has been implemented in the SUIF (Stan-\r\nford University Intermediate Format) compiler, and is suc-\r\ncessful in optimizing codes such as matrix multiplica-\r\ntion, successive over-relaxation (SOR), LU decomposition\r\nwithout pivoting, and Givens QR factorization. Perfor-\r\nmance evaluation indicates that locality optimization is es-\r\npecially crucial for scaling up the performance of parallel\r\ncode.\r\n1 Introduction\r\nAs processor speed continues to increase faster than me-\r\nmory speed, optimizations to use the memory hierarchy\r\nefficiently become ever more important. Blocking [9] or\r\ntiling [18] is a well-known technique that improves the\r\ndata locality of numerical algorithms [1, 6, 7, 12, 13].\r\nTiling can be used for different levels of memory hierarchy\r\nsuch as physical memory, caches and registers; multi-level\r\ntiling can be used to achieve locality in multiple levels of\r\nthe memory hierarchy simultaneously.\r\nTo illustrate the importance of tiling, consider the ex-\r\nample of matrix multiplication:\r\nfor I1 := 1 to n\r\nfor I2 := 1 to n\r\nfor I3 := 1 to n\r\nThis research was supported in part by DARPA contract N00014-87-K-\r\n0828.\r\nC[I1,I3] += A[I1,I2] * B[I2,I3];\r\nIn this code, although the same row of C and B are reused\r\nin the next iteration of the middle and outer loop, respec-\r\ntively, the large volume of data used in the intervening\r\niterations may replace the data from the register file or the\r\ncache before it can be reused. Tiling reorders the execu-\r\ntion sequence such that iterations from loops of the outer\r\ndimensions are executed before completing all the itera-\r\ntions of the inner loop. The tiled matrix multiplication\r\nis\r\nfor II2 := 1 to n by s\r\nfor II3 := 1 to n by s\r\nfor I1 := 1 to n\r\nfor I2 := II2 to min(II2 + s \0 1,n)\r\nfor I3 := II3 to min(II3 + s \0 1, n)\r\nC[I1,I3] += A[I1,I2] * B[I2,I3];\r\nTiling reduces the number of intervening iterations and\r\nthus data fetched between data reuses. This allows reused\r\ndata to still be in the cache or register file, and hence\r\nreduces memory accesses. The tile size s can be chosen\r\nto allow the maximum reuse for a specific level of memory\r\nhierarchy.\r\nThe improvement obtained from tiling can be far greater\r\nthan from traditional compiler optimizations. Figure 1\r\nshows the performance of 500  500 matrix multiplica-\r\ntion on an SGI 4D/380 machine. The SGI 4D/380 has\r\neight MIPS/R3000 processors running at 33 Mhz. Each\r\nprocessor has a 64 KB direct-mapped first-level cache and\r\na 256 KB direct-mapped second-level cache. We ran four\r\ndifferent experiments: without tiling, tiling to reuse data\r\nin caches, tiling to reuse data in registers [5], and tiling\r\nfor both register and caches. For cache tiling, the data are\r\ncopied into consecutive locations to avoid cache interfer-\r\nence [12].\r\nTiling improves the performance on a single processor\r\nby a factor of 2:75. The effect of tiling on multiple pro-\r\ncessors is even more significant since it not only reduces\r\nthe average data access latency but also the required me-\r\nmory bandwidth. Without cache tiling, contention over the\r\n both tiling\r\n\r\n cache tiling\r\n\r\n register tiling\r\n\r\n no tiling\r\n|\r\n0\r\n|\r\n1\r\n|\r\n2\r\n|\r\n3\r\n|\r\n4\r\n|\r\n5\r\n|\r\n6\r\n|\r\n7\r\n|\r\n8\r\n|0\r\n|5\r\n|10\r\n|15\r\n|20\r\n|25\r\n|30\r\n|35\r\n|40\r\n|45\r\n|50\r\n|55\r\n|60\r\n Processors\r\n \r\nM\r\nFl\r\nop\r\ns\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 1: Performance of 500  500 double precision ma-\r\ntrix multiplication on the SGI 4D/380. Cache tiles are 64\r\n 64 iterations and register tiles are 4  2.\r\nmemory bus limits the speedup to about 4.5 times. Cache\r\ntiling permits speedups of over seven for eight processors,\r\nachieving an impressive speed of 64 MFLOPS when com-\r\nbined with register tiling.\r\n1.1 The Problem\r\nThe problem addressed in this paper is the use of loop\r\ntransformations such as interchange, skewing and reversal\r\nto improve the locality of a loop nest. Matrix multipli-\r\ncation is a particularly simple example because it is both\r\nlegal and advantageous to tile the entire nest. In general,\r\nit is not always possible to tile the entire loop nest. Some\r\nloop nests may not be tilable. Sometimes it is necessary\r\nto apply transformations such as interchange, skewing and\r\nreversal to produce a set of loops that are both tilable and\r\nadvantageous to tile.\r\nFor example, consider the example of an abstraction of\r\nhyperbolic PDE code in Figure 2(a). Suppose the array\r\nin this example is larger than the memory hierarchy level\r\nof interest; the entire array must be fetched anew for each\r\niteration of the outermost loop. Due to dependences, the\r\nloops must first be skewed before they can be tiled. This\r\nis also equivalent to finding non-rectangular tiles. Figure 2\r\ncontains the entire derivation of the tiled code, which we\r\nwill use to illustrate our locality algorithm in the rest of\r\nthe paper.\r\nThere are two major representations used in loop trans-\r\nformations: distance vectors and direction vectors [2, 17].\r\nLoops whose dependences can be summarized by distance\r\nvectors are special in that it is advantageous, possible and\r\neasy to tile all loops [10, 15]. General loop nests, whose\r\ndependences are represented by direction vectors, may not\r\nbe tilable in their entirety. The data locality problem ad-\r\ndressed in this paper is to find the best combination of\r\nloop interchanges, skewing, reversal and tiling that max-\r\nimizes the data locality within loop nests, subject to the\r\nconstraints of direction and distance vectors.\r\nResearch has been performed on both the legality and\r\nthe desirability of loop transformations with respect to data\r\nlocality. Early research on optimizing loops with direction\r\nvectors concentrated on the legality of pairwise transfor-\r\nmations, such as when it is legal to interchange a pair of\r\nloops. However, in general, it is necessary to apply a series\r\nof primitive transformations to achieve goals such as par-\r\nallelism and data locality. This has led to work on combi-\r\nnations of primitive transforms. For example, Wolfe [18]\r\nshows how to determine when a loop nest can be tiled;\r\ntwo-dimensional tiling can be achieved via a pair of trans-\r\nformations known as strip-mine and interchange [14]\r\nor unroll and jam [5]. Wolfe also shows that skewing\r\ncan make a pair of loops tilable. Banerjee discusses gen-\r\neral unimodular transforms for two-deep loop nests [4]. A\r\ntechnique used in practice to handle general n-dimensional\r\nloop nests is to determine a priori the sequence of loop\r\ntransforms to attempt. This technique is inadequate be-\r\ncause certain transformations, such as loop skewing, may\r\nnot improve code, but may enable other optimizations that\r\ndo so. Which of these to perform will depend on which\r\nother optimizations will be enabled: the desirablity of a\r\ntransformation cannot be evaluated locally. Furthermore,\r\nthe correct ordering of optimizations are highly program\r\ndependent.\r\nOn the desirability of tiling, previous work concentrated\r\non how to determine the cache performance and tune the\r\nloop parameters for a given loop nest. Porterfield gives an\r\nalgorithm for estimating the hit rate of a fully-associative\r\nLRU (least recently used replacement policy) cache of a\r\ngiven size [14]. Gannon et al. uses reference windows\r\nto determine the minimum memory locations necessary to\r\nmaximize reuse in a loop nest [8]. These evaluation func-\r\ntions are useful for comparing the locality performance af-\r\nter applying transformations, but do not suggest the trans-\r\nformations to apply when a series of transformations may\r\nfirst need to be applied before tiling becomes feasible and\r\nuseful.\r\nIf we were to use these evaluation functions to find\r\nthe suitable transformations, we would need to search the\r\ntransformation space exhaustively. The previously pro-\r\nposed method of enumerating the all possible combina-\r\ntions of legal transformations is expensive and not even\r\npossible if there are infinitely many combinations, as is\r\nthe case when we include skewing.\r\n(a): Extract dependence information\r\nfor I1 := 0 to 5 do\r\nfor I2 := 0 to 6 do\r\nA[I2 + 1] := 1/3 * (A[I2] + A[I2 + 1] + A[I2 + 2]);\r\nD = f(0; 1); (1; 0); (1;\01)g.\r\n(b): Extract locality information\r\nUniformly generated set = fA[I I + I + g\r\nreuse category\r\nreuse vector\r\nself-temporal\r\nspanf(1; 0)g s\r\ns lf-spatial\r\nspanf(1; 0); ( ; )g l\r\ngroup\r\nspanf(1; 0); ( ; )g\r\nfI ; I g\r\nf( ; )g T =\r\n \r\n=l\r\nf( ; )g\r\nf( ; ); ( ; )g T =\r\n \r\n=(ls)\r\nI I\r\nI\r\n0\r\nI\r\n0\r\nI\r\n0\r\nI\r\n0\r\nI\r\n0\r\n\0 I\r\n0\r\n+ I\r\n0\r\n\0 I\r\n0\r\nI\r\n0\r\n\0 I\r\n0\r\n+ I\r\n0\r\n\0 I\r\n0\r\n+\r\nT =\r\n \r\nD\r\n0\r\n= TD = f( ; ); ( ; ); ( ; )g\r\nII\r\n0\r\nI\r\n0\r\nI\r\n0\r\nI\r\n0\r\nII\r\n0\r\nI\r\n0\r\nII\r\n0\r\n+\r\nI\r\n0\r\n\0 I\r\n0\r\n+ I\r\n0\r\n\0 I\r\n0\r\nI\r\n0\r\n\0 I\r\n0\r\n+ I\r\n0\r\n\0 I\r\n0\r\n+\r\n1.2 An Overview\r\nThis paper focuses on maximizing data locality at the\r\ncache level. Although the basic principles in memory hi-\r\nerarchy optimization are similar for all levels, each level\r\nhas slightly different characteristics, requiring slightly dif-\r\nferent considerations. Caches usually have small set as-\r\nsociativity, so cache data conflicts can cause desired data\r\nto be replaced. We have found that the performance of\r\ntiled code fluctuates dramatically with the size of the data\r\nmatrix, due to cache interference [12]. We show that this\r\neffect can be mitigated by copying reused data to consecu-\r\ntive locations before the computation, or choosing the tile\r\nsize according to the matrix size. Both of these optimiza-\r\ntions can be performed after code transformation, and thus\r\ncache interference need not be'),
(318, 569, 'Algorithmic mechanism design', 'Games and Economic Behavior 35, 166196 (2001)\ndoi:10.1006/game.1999.0790, available online at http://www.idealibrary.com on\nAlgorithmic Mechanism Design\n\nNoam Nisan\n\nInstitute of Computer Science, Hebrew University of Jerusalem,\nGivat Ram 91904, Israel\nand\nSchool of Computer Science, IDC, Herzliya\nand\nAmir Ronen\n\nInstitute of Computer Science, Hebrew University of Jerusalem,\nGivat Ram 91904, Israel\nReceived February 9, 1999; published online January 24, 2001\nWe consider algorithmic problems in a distributed setting where the participants\ncannot be assumed to follow the algorithm but rather their own self-interest. As\nsuch participants, termed agents, are capable of manipulating the algorithm, the\nalgorithm designer should ensure in advance that the agents interests are best\nserved by behaving correctly. Following notions from the eld of mechanism design,\nwe suggest a framework for studying such algorithms. Our main technical contri-\nbution concerns the study of a representative task scheduling problem for which\nthe standard mechanism design tools do not sufce. Journal of Economic Literature\nClassication Numbers: C60, C72, D61, D70, D80.  2001 Academic Press\n1. INTRODUCTION\n1.1. Motivation\nA large part of research in computer science is concerned with protocols\nand algorithms for inter-connected collections of computers. The designer\nof such an algorithm or protocol always makes an implicit assumption that\n\nThis research was supported by grants from the Israeli ministry of Science and the Israeli\nacademy of sciences.\n\nE-mail: noam@cs.huji.ac.il.\n\nTo whom correspondence should be addressed. E-mail: amiry@cs.huji.ac.il.\n166\n0899-8256/01 $35.00\nCopyright 2001 by Academic Press\nAll rights of reproduction in any form reserved.algorithmic mechanism design 167\nthe participating computers will act as instructedexcept, perhaps, for the\nfaulty or malicious ones.\nWith the emergence of the Internet as the platform of computation, this\nassumption can no longer be taken for granted. Computers on the Internet\nbelong to different persons or organizations and will likely do what is most\nbenecial to their owners. We cannot simply expect each computer on the\nInternet to faithfully follow the designed protocols or algorithms. It is more\nreasonable to expect that each computer will try to manipulate it for its\nowners benet. Such an algorithm or protocol must therefore be designed\nin advance for this kind of behavior! Let us sketch two example applications\nwe have in mind.\nLoadBalancing. The aggregate power of all computers on the Internet\nis huge. In a dream world this aggregate power will be optimally allocated\nonline among all connected processors. One could imagine CPU-intensive\njobs automatically migrating to CPU-servers, caching automatically done\nby computers with free disk space, etc. Access to data, communication\nlines, and even physical attachments (such as printers) could all be allo-\ncated across the Internet. This is clearly a difcult optimization problem\neven within tightly linked systems, and is addressed, in various forms and\nwith varying degrees of success, by all distributed operating systems. The\nsame type of allocation over the Internet requires handling an additional\nproblem: the resources belong to different parties who may not allow others\nto freely use them. The algorithms and protocols may, thus, need to provide\nsome motivation for these owners to play along.\nRouting. When one computer wishes to send information to another,\nthe data usually gets routed through various intermediate routers. So far\nthis has been done voluntarily, probably due to the low marginal cost of\nforwarding a packet. However, when communication of larger amounts of\ndata becomes common (e.g., video), and bandwidth needs to be reserved\nunder various quality of service (QoS) protocols, this altruistic behavior of\nthe routers may no longer hold. If so, we will have to design protocols\nspecically taking the routers self-interest into account.\n1.2. ThisWork\nIn this paper we propose a formal model for studying algorithms that\nassumes that the participants all act according to their own self-interest.\nWe adopt a rationality-based approach, using notions from game theory\nand micro-economics, and in particular from the eld of mechanism design.\nWe assume that each participant has a well-dened utility function\n1\nthat\n1\nThis notion from micro-economics is often used in mechanism design.168 nisan and ronen\nrepresents its preference over the possible outputs of the algorithm, and we\nassume that participants act as to rationally optimize their utility. We term\nsuch rational and selsh participants agents.\n2\nThe solutions we consider\ncontain both an algorithmic ingredient (obtaining the intended results), and\na payment ingredient that motivates the agents. We term such a solution a\nmechanism.\n3\nOur contributions in this work are as follows:\n1. We present a formal model for studying optimization problems.\nThe model is based on the eld of mechanism design.\n4\nA problem in this\nmodel has, in addition to the output specication, a description of the\nagents utilities. The mechanism has, in addition to the algorithm produc-\ning the desired output, payments to the participating agents. An exposition\nof applying several classic notions from mechanism design in our model\nappears in Nisan (1999).\n2. We observe that the known techniques from mechanism design\nprovide solutions for several basic optimization problems, and in particular\nfor the shortest path problem, where each edge may belong to a different\nagent.\n3. We study a basic problem, task scheduling, which requires new\ntechniques, and prove the following:\n We design ann-approximation mechanism, wheren is the number\nof agents.\n We prove a lower bound of 2 to the approximation ratio that\ncan be achieved by any mechanism. This bound is tight for the case of\ntwo agents, but leaves a gap for more agents. We conjecture that the\nupper bound is tight in general and prove it for two restricted classes of\nmechanisms.\n We design a randomized mechanism that beats the deterministic\nlower bound.\n4. We extend the basic model, formalizing a model where the mech-\nanism has more information. We call this model a mechanism with veri-\ncation and argue that it is justied in certain applications.\n2\nThe term is taken from the distributed AI community which has introduced the usage of\nmechanism design in a computational setting. We use it, however, in a much more restricted\nand well-dened sense.\n3\nThis is the standard term used in mechanism design.\n4\nWe are not the rst to use notions from mechanism design in a computational setting. See\nSection 1.3.algorithmic mechanism design 169\n5. We study the task scheduling problem in the extended model and\nobtain two main results:\n An optimal mechanism with verication for task scheduling (that\nrequires exponential computation time).\n A polynomial time \0 1+\0 \0 -approximation mechanism with veri-\ncation for a sub-case of the problem.\nA preliminary version of this paper appeared at the 31st annual sympo-\nsium on theory of computing (Nisan and Ronen (1999)).\n1.3. ExtantWork\nThere have been many works that tried to introduce economic or game-\ntheoretic aspects into computational questions (see, e.g., Lamport et al.\n1982; Ferguson et al., 1995; Huberman and Hogg, 1995; Papadimitriou and\nYannakakis, 1991, 1993; and a survey by Lineal, 1994. Mostof t hese were\nnot aimed at the problem of the cooperation of selsh entities, and those\nthat were (Monderer and Tennenholtz, forthcoming; Papadimitriou, 1996;\nKorilis et al.,1991;andSandholm,1996didnotpursueourdirection.Many\nsubelds of game theory and economics are also related to our work (see,\ne.g., Mas-Collel et al., 1995, Chapters 14, 21, and 22). We list below the\nresearch work t hatis mostrelevantt o our direct ion.\nMechanism Design. The eld of mechanism design(also known as imple-\nmentation theory) aims to study how privately known preferences of many\npeople can be aggregated toward a social choice. The main motivation\nof this eld is micro-economic, and the tools are game-theoretic. Emphasis\nis put on the implementation of various types of auctions. In the last few\nyears this eld has received much interest, especially due to its inuence on\nlarge privatizations and specturm allocations (McMillan, 1994). An intro-\nduction to this eld can be found in Mas-Collel et al. (1995, Chapter 23),\nOsborne and Rubistein (1994, Chapter 10) and an inuential web site at\nhttp://www.market-design.com.\nDistributedAI. In the last decade or so, researchers in AI have stud-\nied cooperation and competition among software agents. The meaning\nof agents here is very broad, incorporating attributes of code-mobility,\narticial-intelligence, user-customization, and self-interest. A subeld of\nthis general direction of research takes a game-theoretic analysis of agents\ngoals, and in particular uses notions from mechanism design (Rosenschein\nand Zlotkin, 1994; Sandholm 1996; Ephrati and Rosenschein, 1991; and\nShoham and Tanaka, 1997). A related subeld of Distributed AI, some-\ntimes termed market-based computation (Walsh et al., 1998; Ferguson et al.,\n1995; and Walsh and Wellman, 1998), aims to leverage the notions of free170 nisan and ronen\nmarkets in order to solve distributed problems. These subelds of DAI are\nrelated to our work.\nCommunication Networks. In recentyears researchers in t he eld of\nnetwork design adopted a game-theoretic approach (see, e.g., Korilis et al.,\n1991). In particular, mechanism design was applied to various problems\nincluding resource allocation (Lazar and Semret, 1998), cost sharing, and\npricing (Shenkar et al., 1996).\nScheduling. The specic problem we address is the minimization of the\nmake-span of independent tasks on unrelated parallel machines, which was\nextensively studied from an algorithmic point of view. It is known that solv-\ning the problem or even approximating it within a factor of 3/2\0 is NP-\nhard, but a polynomial-time 2-approximation exists (Le');
INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(319, 570, 'Factor Graphs and the Sum-Product Algorithm', 'F actor Graphs and the Sumro duct Algorithm F rank R Ksc hisc hang Brendan J F rey y Hansndrea Lo eliger z July Abstract factor graph is a bipartite graph that expresses ho w a lobal function of man y v ariables factors in to a pro duct of o cal functions F actor graphs subsume man y other graphical mo dels including Ba y esian net w orks Mark o v random lds and T anner graphs F ollo wing one simple computational rule the sumro duct algorithm op erates in factor graphs to computeither exactly or appro ximately arious marginal functions b y distributed messageassing in the graph A wide v ariet y of algorithms dev elop ed in artiial in telligence signal pro cessing and digital comm uni cations can b e deriv ed as sp e ci instances of the sumro duct algorithm including the forw ardac kw ard algorithm the Viterbi algorithm the iterativ e urb o deco ding algorithm P earl b elief propagation algorithm for Ba y esian net w orks the Kalman ter and certain fast F ourier transform algorithms Keyw ords raphical mo dels factor graphs T anner graphs sumro duct algorithm marginalization forw ardac kw ard algorithm Viterbi algorithm iterativ e deco ding b e lief propagation Kalman tering fast F ourier transform Submitted to IEEE T ransactions on Information Theory July This pap er is a v ailable on the w eb at httpw w omm t or ont o a ra nk fac to r Departmen t of Electrical Computer Engineering Univ ersit y of T oron to T oron to On tario M CANAD A frankommtoronto ca y The Bec kman Institute North Mathews Av en ue Urbana IL USA freystoronto a z Endora T ec h A G Gartenstrasse CH Basel SWITZERLAND haloeligercces sh iT yp eset with L A T E X at on July iiCon ten ts In tro duction F actor Graphs Prior Art A Sumro duct Algorithm Example Notational Preliminaries Organization of the P ap er Examples of F actor Graphs Indicating Set Mem b ership T anner Graphs Probabilit y Distributions F urther Examples F unction Crossections Pro jections and Summaries Crossections Pro jections and Summaries Summary Op erators via Binary Op erations The Sumro duct Algorithm Computation b y Message assing The Sumro duct Up date Rule Message P assing Sc hedules The Sumro duct Algorithm in a Finite T ree The Articulation Principle Generalized F orw ardac kw ard Sc hedules An Example on tin ued iiiF orests Message Seman tics under General Sc hedules Applications of the Sumro duct Algorithm The F orw ardac kw ard Algorithm The Minum Semiring and the Viterbi Algorithm Iterativ e Deco ding of T urb oik e Co des Belief Propagation in Ba y esian Net w orks Kalman Filtering F actor Graph T ransformations and Coping with Cycles Grouping Lik e No des Multiplying b y Unit y Stretc hing V ariable No des Spanning T rees An FFT Conclusions A Pro of of Theorem B Complexit y of the Sumro duct Algorithm in a Finite T ree C Co dep eci Simpliations ivIn tro duction F actor Graphs A factor gr aph is a bipartite graph that expresses ho w a lobal function of man y v ari ables factors in to a pro duct of o cal functions Supp ose e that some real alued function g x x x x x of e v ariables can b e written as the pro duct g x x x x x f A x f B x f C x x x f D x x f E x x of e functions f A f B f E The corresp onding factor graph is sho wn in Fig a There is a variable no de for eac h v ariable there is a function no de for eac h factor and the v ariable no de for x i is connected to the function no de for f if and only if x i is an argumen t of f f D f C f B f A x x x x x f E f g f g f g f g f g Figure A facto r graph that exp resses that a global function facto rs as the p ro duct of lo cal functions f A x f B x f C x x x f D x x f E x x V a riableunction view indexubset view V a riable no des a re sho wn as circles function no des a re sho wn as led squa res W e will use the follo wing notation Let X S f x i i S g b e a collection of v ariables indexed b y a ite set S where S is linearly ordered b y F or eac h i S the v ariable x i tak es on v alues from some set A i Most often S will b e a subset of the in tegers with the usual ordering If E is a subset of S then w e denote b y X E f x i i E g to b e the subset of v ariables indexed b y E A particular assignmen t of a v alue to eac h of the v ariables of X S will b e referred to as a c onur ation of the v ariables Conurations of the v ariables can b e view ed as b eing elemen ts of the Cartesian pro duct A S Q i S A i called the c onur ation sp ac e F or concreteness w e supp ose that the comp onen ts of conurations are ordered as in S so that if S f i i i N g with i i i N then a t ypical conuration a is written as a a i a i a i with a i A i for j N Of course the conuration a is equiv alen t to the m ultiple v ariable assignmen t x i a i x i a i and vic e versa By abuse of notation if a is a particular conuration w e will write X S a for this assignmen t W e will ha v e o ccasion to view conurations b oth as assignmen ts of v alues to v ariables and as elemen ts of A SW e will also ha v e o ccasion to consider sub c onur ations if E f j j j M g S with j j j M and a is an y conuration then the M uple a E a j a j is called the sub conuration of a with resp ect to E The set f a E a A S g of all sub conurations with resp ect to E is denoted b y A E clearly A E Q i E A i Again b y abuse of notation if a E is a particular sub conuration with resp ect to E w e will write X E a E for the m ultiple v ariable assignmen t x j a j x j a j etc Finally if C A S is some set of conurations w e will denote b y C E the set of sub conurations of the elemen ts of C with resp ect to E i C E f a E a C g Clearly C E A E Let g A S R b e a function with the elemen ts of X S as argumen ts F or the momen t w e require the domain R of g to b e equipp ed with a binary pro duct enoted and a unit elemen t enoted satisfying for all u v and w in R u u u v v u u v w u v w so that R forms a comm utativ e semigroup with unit y The reader will lose nothing essen tial in most cases b y assuming that R is a ld e the real n um b ers under the usual pro duct W e will usually denote the pro duct of elemen ts x and y b y the juxtap osition xy and only o ccasionally as x y Supp ose for some collection Q of subsets of S that the function g factors as g X S Y E Q f E X E where for eac h E Q f E A E R is a function of the sub conurations with resp ect to E W e refer to eac h factor f E X E in as a lo c al function f some E Q is empt y i E w e in terpret the corresp onding lo cal unction f with no argumen ts as a constan t in R Often as is common practice in probabilit y theory w e will use an abbreviated notation in whic h the argumen ts of a function determine the function domain so that e f x x w ould denote a function from A A R as w ould f x x In this abbreviated notation w ould b e written as g X S Q E Q f X E A factor graph represen tation of is a bipartite graph denoted F S Q with v ertex set S Q and edge set ff i E g i S E Q i E g In w ords F S Q con tains an edge f i E g if and only if i E i if and only if x i is an argumen t of the lo cal function f E Those v ertices that are elemen ts of S are called variable no des and those v ertices that are elemen ts of Q are called function no des F or example in w e ha v e S f g and Q ff g f g f g f g f gg whic h giv es the factor graph F S Q sho wn in Fig b Throughout w e will translate freely b et w een factor graphs lab eled with v ariables and lo cal functions he ariableo cal function view of Fig a and the cor resp onding factor graph lab eled with v ariable indices and index subsets he ndexubset view of Fig b T o a v oid the more precise but often quite tedious men tion of functionsorresp onding to function no des and v ariables orresp onding to v ariable no des w e will blur the distinction b et w een the no des and the ob jects asso ciated with them thereb y making it legitimate to refer to sa y the argumen ts of a function no de f or the edges inciden t on a v ariable x i It will often b e useful to refer to an arbitrary edge of a factor graph Suc h an edge f v w g b y deition is inciden t on a function no de and a v ariable no de the latter is called the v ariable asso ciate d with the giv en edge and is denoted b y x f v g Prior Art W e will see in Section that factor graphs subsume man y other graphical mo dels in signal pro cessing probabilit y theory and co ding including Mark o v random lds Ba y esian net w orks and T anner graphs Our original motiv ation for in tro ducing factor graphs w as to mak e explicit the commonalitie s b et w een Ba y esian net w orks lso kno wn as b elief net w orks causal net w orks and inence diagrams and T anner graphs b oth of whic h had previously b een used to explain the iterativ e deco ding of turb o co des and lo wensit y parit y c hec k co des In that resp ect factor graphs and their applications to co ding are just a sligh t reform ulation of the approac h of Wib erg et al Ho w ev er a main thesis of this pap er is that factor graphs ma y naturally b e used in a wide v ariet y of lds other than co ding including signal pro cessing system theory exp ert systems and artiial neural net w orks It is plausible that man y algorithms in these lds are naturally expressed in terms of factor graphs In this pap er w e will consider only one suc h algorithm the sumr o duct algorithm whic h op erates in a factor graph b y passing essages along the edges of the graph follo wing a single simple computational rule y w a y of preview a v ery simple example of the op eration of the sumro duct algorithm op erating in the factor graph of Fig is giv en in the next subsection The main purp ose of this essen tially tutorial pap er is to illuminate the simplici t y of the sumro duct algorithm in the general factor graph setting and then p oin t out a v ariet y of applications In parallel with the dev elopmen t of this pap er Aji and McEliece dev elop the closely related eneralized distributiv e la w an alternativ e approac h based on the prop erties of junction trees nd not factor graphs A'),
(320, 571, 'A Fast Quantum Mechanical Algorithm for Database Search', '                                                                                        1\r\nSummary\r\nImagine a phone directory containing N names\r\narranged in completely random order. In order to find\r\nsomeones phone number with a probability of , any\r\nclassical algorithm (whether deterministic or probabilis-\r\ntic) will need to look at a minimum of  names. Quan-\r\ntum mechanical systems can be in a superposition of\r\nstates and simultaneously examine multiple names. By\r\nproperly adjusting the phases of various operations, suc-\r\ncessful computations reinforce each other while others\r\ninterfere randomly. As a result, the desired phone num-\r\nber can be obtained in only  steps. The algo-\r\nrithm is within a small constant factor of the fastest\r\npossible quantum mechanical algorithm.\r\n1. Introduction\r\n1.0 Background Quantum mechanical computers\r\nwere proposed in the early 1980s [Benioff80] and in\r\nmany respects, shown to be at least as powerful as clas-\r\nsical computers - an important but not surprising result,\r\nsince classical computers, at the deepest level, ulti-\r\nmately follow the laws of quantum mechanics. The\r\ndescription of quantum mechanical computers was for-\r\nmalized in the late 80s and early 90s [Deutsch85]\r\n[BB94] [BV93] [Yao93] and they were shown to be\r\nmore powerful than classical computers on various spe-\r\ncialized problems. In early 1994, [Shor94] demonstrated\r\nthat a quantum mechanical computer could efficiently\r\nsolve a well-known problem for which there was no\r\nknown efficient algorithm using classical computers.\r\nThis is the problem of integer factorization, i.e. finding\r\nthe factors of a given integer N, in a time which is poly-\r\nnomial in .\r\n-----------------------------------------------\r\nThis is an updated version of a paper that originally\r\nappeared in Proceedings, STOC 1996, Philadelphia PA\r\nUSA, pages 212-219.\r\nThis paper applies quantum computing to a\r\nmundane problem in information processing and pre-\r\nsents an algorithm that is significantly faster than any\r\nclassical algorithm can be. The problem is this: there is\r\nan unsorted database containing N items out of which\r\njust one item satisfies a given condition - that one item\r\nhas to be retrieved. Once an item is examined, it is pos-\r\nsible to tell whether or not it satisfies the condition in\r\none step. However, there does not exist any sorting on\r\nthe database that would aid its selection. The most effi-\r\ncient classical algorithm for this is to examine the items\r\nin the database one by one. If an item satisfies the\r\nrequired condition stop; if it does not, keep track of this\r\nitem so that it is not examined again. It is easily seen\r\nthat this algorithm will need to look at an average of\r\nitems before finding the desired item.\r\n1.1 Search Problems in Computer Science\r\nEven in theoretical computer science, the typical prob-\r\nlem can be looked at as that of examining a number of\r\ndifferent possibilities to see which, if any, of them sat-\r\nisfy a given condition. This is analogous to the search\r\nproblem stated in the summary above, except that usu-\r\nally there exists some structure to the problem, i.e some\r\nsorting does exist on the database. Most interesting\r\nproblems are concerned with the effect of this structure\r\non the speed of the algorithm. For example the SAT\r\nproblem asks whether it is possible to find any combina-\r\ntion of n binary variables that satisfies a certain set of\r\nclauses C, the crucial issue in NP-completeness is\r\nwhether it is possible to solve it in time polynomial in n.\r\nIn this case there are N=2n possible combinations which\r\nhave to be searched for any that satisfy the specified\r\nproperty and the question is whether we can do that in a\r\ntime which is polynomial in , i.e. .\r\nThus if it were possible to reduce the number of steps to\r\na finite power of  (instead of  as in\r\nthis paper), it would yield a polynomial time algorithm\r\nfor NP-complete problems.\r\nIn view of the fundamental nature of the search\r\nproblem in both theoretical and applied computer sci-\r\n1\r\n2--\r\nN\r\n2---\r\nO N( )\r\nNlog\r\nN\r\n2---\r\nO Nlog( ) O nk( )\r\nO Nlog( ) O N( )\r\nA fast quantum mechanical algorithm for database search\r\nLov K. Grover\r\n3C-404A, Bell Labs\r\n600 Mountain Avenue\r\nMurray Hill NJ 07974\r\nlkgrover@bell-labs.com\r\n                                                                                        2\r\nence, it is natural to ask - how fast can the basic identifi-\r\ncation problem be solved without assuming anything\r\nabout the structure of the problem? It is generally\r\nassumed that this limit is  since there are N items\r\nto be examined and a classical algorithm will clearly\r\ntake  steps. However, quantum mechanical sys-\r\ntems can simultaneously be in multiple Schrodinger cat\r\nstates and carry out multiple tasks at the same time. This\r\npaper presents an  step algorithm for the search\r\nproblem.\r\nThere is a matching lower bound on how fast\r\nthe desired item can be identified. [BBBV96] show in\r\ntheir paper that in order to identify the desired element,\r\nwithout any information about the structure of the data-\r\nbase, a quantum mechanical system will need at least\r\n steps. Since the number of steps required by\r\nthe algorithm of this paper is , it is within a con-\r\nstant factor of the fastest possible quantum mechanical\r\nalgorithm.\r\n1.2 Quantum Mechanical Algorithms A good\r\nstarting point to think of quantum mechanical algo-\r\nrithms is probabilistic algorithms [BV93] (e.g. simu-\r\nlated annealing). In these algorithms, instead of having\r\nthe system in a specified state, it is in a distribution over\r\nvarious states with a certain probability of being in each\r\nstate. At each step, there is a certain probability of mak-\r\ning a transition from one state to another. The evolution\r\nof the system is obtained by premultiplying this proba-\r\nbility vector (that describes the distribution of probabili-\r\nties over various states) by a state transition matrix.\r\nKnowing the initial distribution and the state transition\r\nmatrix, it is possible in principle to calculate the distri-\r\nbution at any instant in time.\r\nJust like classical probabilistic algorithms,\r\nquantum mechanical algorithms work with a probability\r\ndistribution over various states. However, unlike classi-\r\ncal systems, the probability vector does not completely\r\ndescribe the system. In order to completely describe the\r\nsystem we need the amplitude in each state which is a\r\ncomplex number. The evolution of the system is\r\nobtained by premultiplying this amplitude vector (that\r\ndescribes the distribution of amplitudes over various\r\nstates) by a transition matrix, the entries of which are\r\ncomplex in general. The probabilities in any state are\r\ngiven by the square of the absolute values of the ampli-\r\ntude in that state. It can be shown that in order to con-\r\nserve probabilities, the state transition matrix has to be\r\nunitary [BV93].\r\nThe machinery of quantum mechanical algo-\r\nrithms is illustrated by discussing the three operations\r\nthat are needed in the algorithm of this paper. The first is\r\nthe creation of a configuration in which the amplitude of\r\nthe system being in any of the 2n basic states of the sys-\r\ntem is equal; the second is the Walsh-Hadamard trans-\r\nformation operation and the third the selective rotation\r\nof different states.\r\nA basic operation in quantum computing is that\r\nof a fair coin flip performed on a single bit whose\r\nstates are 0 and 1 [Simon94]. This operation is repre-\r\nsented by the following matrix: . A bit\r\nin the state 0 is transformed into a superposition in the\r\ntwo states: . Similarly a bit in the state 1 is\r\ntransformed into , i.e. the magnitude of the\r\namplitude in each state is  but the phase of the\r\namplitude in the state 1 is inverted. The phase does not\r\nhave an analog in classical probabilistic algorithms. It\r\ncomes about in quantum mechanics since the ampli-\r\ntudes are in general complex. In a system in which the\r\nstates are described by n bits (it has 2n possible states)\r\nwe can perform the transformation M on each bit inde-\r\npendently in sequence thus changing the state of the sys-\r\ntem. The state transition matrix representing this\r\noperation will be of dimension 2n X 2n. In case the ini-\r\ntial configuration was the configuration with all n bits in\r\nthe first state, the resultant configuration will have an\r\nidentical amplitude of  in each of the 2n states. This\r\nis a way of creating a distribution with the same ampli-\r\ntude in all 2n states.\r\nNext consider the case when the starting state\r\nis another one of the 2n states, i.e. a state described by\r\nan n bit binary string with some 0s and some 1s. The\r\nresult of performing the transformation M on each bit\r\nwill be a superposition of states described by all possi-\r\nble n bit binary strings with amplitude of each state hav-\r\ning a magnitude equal to  and sign either + or -. To\r\ndeduce the sign, observe that from the definition of the\r\nmatrix M, i.e. , the phase of the result-\r\ning configuration is changed when a bit that was previ-\r\nously a 1 remains a 1 after the transformation is\r\nperformed. Hence if  be the n-bit binary string describ-\r\ning the starting state and  the n-bit binary string\r\nO N( )\r\nO N( )\r\nO N( )\r\n N( )\r\nO N( )\r\nM 1\r\n2\r\n------\r\n1 1\r\n1 1\r\n=\r\n1\r\n2\r\n------\r\n1\r\n2\r\n------,  \r\n1\r\n2\r\n------\r\n1\r\n2\r\n------,  \r\n1\r\n2\r\n------\r\n2\r\nn\r\n2--\r\n2\r\nn\r\n2--\r\nM 1\r\n2\r\n------\r\n1 1\r\n1 1\r\n=\r\nx\r\ny\r\n                                                                                        3\r\ndescribing the resulting string, the sign of the amplitude\r\nof  is determined by the parity of the bitwise dot prod-\r\nuct of  and , i.e. . This transformation is\r\nreferred to as the Walsh-Hadamard transformation\r\n[DJ92]. This operation (or a closely related operation\r\ncalled the Fourier Transformation) is one of the things\r\nthat makes quantum mechanical algorithms more pow-\r\nerful than classical algorithms and forms the basis for\r\nmost significant quantum mechanical algorithms.\r\nThe third transformation that we will nee'),
(321, 572, 'A training algorithm for optimal margin classifiers', 'A Training Algorithm for\r\nOptimal Margin Classi\0ers\r\nBernhard E\0 Boser\r\n\0\r\nEECS Department\r\nUniversity of California\r\nBerkeley\0 CA \r\nbosereecsberkeleyedu\r\nIsabelle M\0 Guyon\r\nATT Bell Laboratories\r\n	 Fremont Street\0 \nth Floor\r\nSan Francisco\0 CA 	\r\nisabelleneuralattcom\r\nVladimir N\0 Vapnik\r\nATT Bell Laboratories\r\nCrawford Corner Road\r\nHolmdel\0 NJ \r\nvladneuralattcom\r\nAbstract\r\nA training algorithm that maximizes the mar\r\r\ngin between the training patterns and the de\r\r\ncision boundary is presented The technique\r\nis applicable to a wide variety of classiac\r\r\ntion functions\0 including Perceptrons\0 polyno\r\r\nmials\0 and Radial Basis Functions The ef\r\r\nfective number of parameters is adjusted auto\r\r\nmatically to match the complexity of the prob\r\r\nlem The solution is expressed as a linear com\r\r\nbination of supporting patterns These are the\r\nsubset of training patterns that are closest to\r\nthe decision boundary Bounds on the general\r\r\nization performance based on the leave\rone\rout\r\nmethod and the VC\rdimension are given Ex\r\r\nperimental results on optical character recog\r\r\nnition problems demonstrate the good gener\r\r\nalization obtained when compared with other\r\nlearning algorithms\r\n\0 INTRODUCTION\r\nGood generalization performance of pattern classiers is\r\nachieved when the capacity of the classication function\r\nis matched to the size of the training set Classiers with\r\na large number of adjustable parameters and therefore\r\nlarge capacity likely learn the training set without error\0\r\nbut exhibit poor generalization Conversely\0 a classier\r\nwith insucient capacity might not be able to learn the\r\ntask at all In between\0 there is an optimal capacity of\r\nthe classier which minimizes the expected generaliza\r\r\ntion error for a given amount of training data Both\r\nexperimental evidence and theoretical studies GBD\0\r\n\0\r\nPart of this work was performed while B\0 Boser was\r\nwith ATT Bell Laboratories\0 He is now at the University\r\nof California Berkeley\0\r\nMoo\0 GVB\r\n\0\r\n\0 Vap\0 BH\0 TLS\0 Mac link the\r\ngeneralization of a classier to the error on the training\r\nexamples and the complexity of the classier Meth\r\r\nods such as structural risk minimization Vap vary\r\nthe complexity of the classication function in order to\r\noptimize the generalization\r\nIn this paper we describe a training algorithm that au\r\r\ntomatically tunes the capacity of the classication func\r\r\ntion by maximizing the margin between training exam\r\r\nples and class boundary KM\0 optionally after re\r\r\nmoving some atypical or meaningless examples from the\r\ntraining data The resulting classication function de\r\r\npends only on so\rcalled supporting patterns Vap\r\nThese are those training examples that are closest to\r\nthe decision boundary and are usually a small subset of\r\nthe training data\r\nIt will be demonstrated that maximizing the margin\r\namounts to minimizing the maximum loss\0 as opposed\r\nto some average quantity such as the mean squared er\r\r\nror This has several desirable consequences The re\r\r\nsulting classication rule achieves an errorless separa\r\r\ntion of the training data if possible Outliers or mean\r\r\ningless patterns are identied by the algorithm and can\r\ntherefore be eliminated easily with or without super\r\r\nvision This contrasts classiers based on minimizing\r\nthe mean squared error\0 which quietly ignore atypi\r\r\ncal patterns Another advantage of maximum margin\r\nclassiers is that the sensitivity of the classier to lim\r\r\nited computational accuracy is minimal compared to\r\nother separations with smaller margin In analogy to\r\nVap\0 HLW a bound on the generalization perfor\r\r\nmance is obtained with the leave\rone\rout method For\r\nthe maximummargin classier it is the ratio of the num\r\r\nber of linearly independent supporting patterns to the\r\nnumber of training examples This bound is tighter than\r\na bound based on the capacity of the classier family\r\nThe proposed algorithm operates with a large class of\r\ndecision functions that are linear in their parameters\r\nbut not restricted to linear dependences in the input\r\ncomponents Perceptrons Ros\n\0 polynomial classi\r\r\ners\0 neural networks with one hidden layer\0 and Radial\r\nBasis Function RBF or potential function classiers\r\nABR\n\0 BL\0 MD fall into this class As pointed\r\nout by several authors ABR\n\0 DH\0 PG\0 Percep\r\r\ntrons have a dual kernel representation implementing\r\nthe same decision function The optimal margin algo\r\r\nrithm exploits this duality both for improved eciency\r\nand exibility In the dual space the decision function\r\nis expressed as a linear combination of basis functions\r\nparametrized by the supporting patterns The support\r\r\ning patterns correspond to the class centers of RBF\r\nclassiers and are chosen automatically by the maxi\r\r\nmum margin training procedure In the case of polyno\r\r\nmial classiers\0 the Perceptron representation involves\r\nan untractable number of parameters This problem is\r\novercome in the dual space representation\0 where the\r\nclassication rule is a weighted sum of a kernel func\r\r\ntion Pog	 for each supporting pattern High order\r\npolynomial classiers with very large training sets can\r\ntherefore be handled eciently with the proposed algo\r\r\nrithm\r\nThe training algorithm is described in Section  Section\r\n summarizes important properties of optimal margin\r\nclassiers Experimental results are reported in Section\r\n\r\n MAXIMUM MARGIN TRAINING\r\nALGORITHM\r\nThe maximum margin training algorithm nds a deci\r\r\nsion function for pattern vectors x of dimension n be\r\r\nlonging to either of two classes A and B The input to\r\nthe training algorithm is a set of p examples x\r\ni\r\nwith\r\nlabels y\r\ni\r\n\r\nx\r\n\r\n\0 y\r\n\r\n\0 x\r\n\r\n\0 y\r\n\r\n\0 x\r\n\r\n\0 y\r\n\r\n\0    \0 x\r\np\r\n\0 y\r\np\r\n \r\nwhere\r\n\0\r\ny\r\nk\r\n  if x\r\nk\r\n\0 class A\r\ny\r\nk\r\n  if x\r\nk\r\n\0 class B\r\nFrom these training examples the algorithm nds the\r\nparameters of the decision functionDx during a learn\r\r\ning phase After training\0 the classication of unknown\r\npatterns is predicted according to the following rule\r\nx \0 A if Dx  \r\nx \0 B otherwise\r\n\r\nThe decision functions must be linear in their parame\r\r\nters but are not restricted to linear dependences of x\r\nThese functions can be expressed either in direct\0 or in\r\ndual space The direct space notation is identical to the\r\nPerceptron decision function Ros\n\r\nDx \r\nN\r\nX\r\ni\r\nw\r\ni\r\n\r\ni\r\nx  b \r\nIn this equation the \r\ni\r\nare predened functions of x\0 and\r\nthe w\r\ni\r\nand b are the adjustable parameters of the deci\r\r\nsion function Polynomial classiers are a special case of\r\nPerceptrons for which \r\ni\r\nx are products of components\r\nof x\r\nIn the dual space\0 the decision functions are of the form\r\nDx \r\np\r\nX\r\nk\r\n\r\nk\r\nKx\r\nk\r\n\0x  b\0 \r\nThe coecients \r\nk\r\nare the parameters to be adjusted\r\nand the x\r\nk\r\nare the training patterns The function K\r\nis a predened kernel\0 for example a potential function\r\nABR\n or any Radial Basis Function BL\0 MD\r\nUnder certain conditions CH	\0 symmetric kernels\r\npossess nite or innite series expansions of the form\r\nKx\0x\r\n\r\n \r\nX\r\ni\r\n\r\ni\r\nx\r\ni\r\nx\r\n\r\n 	\r\nIn particular\0 the kernel Kx\0x\r\n\r\n  x  x\r\n\r\n \r\nq\r\ncor\r\r\nresponds to a polynomial expansion \0x of order q\r\nPog	\r\nProvided that the expansion stated in equation 	 exists\0\r\nequations  and  are dual representations of the same\r\ndecision function and\r\nw\r\ni\r\n\r\np\r\nX\r\nk\r\n\r\nk\r\n\r\ni\r\nx\r\nk\r\n \n\r\nThe parameters w\r\ni\r\nare called direct parameters\0 and the\r\n\r\nk\r\nare referred to as dual parameters\r\nThe proposed training algorithm is based on the gener\r\r\nalized portrait method described in Vap that con\r\r\nstructs separating hyperplanes with maximum margin\r\nHere this algorithm is extended to train classiers lin\r\r\near in their parameters First\0 the margin between the\r\nclass boundary and the training patterns is formulated\r\nin the direct space This problem description is then\r\ntransformed into the dual space by means of the La\r\r\ngrangian The resulting problem is that of maximizing\r\na quadratic form with constraints and is amenable to\r\necient numeric optimization algorithms Lue\r\n\0 MAXIMIZING THE MARGIN IN THE\r\nDIRECT SPACE\r\nIn the direct space the decision function is\r\nDx  w \0x  b\0 \r\nwhere w and \0x are N dimensional vectors and b is\r\na bias It denes a separating hyperplane in \0\rspace\r\nThe distance between this hyperplane and pattern x\r\nis Dxkwk Figure  Assuming that a separation\r\nof the training set with margin M between the class\r\nboundary and the training patterns exists\0 all training\r\npatterns fulll the following inequality\r\ny\r\nk\r\nDx\r\nk\r\n\r\nkwk\r\n M \r\nThe objective of the training algorithm is to nd the\r\nparameter vector w that maximizes M \r\nM\r\n\0\r\n max\r\nw\0kwk\r\nM \r\nsubject to y\r\nk\r\nDx\r\nk\r\n  M\0 k  \0 \0    \0 p\r\nThe bound M\r\n\0\r\nis attained for those patterns satisfying\r\nmin\r\nk\r\ny\r\nk\r\nDx\r\nk\r\n  M\r\n\0\r\n \r\n1 | b ||| w ||\r\nw\r\nx\r\n2\r\n| D(x) |\r\n || w ||\r\nD(x)=w.x+b=0\r\nD(x)>0\r\nD(x)<0\r\n0 x1\r\nx2\r\nM\r\nM\r\n*\r\n*\r\n1\r\n2\r\n1\r\nFigure  Maximum margin linear decision function Dx  w  x b \0  x The gray levels encode the absolute\r\nvalue of the decision function solid black corresponds to Dx   The numbers indicate the supporting patterns\r\nThese patterns are called the supporting patterns of the\r\ndecision boundary\r\nA decision function with maximummargin is illustrated\r\nin gure  The problem of nding a hyperplane in\r\n\0\rspace with maximum margin is therefore a minimax\r\nproblem\r\nmax\r\nw\0kwk\r\nmin\r\nk\r\ny\r\nk\r\nDx\r\nk\r\n \r\nThe norm of the parameter vector in equations  and\r\n is xed to pick one of an innite number of possible\r\nsolutions that dier only in scaling Instead of xing\r\nthe norm of w to take care of the scaling problem\0 the\r\nproduct of the margin M and the norm of a weight\r\nvector w can be xed\r\nMkwk   \r\nThus\0 maximizing the margin M is equivalent to mini\r\r\nmizin'),
(322, 573, 'Graph-based algorithms for Boolean function manipulation', 'Graph-Based Algorithms\r\nfor Boolean Function Manipulation12\r\nRandal E. Bryant3\r\nAbstract\r\nIn this paper we present a new data structure for representing Boolean functions and an associated set of\r\nmanipulation algorithms.  Functions are represented by directed, acyclic graphs in a manner similar to the\r\nrepresentations introduced by Lee [1] and Akers [2], but with further restrictions on the ordering of decision\r\nvariables in the graph.  Although a function requires, in the worst case, a graph of size exponential in the number of\r\narguments, many of the functions encountered in typical applications have a more reasonable representation.  Our\r\nalgorithms have time complexity proportional to the sizes of the graphs being operated on, and hence are quite\r\nefficient as long as the graphs do not grow too large. We present experimental results from applying these\r\nalgorithms to problems in logic design verification that demonstrate the practicality of our approach.\r\nIndex Terms: Boolean functions, symbolic manipulation, binary decision diagrams, logic design verification\r\n1. Introduction\r\nBoolean Algebra forms a cornerstone of computer science and digital system design.  Many problems in digital\r\nlogic design and testing, artificial intelligence, and combinatorics can be expressed as a sequence of operations on\r\nBoolean functions.  Such applications would benefit from efficient algorithms for representing and manipulating\r\nBoolean functions symbolically. Unfortunately, many of the tasks one would like to perform with Boolean\r\nfunctions, such as testing whether there exists any assignment of input variables such that a given Boolean\r\nexpression evaluates to 1 (satisfiability), or two Boolean expressions denote the same function (equivalence) require\r\nsolutions to NP-Complete or coNP-Complete problems [3]. Consequently, all known approaches to performing\r\nthese operations require, in the worst case, an amount of computer time that grows exponentially with the size of the\r\nproblem. This makes it difficult to compare the relative efficiencies of different approaches to representing and\r\nmanipulating Boolean functions.  In the worst case, all known approaches perform as poorly as the naive approach\r\nof representing functions by their truth tables and defining all of the desired operations in terms of their effect on\r\ntruth table entries. In practice, by utilizing more clever representations and manipulation algorithms, we can often\r\navoid these exponential computations.\r\nA variety of methods have been developed for representing and manipulating Boolean functions.  Those based on\r\nclassical representations such as truth tables, Karnaugh maps, or canonical sum-of-products form [4] are quite\r\n1This research was funded at the California Institute of Technology by the Defense Advanced Research\r\nProjects Agency ARPA Order Number 3771 and at Carnegie-Mellon University by the Defense\r\nAdvanced Research Projects Agency ARPA Order Number 3597.  A preliminary version of this paper\r\nwas presented under the title Symbolic Manipulation of Boolean Functions Using a Graphical\r\nRepresentation at the 22nd Design Automation Conference, Las Vegas, NV, June 1985.\r\n2Update: This paper was originally published in IEEE Transactions on Computers, C-35-8, pp.\r\n677-691, August, 1986.  To create this version, we started with the original electronic form of the\r\nsubmission. All of the figures had to be redrawn, since they were in a now defunct format.  We have\r\nincluded footnotes (starting with Update:) discussing some of the (minor) errors in the original version\r\nand giving updates on some of the open problems.\r\n3Current address:  Department of Computer Science, Carnegie-Mellon University, Pittsburgh, PA 15213\r\n2impractical---every function of n arguments has a representation of size 2n or more.  More practical approaches\r\nutilize representations that at least for many functions, are not of exponential size.  Example representations include\r\nas a reduced sum of products [4], (or equivalently as sets of prime cubes [5]) and factored into unate functions [6].\r\nThese representations suffer from several drawbacks.  First, certain common functions still require representations\r\nof exponential size.  For example, the even and odd parity functions serve as worst case examples in all of these\r\nrepresentations. Second, while a certain function may have a reasonable representation, performing a simple\r\noperation such as complementation could yield a function with an exponential representation.  Finally, none of these\r\nrepresentations are canonical forms, i.e. a given function may have many different representations.  Consequently,\r\ntesting for equivalence or satisfiability can be quite difficult.\r\nDue to these characteristics, most programs that process a sequence of operations on Boolean functions have\r\nrather erratic behavior.  They proceed at a reasonable pace, but then suddenly blow up, either running out of\r\nstorage or failing to complete an operation in a reasonable amount of time.\r\nIn this paper we present a new class of algorithms for manipulating Boolean functions represented as directed\r\nacyclic graphs. Our representation resembles the binary decision diagram notation introduced by Lee [1] and further\r\npopularized by Akers [2]. However, we place further restrictions on the ordering of decision variables in the\r\nvertices. These restrictions enable the development of algorithms for manipulating the representations in a more\r\nefficient manner.\r\nOur representation has several advantages over previous approaches to Boolean function manipulation.  First,\r\nmost commonly-encountered functions have a reasonable representation.  For example, all symmetric functions\r\n(including even and odd parity) are represented by graphs where the number of vertices grows at most as the square\r\nof the number of arguments.  Second, the performance of a program based on our algorithms when processing a\r\nsequence of operations degrades slowly, if at all.  That is, the time complexity of any single operation is bounded by\r\nthe product of the graph sizes for the functions being operated on.  For example, complementing a function requires\r\ntime proportional to the size of the function graph, while combining two functions with a binary operation (of which\r\nintersection, subtraction, and testing for implication are special cases) requires at most time proportional to the\r\nproduct of the two graph sizes.  Finally, our representation in terms of reduced graphs is a canonical form, i.e.  every\r\nfunction has a unique representation.  Hence, testing for equivalence simply involves testing whether the two graphs\r\nmatch exactly, while testing for satisfiability simply involves comparing the graph to that of the constant function 0.\r\nUnfortunately, our approach does have its own set of undesirable characteristics.  At the start of processing we\r\nmust choose some ordering of the system inputs as arguments to all of the functions to be represented.  For some\r\nfunctions, the size of the graph representing the function is highly sensitive to this ordering.  The problem of\r\ncomputing an ordering that minimizes the size of the graph is itself a coNP-Complete problem.  Our experience,\r\nhowever, has been that a human with some understanding of the problem domain can generally choose an\r\nappropriate ordering without great difficulty.  It seems quite likely that using a small set of heuristics, the program\r\nitself could select an adequate ordering most of the time.  More seriously, there are some functions that can be\r\nrepresented by Boolean expressions or logic circuits of reasonable size but for all input orderings the representation\r\nas a function graph is too large to be practical.  For example, we prove in an appendix to this paper that the functions\r\ndescribing the outputs of an integer multiplier have graphs that grow exponentially in the word size regardless of the\r\ninput ordering. With the exception of integer multiplication, our experience has been that such functions seldom\r\narise in digital logic design applications.  For other classes of problems, particularly in combinatorics, our methods\r\nseem practical only under restricted conditions.\r\nA variety of graphical representations of discrete functions have be presented and studied extensively.  A survey\r\nof the literature on the subject by Moret [7] cites over 100 references, but none of these describe a sufficient set of\r\n3algorithms to implement a Boolean function manipulation program.  Fortune, Hopcroft, and Schmidt [8] studied the\r\nproperties of graphs obeying similar restrictions to ours, showing that two graphs could be tested for functional\r\nequivalence in polynomial time and that some functions require much larger graphs under these restrictions than\r\nunder milder restrictions.  Payne [9] describes techniques similar to ours for reducing the size of the graph\r\nrepresenting a function.  Our algorithms for combining two functions with a binary operation, and for composing\r\ntwo functions are new, however, and these capabilities are central to a symbolic manipulation program.\r\nThe next section of this paper contains a formal presentation of function graphs.  We define the graphs, the\r\nfunctions they represent, and a class of reduced graphs.  Then we prove a key property of reduced function graphs:\r\nthat they form a canonical representation of Boolean functions.  In the following section we depart from this formal\r\npresentation to give some examples and to discuss issues regarding to the efficiency of our representation.\r\nFollowing this, we develop a set of algorithms for manipulating Boolean functions using our representation.  These\r\nalgorithms utilize many of the classical techniques for graph algorithms, and we assume the reader has some\r\nfamiliarity with these techniques.  We then present some experimental investigations into the practicality of our\r\nmethods. We conclude by suggesting further refinements of our methods.');
INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(323, 574, 'The FERET evaluation methodology for face recognition algorithms', '\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\n\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\n\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0 \0\0\0\0 \0 \0\0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0\0\0 \0 \0\0\0\0 \0 !\0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0\0 \0 \0\0\0\0 \0 \0\0\0 \0\0\0\0\0\0# \0\0 \0\0\0\0\0\n\0\0\0\0\0\0\0\0 $\0%\0 \0\0 \0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0 \0\0&\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0 \0 \0\0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0 \0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 (\0\0\0\0\0) \0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\n\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0 *+\0*,- \0\0\0\0\0\0 \0\0\0\0 *\0*.. \0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0 %\0\0\0\0 \0\0 \0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0&\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0 /\0\n!\0\0\0\0\0\0\0 *..-\0 \0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0 \0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0 \0 \0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0 %\0\0\0 \0\0 *) \0\0\0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0 ,) \0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0\0 0) \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 $\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\n\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0 \0 \0\0\0 \0\0\0 \0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0 !\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0 \0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0 \0 \0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0 \0 \0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 !\0\0 #\0\0\0! \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 !\0\0\0\0 \0\0\0\0 \0\0\0\0 \0\0\0\0\0 #\0\0\0! \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0 \0\0\0\0 \0\0\0\0\0\0 \0\0\0\0\0 \0\0\0 $\0\0\0\0\0\0\0\0 %&&\0 #\0\0\0! \0\0\0\0\0 !\0\0$\0\0\0\0\0\0\0\0%&&\0#\0\0\0!\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0!\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\n\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 (\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0 \0\0)\0\0\0\0 \0\0\0\0 \0\0\0\0 \0\0 \0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0 \0 \0\0\0\0 \0\0\0 \0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0*\0\0\0\0 \0\0\0 \0\0\0\0\0\0 (\0 \0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0 \0\0\0 #\0\0\0! \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0 $\0\0\0\0\0\0\0\0\n%&&\0 #\0\0\0! \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0+\0\0 \0\0 \0\0\0 \0%, \0\n!\0\0 #\0\0\0! \0\0\0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0\0 #\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0 \0 \0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0 -\0\0 \0\0\0\0\0\0\0. \0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0 (\0 \0\0\0 #\0\0\0\0\0\0 /\0\0\0\0\0 \0\0 (\0\0\0\0\0\0\0\0\0\0\0\0\0 -#/(. (\0\0\0\0\0\0\0\0\0\n0\0\0\0\0\0\0\0\0 #\0\0\0\0\0\0\0\0\0\0 (\0\0\0\0\0\0\0\0\0\0\0\0\0 $\0\0\0\0\0 -(0#($.\0 \0\0\0 \0\0\0\0 \0\01\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0 \0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0 \02 \0 !\0\0 (0#($\n\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0 \0\0 \0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0 3\0\0\0\0\0 $\0\0\0\0\0\0 \0\0\0\0\0 \04\0\0\0\0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0 5\0\0\0 \0\0\0 (0#($ \0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0 \04\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0 6\0,,, \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0 -%\07,,\0,,, \0\0\0 \0\0\0\0.\0\n/\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 3\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\01\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 #\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0 \0 \0\0\0\0 \0\0\0\0\0\0\0 \0 \0\0\0\0\0 \0\0\0\0 \0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0 !\0\0 \0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0 \0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0 /\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0*\0\0\0\0\0\0 /\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0 \0 \0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0 \0 \0\0\0\0\0 \0\0\0\0 \0\0\0 \0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0 \0\0\0\0 \0\0 \0\0\0\0\0\0\0 !\0\0\0\0\n\0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 0\0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0 \0\0\0 \0\0\0\0\0\0 \0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0 \0 \0\0\0\0\0 \0\0\0\0\0 !\0\0 #\0\0\0! \0\0\0 \0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 #\0\0\0! \0\0\0\0\0\0\0\0\0 !\0\0 \0\0\0\0 \0\0\0\0\0 \0\0 \0\0\0 #\0\0\0! \0\0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 !\0\0\0\0 \0\0\0 #\0\0\0! \0\0\0\0 \0\0\0 \0\0\0 *1.1 /\0\0\0 \0\0\02!\03\0/42! 42 \0\0\0\0\0\02 \02\056!/! \027 \0\03\0/2\0 /2\0\055/8\023\0\0 945\0 ,,\0 24\0 *1\0 43\04:\0\0 ,111\n\0 \0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0\0 !\0\0 #\0\0\0\0\0\0\0$\0\0\0\0 %\0 &\0 !! !\0\0\n()\0\0\0* +\0\0\0\0\0\0\0,\0\0\0\0\0\0\0-\0\n\0 .\0 %\0\0\0 \0\0 \0\0\0\0 /\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0 0\0 \0\0\0\0\0\0 1\0\0\0\0 /\0\0\0\0\0\0\0\0\0 %2 \0\03\0\0\n()\0\0\0* \0),\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0)\0 \0 \0\02\0 1\04-\0 \0\0 \0\0\0\0 \0\0\0 \0\0\0\0\0\0)\0\0\0 \0\0 (\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 5\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0\0657\08\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0 \08 \0\00\0\0\n()\0\0\0* \0\04-\0,\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0 \0\0\0\0 1\0\0\0\0 \0\0 \0\0\0\0 \0\0\0 2\0)\0 1\0\0\0\0\0\0\0 /\0$\0\0\0\0\0\0\0\0 & \0\0 \0\0\0\0\0\0 %\0\0\0 1\0\0\0\0\n2\0\0\0\0\0\0\0 %\0 &\09 0\0\0!9\0 ()\0\0\0* \0\0\0\0\0,\0\0\0)\0\0\0\0\0)\0 %\0\0\0\0\0\0\0\0\0 \0\0\0\0\0-\0\0 & \0\0-\0 \0!! : \0\0-\0\0\0\0 & \0\0\0\0\0 \0!!!: \0\0\0\0\0\0\0\0 \0! %\0\0\n&\0\0\0\0\n1\0\0\0))\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0 $\0 \0\0\0\0 ;\0\0\0\0)\0\0\0\n<\0\0 \0\0\0\0\0)\0\0\0\0\0 \0\0 \0$\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0\0 \0)\0\0\0 \0\0*\n\0\0\0)\0,\0\0)\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0(((5\0 /\0\0 \0\0)$\0\0 \0\0 \0=3\0\n1*-,\0;;,;<11<=*1\011 \0 ,111 /\0\0\0\04\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0 \0\0\0 \0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0 \0 \0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0 !\0 \0\0\0\0\0\0 \0 \0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0 !\0\0\n\0\0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0)\0\0 \0\0\0 \0\0 \0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0 \0\0\0\0 \0\0\0\0\0\0\0 \0\0\0 \0\01\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0 /\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0 \0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0)\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\n\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 !\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 )\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0 %. \0\0\0\0 \0\0\0\0\0\0+\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0+\0\0\0\0\0\n\0\0\0 2. \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 5\0 \0\0\0 \0\0\0 \0\0\0\0 \0\0\0\0 \0\0\0\0\0\0+\0\0\0\0\0\n\0\0\0 \0\0\0\0\0\0\0+\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0 (\0\n\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0) \0\0 \0\0 \0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0 \0\0\0 \0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0 \0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0 (\0 \0\0\0 #\0\0\0! \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0 \0\0 \0\0 \0\0\0\0\0\0 (\0 \0\0\0 \0\0\0\0\0 \0\0\0\0 \0\0 \0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0 \0\0 \0\0\0\0\0\0\0 \0\0 \0\0 \0\0\0\0\0 \0\0\0 \0\0\0\0 \0\0\0 \0\0\0\0 \0\0 \0\0\0\0\0\0\0+\0\0 \0\0\0\0 \0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0 3\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0+\0\0\0\0\0 \0\01\0\0\0\0\0 \0\0\0\0 \0 \0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0 \0\0\0\0\0\0 \0\0 \0\04\0\0\0\0\n0\0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0 \0\0\0\0\0 \0\0\0\0)\0\0\0\0 \0\0\0\0\0\0\0\0)\0 \0\0\0 \0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0\0 \0\0\0 \0\0\0\0\0\0 \0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0)\0\0\0\0 \0\0\0\0\0\0\0\0)\0 \0 -0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0\0 0\0\0\0\0\0\04\0. !\0\0$\0\0\0\0\0\0\0\0 %&&\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0 \0 \0\0\0\0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0 #\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0 !\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 #\0\0\0! \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0 \0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 /\0\0\0\0\0 \0\0\0 #\0\0\0! \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0 \0\0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 -\0\0\0\0\0\0\0\n\0 \0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0. \0\0 \0\0\0\0\0\0\0\0\0\0+\0 \0\0\0\0\0\0\0\0\0 -\0\0\0\0\0\0\0 \0 \0\0 \0\0\0\0\0\0\0\0\0\0\0.\0 -(\0 \0\0\0\0\0 \0\0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0. \0\0\0\0 \0 \0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0+\0\0\0 \0 \0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0 0\0 \0 \0\0\0\0\0\0 1\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0 \0\0 \0\0\0\0\0\0 \0\0 \0\0)\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 !\0\0 #\0\0\0! \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0 \0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0 \0 \0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0 \0 \0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0 !\0\0\n\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 #\0\0\0! \0\0\0\0 \0\0\0\0\0\0 \0\0\0 \0 \0\0\0\0\0\0 1\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\n\0\0\0)\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 8\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0 #\0\0\0! \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0 \0\0 \0\0\0 \0\0\0 \0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 !\0\0 #\0\0\0! \0\0\0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0)\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0 \0\0\0 \0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 !\0\0\0\0\0\0\n\0\0\0\0 \0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0 \0\0\0\0 \0\0\0\0\0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0 \0\0\0 \0\0\0\0\0\0\0 \0\0\0 \0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0 !\0\0 \0\0\0\0\0 #\0\0\0! \0\0\0\0\0 \0\0\0) \0\0\0\0\0 \0\0 0\0\0\0\0\0 %&&9 \0\0\0\n8\0\0\0\0 %&&6 -\0\0\0 \0\0\0\0\0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0 \0\0\0 \0\0\0 #\0\0\0! \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0 \0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0 \0: \0 \0\0\0 \0\0\0\0\0 \0\0 \0\0\0 \07 .\0 !\0\0 #\0\0\0! \0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\0 $\0\0\0\0\0\0\0\0 %&&; \0\0\0\0\0 \0\0\0\0 \0\0\0 #\0\0\0! \0\0\0\0\0\0\0\0 !\0\0 0\0\0\0\0\0 %&&9 \0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0\0 \0 \0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 !\0\0\0\n\0\0\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0+\0\0 \0\0\0 \0\0\0\0\0\0\0\0 \0\0\0\0\0 \0\0\0\0 \0 \0\0\0\0\0\0\0\0\0 !\0\0 \0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0 \0\0\0\0\0 \0\0\0\0\0\0\0\0\0 \0\0\0\0 \0\0\0\0 \0 \0\0\0\0\0\0\0\0\0 \0\0\0\0\0\0\0 \0\0\0 \0\0\0\0\0 \0\0\0\0 !\0\0 \0\0\0\0\0\0\0 \0\0\0\0\0\0\0\0 \0\0\0 \0\0\0 \0\0 )\0\0\0\0 \0\0'),
(324, 575, 'A universal algorithm for sequential data compression', 'IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. IT-23, NO. 3, MAY 1977 337 \r\nA Universal Algorithm for Sequentia l Data Compression \r\nJACOB ZIV, FELLOW, IEEE, AND ABRAHAM LEMPEL, MEMBER, IEEE \r\nAbstract-A universal algorithm for sequential data compres- \r\nsion is presented. Its performance is investigated with respect to \r\na nonprobabilistic model of constrained sources. The compression \r\nratio achieved by the proposed universal code uniformly ap- \r\nproaches the lower bounds on the compression ratios attainable by \r\nblock-to-variable codes and variable-to-block codes designed to \r\nmatch a completely specified source. \r\nI. INTRODUCTION \r\nI N MANY situations arising in digital com- munications and data processing, the encountered \r\nstrings of data display various structural regularities or are \r\notherwise subject to certain constraints, thereby allowing \r\nfor storage and time-saving techniques of data compres- \r\nsion. Given a discrete data source, the problem of data \r\ncompression is first to identify the limitations of the source, \r\nand second to devise a coding scheme which, subject to \r\ncertain performance criteria, will best compress the given \r\nsource. \r\nOnce the relevant source parameters have been identi- \r\nfied, the problem reduces to one of minimum-redundancy \r\ncoding. This phase of the problem has received extensive \r\ntreatment in the literature [l]-[7]. \r\nWhen no a priori knowledge of the source characteristics \r\nis available, and if statistical tests are either impossible or \r\nunreliable, the problem of data compression becomes \r\nconsiderably more complicated. In order to overcome these \r\ndifficulties one must resort to universal coding schemes \r\nwhereby the coding process is interlaced with a learning \r\nprocess for the varying source characteristics [8], [9]. Such \r\ncoding schemes inevitably require a larger working mem- \r\nory space and generally employ performance criteria that \r\nare appropriate for a  wide variety of sources. \r\nIn this paper, we describe a universal coding scheme \r\nwhich can be applied to any discrete source and whose \r\nperformance is comparable to certain optimal fixed code \r\nbook schemes designed for completely specified sources. \r\nFor lack of adequate criteria, we do not attempt to rank the \r\nproposed scheme with respect to other possible universal \r\ncoding schemes. Instead, for the broad class of sources \r\ndefined in Section III, we derive upper bounds on the \r\ncompression efficiency attainable with full a  priori \r\nknowledge of the source by fixed code book schemes, and \r\nManuscript received June 23, 1975; revised July 6, 1976. Paper pre- \r\nviously presented at the IEEE International Symposium on Information \r\nTheory, Ronneby, Sweden, June 21-24,1976. \r\nJ. Ziv was with the Department of Electrical Engineering, Technion- \r\nIsrael Institute of Technology, Haifa, Israel. He is now with the Bell \r\nTelephone Laboratories, Murray Hill, NJ 07974. \r\nA. Lempel was with the Department of Electrical Engineering, Tech- \r\nnion-Israel Institute of Technology, Haifa, Israel. He is now with the \r\nSperry Research Center, Sudbury, MA 01776. \r\nthen show that the efficiency of our universal code with no \r\na priori knowledge of the source approaches those \r\nbounds. \r\nThe proposed compression algorithm is an adaptation \r\nof a  simple copying procedure discussed recently [lo] in \r\na study on the complexity of finite sequences. Basically, \r\nwe employ the concept of encoding future segments of the \r\nsource-output via maximum-length copying from a buffer \r\ncontaining the recent past output. The transmitted \r\ncodeword consists of the buffer address and the length of \r\nthe copied segment. W ith a predetermined initial load of \r\nthe buffer and the information contained in the codewords, \r\nthe source data can readily be reconstructed at the de- \r\ncoding end of the process. \r\nThe main drawback of the proposed algorithm is its \r\nsusceptibility to error propagation in the event of a  channel \r\nerror. \r\nII. THE COMPRESSION ALGORITHM \r\nThe proposed compression algorithm consists of a  rule \r\nfor parsing strings of symbols from a finite alphabet A into \r\nsubstrings, or words, whose lengths do not exceed a pre- \r\nscribed integer L,, and a coding scheme which maps these \r\nsubstrings sequentially into uniquely decipherable code- \r\nwords of fixed length L, over the same alphabet A. \r\nThe word-length bounds L, and L, allow for bounded- \r\ndelay encoding and decoding, and they are related by \r\nL, = 1 + [log (n - L,$)l + [log L,l, (1) \r\nwhere [xl is the least integer not smaller than x, the log- \r\narithm base is the cardinality (Y of the alphabet A, and n \r\nis the length of a  buffer, employed at the encoding end of \r\nthe process, which stores the latest n  symbols emitted by \r\nthe source. The exact relationship between n and L, is \r\ndiscussed in Section III. Typically, n  N L,ahLs, where 0 \r\n< h < 1. For on-line decoding, a buffer of similar length has \r\nto be employed at the decoding end also. \r\nTo describe the exact mechanics of the parsing and \r\ncoding procedures, we need some preparation by way of \r\nnotation and definitions. \r\nConsider a finite alphabet A of CY symbols, say A = \r\n(O,l, * * * ,cr - 1). A string, or word, S of length a(S) = h over \r\nA is an ordered h-tuple S = ~1.~2 - * . Sk of symbols from A. \r\nTo indicate a substring of S which starts at position i and \r\nends at position j, we write S(i,j). When i 5  j, S(i,j) = \r\ns;s,+i .-*sj, but when i > j, we take S(i,j) = A, the null \r\nstring of length zero. \r\nThe concatenation of strings Q and R forms a new string \r\nS = QR; if k(Q) = h and t(R) = m, then a(S) = h + m, Q \r\n= S(l,h), and R = S(k + 1, h  + m). For each j, 0  I j 5  \r\n338 IEEE TRANSACTIONS ON INFORMATION THEORY, MAY 1977 \r\ne(S), S(lj) is called a prefix of S; S(lj) is a proper prefix \r\nof S if j < a(S). \r\nGiven a proper prefix S(lj) of a string S and a positive \r\ninteger i such that i I j, let L(i) denote the largest non- \r\nnegative integer e i e(S) - j such that \r\nS(i,i + 4 - 1) = Sg + 1,j + e), \r\nand let p be a position of S(l,j) for which \r\nL(p) = max {L(i)}. \r\nl&s; \r\nThe substring Sg + 1,j + L(p)) of S is called the repro- \r\nducible extension of S(l,j) into S, and the integer p is \r\ncalled the pointer of the reproduction. For example, if S \r\n= 00101011 and j = 3, then L(1) = 1 since S(j + 1,j + 1) = \r\nS(1,l) but S(i + 1,j + 2) # S(1,2). Similarly, L(2) = 4 and \r\nL(3) = 0. Hence, S(3 + 1,3 + 4) = 0101 is the reproducible \r\nextension of S(1,3) = 001 into S with pointer p = 2. \r\nNow, to describe the encoding process, let S = sisz * * . \r\ndenote the string of symbols emitted by the source. The \r\nsequential encoding of S entails parsing S into successive \r\nsource words, S = S1Sz.. . , and assigning a codeword Ci \r\nfor each Si. For bounded-delay encoding, the length e, of \r\neach Si is at most equal to a predetermined parameter L,? , \r\nwhile each Ci is of fixed length L, as given by (1). \r\nTo initiate the encoding process, we assume that the \r\noutput S of the source was preceded by a string Z of n - \r\nL, zeros, and we store the string Bi = ZS(l,L,) in the \r\nbuffer. If S(l,j) is the reproducible extension of 2 into \r\nZS(l,L, - I), then Si = S(l,j + 1) and Cl = j + 1. To de- \r\ntermine the next source word, we shift out the first tl \r\nsymbols from the buffer and feed into it the next f?i sym- \r\nbols of S to obtain the string Bz = Bi(Ci + l,n)S(L,s + 1, \r\nL, + ai). Now we look for the reproducible extension E of \r\nBs(l,n - L,) into Bs(l,n - l), and set Sz = Es, where s is \r\nthe symbol next to E in Bz. In general, if Bi denotes the \r\nstring of n source symbols stored in the buffer when we are \r\nready to determine the ith source word Si, the successive \r\nencoding steis can be formally described as follows. \r\n1) Initially, set Bi = 0 -L6(1,L,s), i.e., the all-zero \r\nstring of length n - L, followed by the first L, symbols of \r\nS. \r\n2) Having determined B;, i I 1, set \r\nSi = Bi(n - L, + 1,n - L, + l?i), \r\nwhere the prefix of length ei - 1 of S; is the reproducible \r\nextension of $;(l,n - L,) intoBi(l,n - 1). \r\n3) If pi is the reproduction pointer used to determine \r\nSi, then the codeword CL for Si is given by \r\nci = CilCd&, \r\nwhere Cii is the radix-a representation of pi - 1 with \r\n!?(C;,) = [log (n - L,)], Ciz is the radix-a representation \r\nof ei - 1 with e(Ciz) = [log L,], and Cis is the last symbol \r\nof Si, i.e., the symbol occupying position n - L,? + J?i of Bi. \r\nThe total length of CL is given by \r\nE(Ci) = [log (n - L,)] + [log L,J + 1 \r\n4) To update the contents of the buffer, shift out the \r\nsymbols occupying the first ei positions of the buffer while \r\nfeeding in the next ei symbols from the source to obtain \r\nB;+l = Bi(Ci + l,n)S(h; + l,hi + ei), \r\nwhere hi is the position of S occupied by the last symbol \r\nof B;. \r\nThis completes the description of the encoding process. \r\nIt is easy to verify that the parsing rule defined by (2) \r\nguarantees a bounded, positive source word length in each \r\niteration; in fact, 1 5 e; I L, for each i thus allowing for \r\na radix-a representation of ei - 1 with [log L,l symbols \r\nfrom A. Also, since 1 5 p; I n - L,? for each i, it is possible \r\nto represent pi - 1 with [log (n - L,)l symbols from A. \r\nDecoding can be performed simply by reversing the \r\nencoding process. Here we employ a buffer of length n - \r\nL,s to store the latest decoded source symbols. Initially, the \r\nbuffer is loaded with n - L, zeros. If Di = dldz *. . dn-L,, \r\ndenotes the contents of the buffer after CL-1 has been de- \r\ncoded into $1, then \r\nSi-1 = D;(n -L, - ei-1 + 1,n -L,?), \r\nwhere e;-i = E(S-I), and where Di+l can be obtained \r\nfrom Di and Ci as follows. \r\nDetermine pi - 1 and J; - 1 from the first [log (n - L,)] \r\nand the next [log L,l symbols of C;. Then, apply Ci - 1 \r\nshifts while feeding the contents of stage pi into stage n - \r\nL,. The first of these shifts will change the buffer contents \r\nfrom D; to \r\nD(1) = d i 2 3.. . d,+d,, = djdh . . . dj,! ,,,. d \r\nSimilarly, if j I'),
(325, 576, 'Wireless sensor networks: a survey', 'Wireless sensor networks: a survey\r\nI.F. Akyildiz, W. Su*, Y. Sankarasubramaniam, E. Cayirci\r\nBroadband and Wireless Networking Laboratory, School of Electrical and Computer Engineering, Georgia Institute of Technology,\r\nAtlanta, GA 30332, USA\r\nReceived 12 December 2001; accepted 20 December 2001\r\nAbstract\r\nThis paper describes the concept of sensor networks which has been made viable by the convergence of micro-\r\nelectro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the\r\npotential sensor networks applications are explored, and a review of factors influencing the design of sensor networks is\r\nprovided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols\r\ndeveloped for each layer in the literature are explored. Open research issues for the realization of sensor networks are\r\nalso discussed.  2002 Published by Elsevier Science B.V.\r\nKeywords: Wireless sensor networks; Ad hoc networks; Application layer; Transport layer; Networking layer; Routing; Data link\r\nlayer; Medium access control; Error control; Physical layer; Power aware protocols\r\n1. Introduction\r\nRecent advances in micro-electro-mechanical\r\nsystems (MEMS) technology, wireless communi-\r\ncations, and digital electronics have enabled the\r\ndevelopment of low-cost, low-power, multifunc-\r\ntional sensor nodes that are small in size and\r\ncommunicate untethered in short distances. These\r\ntiny sensor nodes, which consist of sensing, data\r\nprocessing, and communicating components, le-\r\nverage the idea of sensor networks based on\r\ncollaborative effort of a large number of nodes.\r\nSensor networks represent a significant improve-\r\nment over traditional sensors, which are deployed\r\nin the following two ways [39]:\r\n Sensors can be positioned far from the actual\r\nphenomenon, i.e., something known by sense\r\nperception. In this approach, large sensors\r\nthat use some complex techniques to distin-\r\nguish the targets from environmental noise\r\nare required.\r\n Several sensors that perform only sensing can be\r\ndeployed. The positions of the sensors and com-\r\nmunications topology are carefully engineered.\r\nThey transmit time series of the sensed pheno-\r\nmenon to the central nodes where computations\r\nare performed and data are fused.\r\nA sensor network is composed of a large num-\r\nber of sensor nodes, which are densely deployed\r\neither inside the phenomenon or very close to it.\r\nComputer Networks 38 (2002) 393422\r\nwww.elsevier.com/locate/comnet\r\n*Corresponding author. Tel.: +1-404-894-5141; fax: +1-404-\r\n894-7883.\r\nE-mail addresses: ian@ece.gatech.edu (I.F. Akyildiz), weil-\r\nian@ece.gatech.edu (W. Su), yogi@ece.gatech.edu (Y. Sanka-\r\nrasubramaniam), erdal@ece.gatech.edu (E. Cayirci).\r\n1389-1286/02/$ - see front matter  2002 Published by Elsevier Science B.V.\r\nPII: S1389-1286 (01 )00302-4\r\nThe position of sensor nodes need not be engi-\r\nneered or pre-determined. This allows random\r\ndeployment in inaccessible terrains or disaster\r\nrelief operations. On the other hand, this also\r\nmeans that sensor network protocols and algo-\r\nrithms must possess self-organizing capabilities.\r\nAnother unique feature of sensor networks is the\r\ncooperative effort of sensor nodes. Sensor nodes\r\nare fitted with an on-board processor. Instead of\r\nsending the raw data to the nodes responsible for\r\nthe fusion, sensor nodes use their processing abil-\r\nities to locally carry out simple computations and\r\ntransmit only the required and partially processed\r\ndata.\r\nThe above described features ensure a wide\r\nrange of applications for sensor networks. Some of\r\nthe application areas are health, military, and se-\r\ncurity. For example, the physiological data about\r\na patient can be monitored remotely by a doctor.\r\nWhile this is more convenient for the patient,\r\nit also allows the doctor to better understand the\r\npatients current condition. Sensor networks can\r\nalso be used to detect foreign chemical agents in\r\nthe air and the water. They can help to identify the\r\ntype, concentration, and location of pollutants. In\r\nessence, sensor networks will provide the end user\r\nwith intelligence and a better understanding of the\r\nenvironment. We envision that, in future, wireless\r\nsensor networks will be an integral part of our\r\nlives, more so than the present-day personal\r\ncomputers.\r\nRealization of these and other sensor network\r\napplications require wireless ad hoc networking\r\ntechniques. Although many protocols and algo-\r\nrithms have been proposed for traditional wireless\r\nad hoc networks, they are not well suited for the\r\nunique features and application requirements of\r\nsensor networks. To illustrate this point, the dif-\r\nferences between sensor networks and ad hoc\r\nnetworks [65] are outlined below:\r\n The number of sensor nodes in a sensor net-\r\nwork can be several orders of magnitude higher\r\nthan the nodes in an ad hoc network.\r\n Sensor nodes are densely deployed.\r\n Sensor nodes are prone to failures.\r\n The topology of a sensor network changes very\r\nfrequently.\r\n Sensor nodes mainly use broadcast communica-\r\ntion paradigm whereas most ad hoc networks\r\nare based on point-to-point communications.\r\n Sensor nodes are limited in power, computa-\r\ntional capacities, and memory.\r\n Sensor nodes may not have global identification\r\n(ID) because of the large amount of overhead\r\nand large number of sensors.\r\nSince large number of sensor nodes are densely\r\ndeployed, neighbor nodes may be very close to each\r\nother. Hence, multihop communication in sensor\r\nnetworks is expected to consume less power than\r\nthe traditional single hop communication. Fur-\r\nthermore, the transmission power levels can be\r\nkept low, which is highly desired in covert opera-\r\ntions. Multihop communication can also effec-\r\ntively overcome some of the signal propagation\r\neffects experienced in long-distance wireless com-\r\nmunication.\r\nOne of the most important constraints on sensor\r\nnodes is the low power consumption requirement.\r\nSensor nodes carry limited, generally irreplaceable,\r\npower sources. Therefore, while traditional net-\r\nworks aim to achieve high quality of service (QoS)\r\nprovisions, sensor network protocols must focus\r\nprimarily on power conservation. They must have\r\ninbuilt trade-off mechanisms that give the end user\r\nthe option of prolonging network lifetime at the\r\ncost of lower throughput or higher transmission\r\ndelay.\r\nMany researchers are currently engaged in de-\r\nveloping schemes that fulfill these requirements. In\r\nthis paper, we present a survey of protocols and\r\nalgorithms proposed thus far for sensor networks.\r\nOur aim is to provide a better understanding of the\r\ncurrent research issues in this field. We also at-\r\ntempt an investigation into pertaining design\r\nconstraints and outline the use of certain tools to\r\nmeet the design objectives.\r\nThe remainder of the paper is organized as\r\nfollows: In Section 2, we present some potential\r\nsensor network applications which show the use-\r\nfulness of sensor networks. In Section 3, we discuss\r\nthe factors that influence the sensor network\r\ndesign. We provide a detailed investigation of\r\ncurrent proposals in this area in Section 4. We\r\nconclude our paper in Section 5.\r\n394 I.F. Akyildiz et al. / Computer Networks 38 (2002) 393422\r\n2. Sensor networks applications\r\nSensor networks may consist of many different\r\ntypes of sensors such as seismic, low sampling rate\r\nmagnetic, thermal, visual, infrared, acoustic and\r\nradar, which are able to monitor a wide variety of\r\nambient conditions that include the following [23]:\r\n temperature,\r\n humidity,\r\n vehicular movement,\r\n lightning condition,\r\n pressure,\r\n soil makeup,\r\n noise levels,\r\n the presence or absence of certain kinds of ob-\r\njects,\r\n mechanical stress levels on attached objects, and\r\n the current characteristics such as speed, direc-\r\ntion, and size of an object.\r\nSensor nodes can be used for continuous sens-\r\ning, event detection, event ID, location sensing,\r\nand local control of actuators. The concept of\r\nmicro-sensing and wireless connection of these\r\nnodes promise many new application areas. We\r\ncategorize the applications into military, environ-\r\nment, health, home and other commercial areas. It\r\nis possible to expand this classification with more\r\ncategories such as space exploration, chemical\r\nprocessing and disaster relief.\r\n2.1. Military applications\r\nWireless sensor networks can be an integral part\r\nof military command, control, communications,\r\ncomputing, intelligence, surveillance, reconnaissance\r\nand targeting (C4ISRT) systems. The rapid de-\r\nployment, self-organization and fault tolerance\r\ncharacteristics of sensor networks make them a\r\nvery promising sensing technique for military\r\nC4ISRT. Since sensor networks are based on\r\nthe dense deployment of disposable and low-cost\r\nsensor nodes, destruction of some nodes by hostile\r\nactions does not affect a military operation as\r\nmuch as the destruction of a traditional sensor,\r\nwhich makes sensor networks concept a better\r\napproach for battlefields. Some of the military\r\napplications of sensor networks are monitoring\r\nfriendly forces, equipment and ammunition; bat-\r\ntlefield surveillance; reconnaissance of opposing\r\nforces and terrain; targeting; battle damage as-\r\nsessment; and nuclear, biological and chemical\r\n(NBC) attack detection and reconnaissance.\r\nMonitoring friendly forces, equipment and am-\r\nmunition: Leaders and commanders can constantly\r\nmonitor the status of friendly troops, the condi-\r\ntion and the availability of the equipment and\r\nthe ammunition in a battlefield by the use of sen-\r\nsor networks. Every troop, vehicle, equipment\r\nand critical ammunition can be attached with\r\nsmall sensors that report the status. These reports\r\nare gathered in sink nodes and sent to the troop\r\nleaders. The data can also be forwarded to the\r\nupper levels of the command hierarchy while being\r\naggregated with the data from other units at each\r\nlevel.\r\nBattlefield su'),
(326, 577, 'SPINS: Security Protocols for Sensor Networks', 'SPINS: Security Protocols for Sensor Networks\r\nAdrian Perrig, Robert Szewczyk, Victor Wen, David Culler, J. D. Tygar\r\nDepartment of Electrical Engineering and Computer Sciences\r\nUniversity of California, Berkeley\r\n{perrig, szewczyk, vwen, culler, tygar}@cs.berkeley.edu\r\nABSTRACT\r\nAs sensor networks edge closer towards wide-spread deployment,\r\nsecurity issues become a central concern. So far, the main research\r\nfocus has been on making sensor networks feasible and useful, and\r\nless emphasis was placed on security.\r\nWe design a suite of security building blocks that are optimized\r\nfor resource-constrained environments and wireless communica-\r\ntion. SPINS has two secure building blocks: SNEP and TESLA.\r\nSNEP provides the following important baseline security primi-\r\ntives: Data condentiality, two-party data authentication, and data\r\nfreshness. A particularly hard problem is to provide efcient broad-\r\ncast authentication, which is an important mechanism for sensor\r\nnetworks. TESLA is a new protocol which provides authenticated\r\nbroadcast for severely resource-constrained environments. We im-\r\nplemented the above protocols, and show that they are practical\r\neven on minimalistic hardware: The performance of the protocol\r\nsuite easily matches the data rate of our network. Additionally, we\r\ndemonstrate that the suite can be used for building higher level pro-\r\ntocols.\r\n1. INTRODUCTION\r\nWe envision a future where thousands to millions of small sen-\r\nsors form self-organizing wireless networks. How can we provide\r\nsecurity for these sensor networks? Security is not easy; compared\r\nwith conventional desktop computers, severe challenges exist \r\nthese sensors will have limited processing, storage, bandwidth, and\r\nenergy.\r\nDespite the challenges, security is important for these devices.\r\nWe gratefully acknowledge funding support for this research. This research was\r\nsponsored in part the United States Postal Service (contract USPS 102592-01-Z-\r\n0236), by the United States Defense Advanced Research Projects Agency (contracts\r\nDABT63-98-C-0038, Ninja, N66001-99-2-8913, Endeavour, and DABT63-96-\r\nC-0056, IRAM), by the United States National Science Foundation (grants FD99-\r\n79852 and RI EIA-9802069) and from gifts and grants from the California MICRO\r\nprogram, Intel Corporation, IBM, Sun Microsystems, and Philips Electronics. DARPA\r\nContract N66001-99-2-8913 is under the supervision of the Space and Naval Warfare\r\nSystems Center, San Diego. This paper represents the opinions of the authors and do\r\nnot necessarily represent the opinions or policies, either expressed or implied, of the\r\nUnited States government, of DARPA, NSF, USPS, or any other of its agencies, or any\r\nof the other funding sponsors.\r\nPermission to make digital or hard copies of all or part of this work for\r\npersonal or classroom use is granted without fee provided that copies are\r\nnot made or distributed for prot or commercial advantage and that copies\r\nbear this notice and the full citation on the rst page. To copy otherwise, to\r\nrepublish, to post on servers or to redistribute to lists, requires prior specic\r\npermission and/or a fee.\r\nMobile Computing and Networking 2001 Rome, Italy\r\nCopyright 2001 ACM X-XXXXX-XX-X/XX/XX ...$5.00.\r\nAs we describe below, we are deploying prototype wireless net-\r\nwork sensors at UC Berkeley. These sensors measure environmen-\r\ntal parameters and we are experimenting with having them control\r\nair conditioning and lighting systems. Serious privacy questions\r\narise if third parties can read or tamper with sensor data. In the\r\nfuture, we envision wireless sensor networks being used for emer-\r\ngency and life-critical systems  and here the questions of security\r\nare foremost.\r\nThis paper presents a set of Security Protocols for Sensor Net-\r\nworks, SPINS. The chief contributions of this paper are:\r\nOur main contributions include:\r\n Exploring the challenges for security in sensor networks.\r\n Designing and developing TESLA (the micro version of\r\nthe Timed, Efcient, Streaming, Loss-tolerant Authentication\r\nProtocol), providing authenticated streaming broadcast.\r\n Designing and developing SNEP (Secure Network Encryp-\r\ntion Protocol) providing data condentiality, two-party data\r\nauthentication, and data freshness, with low overhead.\r\n Designing and developing an authenticated routing protocol\r\nusing SPINS building blocks\r\n1.1 Sensor Hardware\r\nAt UC Berkeley, we are building prototype networks of small\r\nsensor devices under the SmartDust program [32]. Weve deployed\r\nthese in one of our EECS buildings, Cory Hall (see Figure 1).\r\nBy design, these sensors are inexpensive, low-power devices. As\r\na result, they have limited computational and communication re-\r\nsources. The sensors form a self-organizing wireless network and\r\nform a multihop routing topology. Typical applications may peri-\r\nodically transmit sensor readings for processing.\r\nOur current prototype consists of nodes, small battery powered\r\ndevices, that communicate with a more powerful base station, which\r\nin turn is connected to an outside network. As described below, the\r\nsensors form a self-organizing network (see Figure 1). Table 1 sum-\r\nmarizes the performance characteristics of these devices. At 4MHz,\r\nthey are slow and underpowered (the CPU has good support for bit\r\nand byte level I/O operations, but lacks support for many arithmetic\r\nand logic operations). They are only 8-bit processors (note that ac-\r\ncording to [40], 80% of all microprocessors shipped in 2000 were\r\n4 bit or 8 bit devices). Communication is slow at 10 Kbps.\r\nThe operating system is particularly interesting for these devices.\r\nWe use TinyOS [16]. This small, event-driven operating system\r\nconsumes ab operating system consumes almost half of 8KB of\r\ninstruction ash memory, leaving just 4500 bytes for security and\r\nthe application.\r\nCPU 8-bit, 4MHz\r\nStorage 8KB instruction ash\r\n512 bytes RAM\r\n512 bytes EEPROM\r\nCommunication 916 MHz radio\r\nBandwidth 10Kilobits per second\r\nOperating System TinyOS\r\nOS code space 3500 bytes\r\nAvailable code space 4500 bytes\r\nTable 1: Characteristics of prototype SmartDust Nodes\r\nIt is hard to imagine how signicantly more powerful devices\r\ncould be used without consuming large amounts of power. The en-\r\nergy source on our devices is a small battery, so we are stuck with\r\nrelatively limited computational devices. Similarly, since commu-\r\nnication over radio will be the most energy-consuming function\r\nperformed by these devices, we need to minimize communications\r\noverhead. The limited energy supplies creates tensions for secu-\r\nrity: on the one hand, security needs to limit its consumption of\r\nprocessor power; on the other hand, limited power supply limits\r\nkey lifetime (battery replacement is designed to reinitialize devices\r\nand zero out keys.) 1\r\n1.2 Is security on sensors possible?\r\nThese constraints make it impractical to use the majority of the\r\ncurrent secure algorithms, which were designed for powerful work-\r\nstations. For example, the working memory of a sensor node is in-\r\nsufcient to even hold the variables (of sufcient length to ensure\r\nsecurity) that are required in asymmetric cryptographic algorithms\r\n(e.g. RSA [35], Dife-Hellman [8]), let alone perform operations\r\nwith them.\r\nA particular challenge is broadcasting authenticated data to the\r\nentire sensor network. Current proposals for authenticated broad-\r\ncast are impractical for sensor networks. First, most proposals\r\nrely on asymmetric digital signatures for the authentication, which\r\nare impractical for multiple reasons (e.g. long signatures with high\r\ncommunication overhead of 50-1000 bytes per packet, very high\r\noverhead to create and verify the signature).\r\nBroadcast authentication is another problem. Even previously\r\nproposed purely symmetric solutions for broadcast authentication\r\nare impractical: Gennaro and Rohatgis initial work required over\r\n1 Kbyte of authentication information per packet [11], and Ro-\r\nhatgis improved k-time signature scheme requires over 300 bytes\r\nper packet [36]. Some of the authors have also proposed the au-\r\nthenticated streaming broadcast TESLA protocol [31], and TESLA\r\nis efcient for the Internet with regular desktop workstations, but\r\ndoes not scale down to our resource-starved sensor nodes. In this\r\npaper, we extend and adapt TESLA such that it becomes practical\r\nfor broadcast authentication for sensor networks. We call our new\r\nprotocol TESLA.\r\nWeve implemented all of these primitives, Our measurements\r\nshow that adding security to a highly resource-constrained sensor\r\nnetwork is feasible. The paper studies an authenticated routing pro-\r\ntocol and a two-party key agreement protocol, and demonstrates\r\nthat our security building blocks greatly facilitate the implementa-\r\ntion of a complete security solution for a sensor network.\r\nA common characteristic of sensor networks is their severely\r\nlimited energy supply. Ultimately, the available energy determines\r\n1Note that base stations differ from nodes in having longer-lived\r\nenergy supplies and having additional communications connections\r\nto outside networks.\r\nthe amount of computation, sensing, and communication a node\r\ncan perform in its lifetime. Alternatively, the power harvested from\r\nthe environment sets a bound on computation and communication\r\nper unit of time. In order to minimize the energy usage, a secu-\r\nrity subsystem should place minimal requirements on the proces-\r\nsor, and add minimal information to each message transmitted. On\r\nthe other hand, the limited lifespan of each node limits the life time\r\nof usable keys; we think of the battery replacement process as a\r\nrebirth.\r\nGiven the severe hardware and energy constraints, we must be\r\ncareful in the choice of cryptographic primitives and the security\r\nprotocols in the sensor networks.\r\n2. SYSTEM ASSUMPTIONS\r\nBefore we outline the security requirements and present our se-\r\ncurity infrastructure, we need to dene our system archi');
INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(327, 578, 'System architecture directions for networked sensors', 'System Architecture Directions for Networked Sensors\r\nJason Hill, Robert Szewczyk, Alec Woo, Seth Hollar, David Culler, Kristofer Pister\r\nApril 27, 2000\r\nAbstract\r\nTechnological progress in integrated, low-power, CMOS communication devices and sensors makes\r\na rich design space of networked sensors viable. They can be deeply embedded in the physical world\r\nor spread throughout our environment. The missing elements are an overall system architecture and a\r\nmethodology for systematic advance. To this end, we identify key requirements, develop a small device\r\nthat is representative of the class, design a tiny event-driven operating system, and show that it provides\r\nsupport for efficient modularity and concurrency-intensive operation. Our operating system fits in 178\r\nbytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches\r\nin the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a\r\ngroundwork for future architectural advances.\r\n1 Introduction\r\nAs the post-PC era emerges, several new niches of computer system design are taking shape with charac-\r\nteristics that are quite different from traditional desktop and server regimes. One of the most interesting of\r\nthese new design regimes is networked sensors. The networked sensor is enabled, in part, by Moores Law\r\npushing a given level of computing and storage into a smaller, cheaper, lower-power unit. However, three\r\nother trends are equally important: complete systems on a chip, integrated low-power communication, and\r\nintegrated low-power devices that interact with the physical world. The basic microcontroller building block\r\nincludes not just memory and processing, but non-volatile memory and interface resources, such as DAC,\r\nADCs, UARTs, interrupt controllers, and counters. Communication may take the form of wired, short-range\r\nRF, infrared, optical, or various techniques [18]. Sensors interact with various fields and forces to detect\r\nlight, heat, position, movement, chemical presence, and so on. In each of these areas, the technology is\r\ncrossing a critical threshold that makes networked sensors an exciting regime to apply systematic design\r\nmethods.\r\nToday, networked sensors can be constructed using commercial components on the scale of a square inch in\r\nsize and a fraction of a watt in power, using one or more microcontrollers connected to various sensor devices,\r\nusing I2C, SPI, or device specific protocols, and to small transceiver chips. One such sensor is described in\r\nthis study. It is also possible to construct the processing and storage equivalent of a 90s PC on the order\r\nof 10 in2 and 5-10 watts of power. Simple extrapolation suggests that the equivalent will eventually fit in\r\nthe square inch and sub-watt space-power niche and will run a scaled down Unix [12, 42] or an embedded\r\nmicrokernel [13, 26]. However, many researchers envision driving the networked sensor down into microscopic\r\nscales, as communication becomes integrated on-chip and micro-electrical mechanical (MEMS) devices make\r\na rich set of sensors available on the same CMOS chip at extremely low cost [38, 5]. networked sensors will\r\nbe integrated into their physical environment, perhaps even powered by ambient energy [32], and used in\r\nmany smart space scenarios. Others see ramping up the power associated with one-inch devices dramatically.\r\nIn either scenario, it is essential that the network sensor design regime be subjected to the same rigorous,\r\nworkload-driven, quantitative analysis that allowed microprocessor performance to advance so dramatically\r\nover the past 15 years. It should not be surprising that the unique characteristics of this regime give rise to\r\nvery different design trade-offs than current general-purpose systems.\r\n1\r\nThis paper provides an initial exploration of system architectures for networked sensors. The investigation\r\nis grounded in a prototype current generation device constructed from off-the-shelf components. Other\r\nresearch projects [38, 5] are trying to compress this class of devices onto a single chip; however, the key\r\nmissing technology is the system support to manage and operate the device. To address this problem, we\r\nhave developed a tiny microthreaded OS, called TinyOS, on the prototype platform. It draws strongly on\r\nprevious architectural work on lightweight thread support and efficient network interfaces. While working\r\nin this design regime two issues emerge strongly: these devices are concurrency intensive - several different\r\nflows of data must be kept moving simultaneously, and the system must provide efficient modularity -\r\nhardware specific and application specific components must snap together with little processing and storage\r\noverhead. We address these two problems in the context of current network sensor technology and our tiny\r\nmicrothreaded OS. Analysis of this solution provides valuable initial directions for architectural innovation.\r\nSection 2 outlines the design requirements that characterize the networked sensor regime and guide our\r\nmicrothreading approach. Section 3 describes our baseline, current-technology hardware design point. Sec-\r\ntion 4 develops our TinyOS for devices of this general class. Section 5 evaluates the effectiveness of the design\r\nagainst a collection of preliminary benchmarks. Section 6 contrasts our approach with that of prevailing\r\nembedded operating systems. Finally, Section 7 draws together the study and considers its implications for\r\narchitectural directions.\r\n2 Networked Sensor Characteristics\r\nThis section outlines the requirements that shape the design of network sensor systems; these observations\r\nare made more concrete by later sections.\r\nSmall physical size and low power consumption: At any point in technological evolution, size and power\r\nconstrain the processing, storage, and interconnect capability of the basic device. Obviously, reducing the\r\nsize and power required for a given capability are driving factors in the hardware design. At a system level,\r\nthe key observation is that these capabilities are limited and scarce. This sets the background for the rest\r\nof the solution, which must be austere and efficient.\r\nConcurrency-intensive operation: The primary mode of operation for these devices is to flow information from\r\nplace to place with a modest amount of processing on-the-fly, rather than to accept a command, stop, think,\r\nand respond. For example, information may be simultaneously captured from sensors, manipulated, and\r\nstreamed onto a network. Alternatively, data may be received from other nodes and forwarded in multihop\r\nrouting or bridging situations. There is little internal storage capacity, so buffering large amounts of data\r\nbetween the inbound and the outbound flows is unattractive. Moreover, each of the flows generally involve a\r\nlarge number of low-level events interleaved with higher-level processing. Some of these events have real-time\r\nrequirements, such as bounded jitter; some processing will extend over many such time-critical events.\r\nLimited Physical Parallelism and Controller Hierarchy: The number of independent controllers, the capabil-\r\nities of the controllers, and the sophistication of the processor-memory-switch level interconnect are much\r\nlower than in conventional systems. Typically, the sensor or actuator provides a primitive interface direct-\r\nly to a single-chip microcontroller. In contrast, conventional systems distribute the concurrent processing\r\nassociated with the collection of devices over multiple levels of controllers interconnected by an elaborate\r\nbus structure. Although future architectural developments may recreate a low duty-cycle analog of the\r\nconventional federation of controllers and interconnect, space and power constraints and limited physical\r\nconfigurability on-chip are likely to retain the need to support concurrency-intensive management of flows\r\nthrough the embedded microprocessor.\r\nDiversity in Design and Usage: Networked sensor devices will tend to be application specific, rather than\r\ngeneral purpose, and carry only the available hardware support actually needed for the application. As\r\nthere is a wide range of potential applications, the variation in physical devices is likely to be large. On\r\nany particular device, it is important to easily assemble just the software components required to synthesize\r\nthe application from the hardware components. Thus, these devices require an unusual degree of software\r\n2\r\nmodularity that must also be very efficient. A generic development environment is needed which allows spe-\r\ncialized applications to be constructed from a spectrum of devices without heavyweight interfaces. Moreover,\r\nit should be natural to migrate components across the hardware/software boundary as technology evolves.\r\nRobust Operation: These devices will be numerous, largely unattended, and expected to be operational a large\r\nfraction of the time. The application of traditional redundancy techniques is constrained by space and power\r\nlimitations. Although redundancy across devices is more attractive than within devices, the communication\r\ncost for cross device redundancy is prohibitive. Thus, enhancing the reliability of individual devices is\r\nessential. This reinforces the need for efficient modularity: the components should be as independent as\r\npossible and connected with narrow interfaces.\r\n3 Example Design Point\r\nTo ground our system design study, we have developed a small, flexible networked sensor platform that\r\nexpresses many of the key characteristics of the general class and represents the various internal interfaces\r\nusing currently available components [34]. A photograph and schematic for the hardware configuration of\r\nthis device appear in Figure 1. It consists of a microcontroller with internal flash program memory, data\r\nSRAM and data EEPROM, connected to a set of actuator and sensor devices, including LEDs, a'),
(328, 579, 'Snort - Lightweight Intrusion Detection for Networks', 'Proceedings of LISA 99: 13th Systems Administration Conference\r\nSeattle, Washington, USA, November 712, 1999\r\nS N O R T  L I G H T W E I G H T I N T R U S I O N \r\nD E T E C T I O N F O R N E T W O R K S \r\nMartin Roesch\r\nTHE ADVANCED COMPUTING SYSTEMS ASSOCIATION\r\n 1999 by The USENIX Association All Rights Reserved For more information about the USENIX Association:\r\nPhone: 1 510 528 8649 FAX: 1 510 548 5738 Email: office@usenix.org WWW: http://www.usenix.org\r\nRights to individual papers remain with the author or the authors employer.\r\n Permission is granted for noncommercial reproduction of the work for educational or research purposes.\r\nThis copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.\r\nSnort  Lightweight Intrusion\r\nDetection for Networks\r\nMartin Roesch  Stanford Telecommunications, Inc.\r\nABSTRACT\r\nNetwork intrusion detection systems (NIDS) are an important part of any network security\r\narchitecture. They provide a layer of defense which monitors network traffic for predefined\r\nsuspicious activity or patterns, and alert system administrators when potential hostile traffic is\r\ndetected. Commercial NIDS have many differences, but Information Systems departments must\r\nface the commonalities that they share such as significant system footprint, complex deployment\r\nand high monetary cost. Snort was designed to address these issues.\r\nIntroduction\r\nSnort fills an important ecological niche in the\r\nthe realm of network security: a cross-platform,\r\nlightweight network intrusion detection tool that can\r\nbe deployed to monitor small TCP/IP networks and\r\ndetect a wide variety of suspicious network traffic as\r\nwell as outright attacks. It can provide administrators\r\nwith enough data to make informed decisions on the\r\nproper course of action in the face of suspicious activ-\r\nity. Snort can also be deployed rapidly to fill potential\r\nholes in a networks security coverage, such as when a\r\nnew attack emerges and commercial security vendors\r\nare slow to release new attack recognition signatures.\r\nThis paper discusses the background of Snort and its\r\nrules-based traffic collection engine, as well as new\r\nand different applications where it can be very useful\r\nas a part of an integrated network security infrastruc-\r\nture.\r\nSnort is a tool for small, lightly utilized net-\r\nworks. Snort is useful when it is not cost efficient to\r\ndeploy commercial NIDS sensors. Modern commer-\r\ncial intrusion detection systems cost thousands of dol-\r\nlars at minimum, tens or even hundreds of thousands\r\nin extreme cases. Snort is available under the GNU\r\nGeneral Public License [GNU89], and is free for use\r\nin any environment, making the employment of Snort\r\nas a network security system more of a network man-\r\nagement and coordination issue than one of affordabil-\r\nity.\r\nWhat is lightweight intrusion detection?\r\nA lightweight intrusion detection system can eas-\r\nily be deployed on most any node of a network, with\r\nminimal disruption to operations. Lightweight IDS\r\nshould be cross-platform, have a small system foot-\r\nprint, and be easily configured by system administra-\r\ntors who need to implement a specific security solu-\r\ntion in a short amount of time. They can be any set of\r\nsoftware tools which can be assembled and put into\r\naction in response to evolving security situations.\r\nLightweight IDS are small, powerful, and flexible\r\nenough to be used as permanent elements of the net-\r\nwork security infrastructure.\r\nSnort is well suited to fill these roles, weighing\r\nin at roughly 100 kilobytes in its compressed source\r\ndistribution. On most modern architectures Snort takes\r\nonly a few minutes to compile and put into place, and\r\nperhaps another ten minutes to configure and activate.\r\nCompare this with many commercial NIDS, which\r\nrequire dedicated platforms and user training to deploy\r\nin a meaningful way. Snort can be configured and left\r\nrunning for long periods of time without requiring\r\nmonitoring or administrative maintenance, and can\r\ntherefore also be utilized as an integral part of most\r\nnetwork security infrastructures.\r\nWhat is Snort?\r\nSnort is a libpcap-based [PCAP94] packet sniffer\r\nand logger that can be used as a lightweight network\r\nintrusion detection system (NIDS). It features rules\r\nbased logging to perform content pattern matching and\r\ndetect a variety of attacks and probes, such as buffer\r\noverflows [ALE96], stealth port scans, CGI attacks,\r\nSMB probes, and much more. Snort has real-time\r\nalerting capability, with alerts being sent to syslog,\r\nServer Message Block (SMB) WinPopup messages,\r\nor a separate alert file. Snort is configured using\r\ncommand line switches and optional Berkeley Packet\r\nFilter [BPF93] commands. The detection engine is\r\nprogrammed using a simple language that describes\r\nper packet tests and actions. Ease of use simplifies\r\nand expedites the development of new exploit detec-\r\ntion rules. For example, when the IIS Showcode\r\n[IISBT99] web exploits were revealed on the Bugtraq\r\nmailing list [BTQ99], Snort rules to detect the probes\r\nwere available within a few hours.\r\nSnort vs. The World!\r\nSnort shares commonalities with both sniffers\r\nand NIDS. Two programs that lend themselves to\r\ndirect comparison with Snort, tcpdump and Network\r\nFlight Recorder [NFR97], will be examined and con-\r\ntrasted in this section. In many cases, Snort is finan-\r\ncially, technically, and/or administratively easier to\r\nimplement than other Open Source [OSS98] or com-\r\nmercially available tools.\r\n1999 LISA XIII  November 7-12, 1999  Seattle, WA 229\r\nSnort  Lightweight Intrusion Detection for Networks Roesch\r\nHow Is Snort Different From tcpdump?\r\nSnort is cosmetically similar to tcpdump\r\n[TCPD91] but is more focused on the security applica-\r\ntions of packet sniffing. The major feature that Snort\r\nhas which tcpdump does not is packet payload inspec-\r\ntion. Snort decodes the application layer of a packet\r\nand can be given rules to collect traffic that has spe-\r\ncific data contained within its application layer. This\r\nallows Snort to detect many types of hostile activity,\r\nincluding buffer overflows, CGI scans, or any other\r\ndata in the packet payload that can be characterized in\r\na unique detection fingerprint.\r\nAnother Snort advantage is that its decoded out-\r\nput display is somewhat more user friendly than tcp-\r\ndumps output. Snort does not currently lookup host\r\nnames or port names while running, which is a func-\r\ntion that tcpdump can perform. Snort is focused on\r\ncollecting packets as quickly as possible and process-\r\ning them in the Snort detection engine. Performing\r\nrun-time host name lookup is not conducive to high\r\nperformance packet analysis. Figure 1 shows typical\r\nSnort output for a telnet banner display, and Figure 2\r\nshows the same packet as displayed by tcpdump.\r\n20:59:49.153313 0:10:4B:D:A9:66 -> 0:60:97:7:C2:8E type:0x800 len:0x7D\r\n192.168.1.3:23 -> 192.168.1.4:1031 TCP TTL:64 TOS:0x10 DF\r\n***PA* Seq: 0xDF4A6536 Ack: 0xB3A6FD01 Win: 0x446A\r\nFF FA 22 03 03 E2 03 04 82 0F 07 E2 1C 08 82 04 ...............\r\n09 C2 1A 0A 82 7F 0B 82 15 0F 82 11 10 82 13 FF ................\r\nF0 0D 0A 46 72 65 65 42 53 44 20 28 65 6C 72 69 ...FreeBSD (elri\r\n63 2E 68 6F 6D 65 2E 6E 65 74 29 20 28 74 74 79 c.home.net) (tty\r\n70 30 29 0D 0A 0D 0A p0)....\r\nFigure 1: Typical Snort telnet packet display.\r\n20:59:49.153313 0:10:4b:d:a9:66 0:60:97:7:c2:8e 0800 125: 192.168.1.3.23 >\r\n192.168.1.4.1031: P 76:147(71) ack 194 win 17514 (DF) [tos 0x10] (ttl 64,\r\nid 660)\r\n4510 006f 0294 4000 4006 b48d c0a8 0103\r\nc0a8 0104 0017 0407 df4a 6536 b3a6 fd01\r\n5018 446a d2ad 0000 fffa 2203 03e2 0304\r\n820f 07e2 1c08 8204 09c2 1a0a 827f 0b82\r\n150f 8211 1082 13ff f00d 0a46 7265 6542\r\n5344 2028 656c 7269 632e 686f 6d65 2e6e\r\n6574 2920 2874 7479 7030 290d 0a0d 0a\r\nFigure 2: The same telnet packet as displayed by tcpdump.\r\nOne powerful feature that Snort and tcpdump\r\nshare, is the capability to filter traffic with Berkeley\r\nPacket Filter (BPF) commands. This allows traffic to\r\nbe collected based upon a variety of specific packet\r\nfields. For example, both tools may be instructed via\r\nBPF commands to process TCP traffic only. While\r\ntcpdump would collect all TCP traffic, Snort can uti-\r\nlize its flexible rules set to perform additional func-\r\ntions, such as searching out and recording only those\r\npackets that have their TCP flags set a particular way\r\nor containing web requests that amount to CGI vulner-\r\nability probes. The SHADOW IDS [SHD98] from the\r\nNaval Surface Warfare Center is based on tcpdump\r\nand uses extensive BPF filtering. SHADOW is dis-\r\ncussed in more detail near the end of this paper.\r\nSnort and NFR\r\nPerhaps the best comparison of Snort to NFR is\r\nthe analogy of Snort as little brother to NFRs college-\r\nbound football hero. Snort shares some of the same\r\nconcepts of functionality as NFR, but NFR is a more\r\nflexible and complete network analysis tool. That said,\r\nthe little brother idea could be extended in that Snort\r\ntends to fit into small places and is somewhat more\r\nnimble than NFR. For example, NFRs packet fil-\r\ntering n-code language is a serious, full featured\r\nscripting language, while Snorts rules are more one\r\ndimensional. On the other hand, writing a Snort rule to\r\ndetect a new attack takes only minutes once the attack\r\nsignature has been determined. See Appendix A for an\r\nexample of a simple web detection rule written in n-\r\ncode and the analogous Snort rule.\r\nNFR also has a more complete feature set than\r\nSnort, including IP fragmentation reassembly and TCP\r\nstream decoding. These features are essential in any\r\ncommercial product that is meant to perform mission\r\ncritical intrusion detection, and NFR was the first\r\nproduct which could defeat anti-NIDS attacks outlined\r\nby Ptacek and Newsham [PTA98]. Presently, Snort\r\ndoes not implement TCP stream reassembly, but\r\nfuture versions will implement this capability. Snort\r\ncurrently addresses IP fragmentation with a'),
(329, 580, 'A tutorial on learning with Bayesian networks', 'Beta(3,2) Beta(2,2) Beta(1,1) Beta(19,39)\n\n\n\n\n\n\n\n\nFraud Age\nGas\np(f=yes)\0=\00..00001\np(a=<30)\0=\00.25\np(a=30\050)\0=\00.40\np(j=yes|f=yes,a=*,s=*)\0=\00.05\np(j=yes|f=no,a=<30,s=male)\0=\00..0001\np(j=yes|f=no,a=30\050 ,s=male)\0=\00.0004\np(j=yes|f=no,a=>50,s=male)\0=\00.0002\np(j=yes|f=no,a=<30,s=female)\0=\00..0005\np(j=yes|f=no,a=30\050 ,s=female)\0=\00.002\np(j=yes|f=no,a=>50,s=female)\0=\00.001\np(g=yes|f=yes)\0=\00.2\np(g=yes|f=no)\0=\00.01\nSex\nJewelry\np(s=male)\0=\00.5\nsample\0\01\nsample\0\02\nQ y|x\nQ x\nQ y|x\nX Y\nX Y\n\0\n\n\n\n\nFinding\01\nAilment\nFinding\02\nFinding\0 n\n.\0.\0.\n\n\n\n\n\n\ncase\0#\0\0\0 x\n1\n\0\0\0 x\n2\n\0\0\0 x\n3\n\0\0\0 x\n37\n1\n2\n3\n4\n10,000\n3\n2\n1\n3\n2\n3\n2\n3\n2\n2\n2\n2\n3\n3\n2\n4\n3\n3\n1\n3\n\0 \0 \0 \0 17\n25\n6 5 4\n19\n27\n20\n10 21\n37\n31\n11 32\n33\n22\n15\n14\n\0\n23\n13\n16\n29\n8 9\n28\n12\n34 35 36\n24\n30\n7\n26 18\n3 2 1\n(a)\n(b)\n17\n25 18 26\n3\n6 5 4\n19\n27\n20\n10 21\n35 37 36\n31\n11 32 34\n12\n24\n33\n22\n15\n14\n\0\n23\n13\n16\n29\n30\n7 8 9\n28\n2 1\n(c)\n17\n25\n6 5 4\n19\n27\n20\n10 21\n37\n31\n11 32\n33\n22\n15\n14\n\0\n23\n13\n16\n29\n8 9\n28\n12\n34 35 36\n24\n30\n7\n26 18\n3 2 1\n(d)\ndeleted\nD\n3\nH\nD\n2\nD\n1\nC\n1 C\n2\nC\n3\nC\n4\nC\n5\n\n\n\n\n\n(a) (b)\nBuy\nAd\n(a) (b)\nAd\nBuy\nAd\nH\nBuy\nAd\nS\nBuy\nIncome Location\nAd\nBuy\nSES\nSEX\nPE\nIQ\nCP\nlog ( | )\n(| ) .\npDS\npS D\nh\nh\n1\n1\n45653\n10\n=-\n=\nSES\nSEX\nPE\nIQ\nCP\nlog ( | )\n(| ) .\npDS\npS D\nh\nh\n2\n2\n10\n45699\n12 10\n=-\n=\n-p(male)\0=\00.48\nSES\nH\nSEX\nPE\nIQ\nCP\np(H=0)\0=\00.63\np(H=1)\0=\00.37\nH\n0\n1\n0\n1\nPE\nlow\nlow\nhigh\nhigh\np(IQ=high|PE,H)\n0.098\n0.22\n0.21\n0.49\nH\nlow\nhigh\np(SES=high|H)\n0.088\n0.51\nSEX\nmale\nfemale\nmale\nfemale\nSES\nlow\nlow\nhigh\nhigh\np(PE=high|SES,SEX)\n0.32\n0.166\n0.86\n0.81\nSES\nlow\nlow\nlow\nlow\nhigh\nhigh\nhigh\nhigh\nPE\nlow\nhigh\nlow\nhigh\nlow\nhigh\nlow\nhigh\nIQ\nlow\nlow\nhigh\nhigh\nlow\nlow\nhigh\nhigh\np(CP=yes|SES,IQ,PE)\n0.011\n0.170\n0.124\n0.53\n0.093\n0.39\n0.24\n0.84\nlog ( | ) pS D\nh\n@- 45629\n\n'),
(330, 581, 'Next century challenges: Scalable coordination in sensor networks', 'Next Century Challenges: Scalable Coordination in Sensor Networks \r\nDeborah Estrin \r\nRamesh Govindan \r\nJohn Heidemann \r\nSatish Kumar \r\nUSC/Information Sciences Institute \r\n4676 Admiralty Way \r\nMarina de1 Rey, CA 90292, USA \r\n{estrin,govinda.n,joh,kkumar}@isi.edu \r\nAbstract \r\nNetworked sensors-those that coordinate amongst them- \r\nselves to achieve a larger sensing task-will revolutionize \r\ninformation gathering and processing both in urban envi- \r\nronments and in inhospitable terrain. The sheer numbers of \r\nthese sensors and the expected dynamics in these environ- \r\nments present unique challenges in the design of unattended \r\nautonomous sensor networks. These challenges lead us to \r\nhypothesize that sensor network coordination applications \r\nmay need to be structured differently from traditional net- \r\nwork applications. In particular, we believe that localized \r\nalgorithms (in which simple local node behavior achieves a \r\ndesired global objective) may be necessary for sensor net- \r\nwork coordination. In this paper, we describe localized al- \r\ngorithms, and then discuss directed diffusion, a simple com- \r\nmunication model for describing localized algorithms. \r\n1 Introduction \r\nIntegrated low-power sensing devices will permit remote ob- \r\nject monitoring and tracking in many different contexts: in \r\nthe field (vehicles, equipment, personnel), the office building \r\n(projectors, furniture, books, people), the hospital ward (sy- \r\nringes, bandages, IVs) and the factory floor (motors, small \r\nrobotic devices). Networking these sensors-mpowering \r\nthem with the ability to coordinate amongst themselves on a \r\nlarger sensing task-will revolutionize information gathering \r\nand processing in many situations. Large scale, dynamically \r\nchanging, and robust sensor colonies can be deployed in in- \r\nhospitable physical environments such as remote geographic \r\nregions or toxic urban locations. They will also enable low \r\nmaintenance sensing in more benign, but less accessible, en- \r\nvironments: large industrial plants, aircraft interiors etc. \r\nTo motivate the challenges in designing these sensor net- \r\nworks, consider the following scenario. Several thousand \r\nsensors are rapidly deployed (eig., thrown from an aircraft) \r\nin remote terrain. The sensors coordinate to establish a \r\ncommunication network, divide the task of mapping and \r\nmonitoring the terrain amongst themselves in an energy- \r\npermission to make digital or hard copies of all or part ofthjs work fo, \r\nPersonal or classroom use is granted without fee provided that copies \r\nare not made or distributed for profit or commercial advantage and that \r\ncopies bear this notice and the full citation on the first page. l-o copy \r\nothclwisc, to republish, to post on servers or to redistribute to lists. \r\nrequires prior specific permission andior a fee. \r\nMobicom 99 Seattle Washington USA \r\nCopyright ACM 1999 I-581 13-142-9/99/08...$5.00 \r\n263 \r\nefficient manner, adapt their overall sensing accuracy to the \r\nremaining total resources, and re-organize upon sensor fail- \r\nure. When additional sensors are added or old sensors fail, \r\nthe sensors re-organize themselves to take advantage of the \r\nadded system resources. \r\nSeveral aspects of this scenario present systems design \r\nchallenges different from those posed by existing computer \r\nnetworks (Section 2). The sheer numbers of these de- \r\nvices, and their unattended deployment, will preclude re- \r\nliance on broadcast communication or the configuration cur- \r\nrently needed to deploy and operate networked devices. De- \r\nvices may be battery constrained or subject to hostile en- \r\nvironments, so individual device failure will be a regular or \r\ncommon event. In addition, the configuration devices will \r\nfrequently change in terms of position, reachability, power \r\navailability, and even task details. Finally, because these \r\ndevices interact with the physical environment, they, and \r\nthe network as a whole, will experience a significant range \r\nof task dynamics. \r\nThe WINS project [l] has considered device-level com- \r\nmunication primitives needed to satisfy these requirements. \r\nHowever, these requirements potentially affect many other \r\naspects of network design: routing and addressing mech- \r\nanisms, naming and binding services, application architec- \r\ntures, security mechanisms, and so forth. This paper focuses \r\non the principles underlying the design of services and appli- \r\ncations in sensor networks. In particular, since the sensing \r\nis inherently distributed, we argue that sensor network ap- \r\nplications will themselves be distributed. \r\nMany of the lessons learned from Internet and mobile \r\nnetwork design will be applicable to designing sensor net- \r\nwork applications. However, this paper hypothesizes that \r\nsensor networks have different enough requirements to at \r\nleast warrant re-considering the overall structure of appli- \r\ncations and services. Specifically, we believe there are sig- \r\nnificant robustness and scalability advantages to designing \r\napplications using localized algorithms-where sensors only \r\ninteract with other sensors in a restricted vicinity, but nev- \r\nertheless collectively achieve a desired global objective (Sec- \r\ntion 3). We also describe directed diffusion, a promising \r\nmodel for describing localized algorithms (Section 4). \r\nOur research project is starting to investigate the design \r\nof localized algorithms using the directed diffusion model. \r\nThese ideas were developed in the context of a DARPA \r\nISAT study, chaired by one of the authors (Estrin). The \r\nidea of applying directed diffusion to this problem domain \r\nis due to Van Jacobson, based on experiences with reliable \r\nmulticast [2) and adaptive Web caching design. \r\n2 Sensor Network Challenges \r\nBy early next century, sensor integration, coupled with un- \r\nceasing electronic miniaturization, will make it possible to \r\nproduce extremely inexpensive sensing devices. These de- \r\nvices will be able to monitor a wide variety of ambient \r\nconditions: temperature, pressure, humidity, soil makeup, \r\nvehicular movement, noise levels, lighting conditions, the \r\npresence or absence of certain kinds of objects, mechanical \r\nstress levels on attached objects, and so on. These devices \r\nwill also be equipped with significant (i.e., comparable to \r\ntodays high-end portable computers) processing, memory, \r\nand wireless communication capabilities. \r\nEmerging low-level and low-power wireless communica- \r\ntion protocols will enable us to network these sensors. This \r\ncapability will add a new dimension to the capabilities of \r\nsensors: Sensors will be able coordinate amongst themselves \r\non a higher-level sensing task (e.g., reporting, with greater \r\naccuracy than possible with a single sensor, the exact speed, \r\ndirection, size, and other characteristics of an approaching \r\nvehicle). \r\nNetworking inexpensive sensors can revolutionize infor- \r\nmation gathering in a variety of situations. Consider the fol- \r\nlowing scenarios, arranged in increasing order of complexity: \r\nl Each item of inventory in a factory warehouse or office \r\ncomplex has, attached to it, a tag. Stick-on sensors, \r\ndiscreetly attached to walls, or embedded in floors and \r\nceilings, track the location history and use of items. \r\nThe sensor network can automatically locate items, \r\nreport on those needing servicing, analyze long-term \r\ncorrelations between workflow and wear, report unex- \r\npected large-scale movements of items or significant \r\nchanges in inventory levels. Some systems today (for \r\nexample, those based on bar-codes) provide inventory \r\ntracking; full sensor-net based systems will eliminate \r\nmanual scanning and provide more data than simply \r\nlocation. \r\nl Thousands of disposable sensors are densely scattered \r\nover a disaster area. Some of them fall into regions af- \r\nfected by the disaster, say a fire-these sensors are de- \r\nstroyed. The remaining sensors collectively map these \r\naffected regions, direct the nearest emergency response \r\nteams to affected sites, or find safe evacuation paths. \r\nDisaster recovery today is by comparison very human \r\nintensive. \r\nl Every vehicle in a large metropolis has one or more at- \r\ntached sensors. These sensors are capable of detecting \r\ntheir location; vehicle sizes, speeds and densities; road \r\nconditions and so on. As vehicles pass each other, they \r\nexchange information summaries. These summaries \r\neventually diffuse across sections of the metropolis. \r\nDrivers can plan alternate routes, estimate trip times, \r\nand be warned of dangerous driving conditions. Wn- \r\nlike the centralized systems sometimes seen today, one \r\nbased on local communication would scale as the num- \r\nber of vehicles grows and provide much greater local \r\ndetail. \r\nThese futuristic scenarios bring out the two key require- \r\nments of sensor networks: support for very large numbers \r\nof unattended autonomous nodes and adaptivity to environ- \r\nment and task dynamics. \r\nMany large-scale networks exist today; the Internet is a \r\nprime example. Sensor networks present a fundamentally \r\nmore difficult problem, though, because the ratio of com- \r\nmunicating nodes to users is much greater. Each personal \r\ncomputer on the Internet has a user who can resolve or at \r\nleast report all manner of minor errors and problems. Thii \r\nhuman element allows the Internet to function with much \r\nless robust software. Sensor networks, by comparison will \r\nexist with the ratio of thousands of nodes per user (or more). \r\nAt such ratios, it is impossible to pay special attention to \r\nany individual node. Furthermore, even if it were possible to \r\nconsider each node, sensors may be inaccessible, either be- \r\ncause they are embedded in physical structures, or thrown \r\ninto inhospitable terrain. Thus, for such a system to be ef- \r\nfective, it must provide ezception-free, unattended operation \r\n(the term exception-free is due to Mark Weiser). \r\nIt is not completely true that there'),
(331, 582, 'Learning Bayesian networks: The combination of knowledge and statistical data', 'Learning Bayesian Networks: The Combination of\nKnowledge and Statistical Data\nDavid Heckerman Dan Geiger\n\nMicrosoft Research, Bldg 9S\nRedmond, WA 98052-6399\nheckerma@microsoft.com, dang@cs.technion.ac.il, dmax@cs.ucla.edu\nDavid M. Chickering\nAbstract\nWe describe scoring metrics for learning\nBayesian networks from a combination of\nuser knowledge and statistical data. We\nidentify two important properties of metrics,\nwhich we call event equivalence and parame-\nter modularity. These properties have been\nmostly ignored, but when combined, greatly\nsimplify the encoding of a users prior knowl-\nedge. In particular, a user can express his\nknowledgefor the most partas a single\nprior Bayesian network for the domain.\n1 Introduction\nThe elds of ArticialIntelligence andStatistics share\na common goal of modeling real-world phenomena.\nWhereasAIresearchershaveemphasizedaknowledge-\nbased approach to achieving this goal, statisticians\nhave traditionally emphasized a data-based approach.\nIn this paper, we present a unication of the two ap-\nproaches. In particular, we develop algorithms based\non Bayesian principles that take as input (1) a users\nprior knowledge expressedfor the most partas a\nprior Bayesian network and (2) statistical data, and\nreturns one or more improved Bayesian networks.\nSeveral researchers have examined methods for learn-\ning Bayesian networks from data, including Cooper\nand Herskovits (1991,1992), Buntine (1991), and\nSpiegelhalter et al. (1993) (herein referred to as CH,\nBuntine, and SDLC, respectively). These methods all\nhavethe samebasiccomponents: a scoringmetricand\na search procedure. The metric computes a score that\nis proportional to the posterior probability of a net-\nwork structure, given data and a users prior knowl-\nedge. The search procedure generates networks for\n\nAuthors primary aliation: Computer Science De-\npartment, Technion, Haifa 32000, Israel.\nevaluation by the scoring metric. These methods use\nthe two components to identify a network or set of\nnetworks with high posterior probabilities, and these\nnetworks are then used to predict future events.\nIn this paper, we concentrate on scoring metrics. Al-\nthough we restrict ourselves to domains containing\nonlydiscretevariables,asweshowinGeigerandHeck-\nerman(1994),ourmetricscanbeextendedtodomains\ncontainingcontinuousvariables. Amajorcontribution\nof this paper is that we develop our metrics from a\nset of consistent properties and assumptions. Two of\nthese, called parameter modularity and event equiva-\nlence, have been ignored for the most part, and their\ncombined ramications have not been explored. The\nassumption of parameter modularity, which has been\nmadeimplicitlybyCH,Buntine,andSDLC,addresses\nthe relationship among prior distributions of param-\neters for dierent Bayesian-network structures. The\nproperty of event equivalence says that two Bayesian-\nnetwork structures that represent the same set of in-\ndependence assertions should correspond to the same\nevent and therefore receive the same score. We pro-\nvide justications for these assumptions, and show\nthat when combined with assumptions about learn-\ning Bayesian networks made previously, we obtain a\nstraightforwardmethod for combining user knowledge\nand statistical data that makes use of a priornetwork.\nOur approach is to be contrasted with those of CH\nand Buntine who do not make use of a prior network,\nand to those of CH and SDLC who do not satisfy the\nproperty of event equivalence.\nOur identication of the principle of event equivalence\narises from a subtle distinction between two types of\nBayesian networks. The rst type, called belief net-\nworks, represents only assertions of conditional inde-\npendence. The second type, called causal networks,\nrepresents assertions of cause and eect as well as as-\nsertions of independence. In this paper, we argue that\nmetrics for belief networks should satisfy event equiv-\nalence, whereas metrics for causal networks need not.Our score-equivalentmetric for belief networks is sim-\nilar to metrics described by York (1992), Dawid and\nLauritzen(1993)andMadiganandRaferty(1994),ex-\nceptthatourmetricscoresdirectednetworks,whereas\ntheir metrics score undirected networks. In this pa-\nper, we concentrateon directedmodels rather thanon\nundirected models, because we believe that users nd\nthe former easier to build and interpret.\n2 Belief Networks and Notation\nConsideradomainU ofndiscretevariablesx\n1\n,...,x\nn\n.\nWe use lower-case letters to refer to variables and\nupper-case letters to refer to sets of variables. We\nwrite x\ni\n= k when we observe that variable x\ni\nis in\nstate k. We use p(x = i|y = j,) to denote the proba-\nbilityofa personwithbackgroundknowledge forthe\nobservation x = i, given the observation y = j. When\nwe observe the state for every variable in set X, we\ncall this set of observations an instance of X. We use\np(X|Y,) to denote the set of probabilities for all pos-\nsible observationsof X, givenall possible observations\nof Y. The joint space of U is the set of all instances\nof U. The joint probability distribution over U is the\nprobability distribution over the joint space of U.\nAbeliefnetworktherstofthetwotypesofBayesian\nnetworks that we considerrepresents a joint prob-\nability distribution over U by encoding assertions of\nconditional independence as well as a collection of\nprobabilitydistributions. Fromthe chainruleofprob-\nability, we know\np(x\n1\n,...,x\nn\n|)=\nn\nY\ni=1\np(x\ni\n|x\n1\n,...,x\ni1\n,) (1)\nFor each variable x\ni\n, let \ni\n{x\n1\n,...,x\ni1\n} be a set\nof variables that renders x\ni\nand{x\n1\n,...,x\ni1\n} condi-\ntionally independent. That is,\np(x\ni\n|x\n1\n,...,x\ni1\n,)= p(x\ni\n|\ni\n,) (2)\nA belief network is a pair (B\nS\n,B\nP\n), where B\nS\nis a\nbelief-networkstructure that encodes the assertionsof\nconditional independence in Equation 2, and B\nP\nis a\nset of probability distributions corresponding to that\nstructure. Inparticular,B\nS\nis adirectedacyclicgraph\nsuchthat (1) eachvariablein U correspondsto a node\nin B\nS\n, and (2) the parents of the node corresponding\nto x\ni\nare the nodes corresponding to the variables in\n\ni\n. (In the remainder of this paper, we use x\ni\nto refer\nto both the variable and its corresponding node in a\ngraph.) Associated with node x\ni\nin B\nS\nare the prob-\nability distributions p(x\ni\n|\ni\n,). B\nP\nis the union of\nthese distributions. Combining Equations 1 and 2, we\nsee that any belief network for U uniquely determines\na joint probability distribution for U. That is,\np(x\n1\n,...,x\nn\n|) =\nn\nY\ni=1\np(x\ni\n|\ni\n,) (3)\nA minimal belief network is a belief network where\nEquation 2 is violated if any arc is removed. Thus,\na minimal belief network represents both assertions of\nindependence and assertions of dependence.\n3 Metrics for Belief Networks:\nPrevious Work\nIn this section, we summarize previous work,\npresentedfor examplein CH, Buntine, and SDLC\non the computation of a score for a belief-network\nstructure B\nS\n, given a set of cases D ={C\n1\n,...,C\nm\n}.\nEach case C\ni\nis the observation of one or more vari-\nables in U. We sometimes refer to D as a database.\nABayesianmeasureofthegoodnessofabelief-network\nstructure is its posterior probability given a database:\np(B\nS\n|D,) =c p(B\nS\n|) p(D|B\nS\n,)\nwhere c= 1/p(D|)=1/\nP\nBS\np(B\nS\n|) p(D|B\nS\n,) is a\nnormalizationconstant. For evensmalldomains,how-\never, there are too many network structures to sum\nover in order to determine the constant. Therefore re-\nsearchers have used p(B\nS\n|) p(D|B\nS\n,) = p(D,B\nS\n|)\nasa network-structurescore. We notethatthis metric\ntreatsallvariablesasbeingequallyimportant,butcan\nbe generalized [Spiegelhalter et al., 1993].\nTocomputep(D,B\nS\n|)inclosedform,researcherstyp-\nically have made ve assumptions, which we explicate\nhere.\nAssumption 1 The database D is a multinomial\nsample from some belief network (B\nS\n,B\nP\n).\nThere are several assumptions implicit in Assump-\ntion 1. One is that all variables in U are discrete. We\nmodify this assumption in another paper in this pro-\nceedings [Geiger and Heckerman, 1994]. Another as-\nsumptionisthattheusermaybeuncertainastowhich\nbelief-network structure is generating the data. This\nuncertainty is encoded in the prior probabilities for\nnetwork structurep(B\nS\n|). Also implicit is that, given\nthe data comes from a particular network structure,\nthe user may be uncertain about the probabilities for\nthat structure. These probabilities actually should be\nthought of as being long-run fractions that we would\nseein a verylargedatabase,and arecalled parameters\nin the statistical literature. Finally, we note that As-\nsumption 1 implies that the processes generating the\ndata do not change in time.case  1\ncase  2\nM\nx y\nx y\nq y x |\nq y x |\nq x\nFigure 1: Illustration of Assumptions 1 and 2 for the\nnetwork structure xy, where x and y are binary.\nAssumption 1 can be represented in a belief network.\nFigure 1 illustrates the assumption for the network\nstructure x y where x and y are binary variables.\n(We shall use this two-variable domain to illustrate\nmany of the points in this paper.) The parameter \nx\nrepresents the long-run fraction of cases where x is\nobserved to be true. Given \nx\n, the observations of x\nin each case are independent. The parameters \ny|x\nand \ny| x\nrepresentthe long-runfractionof cases where\ny is observed to be true, in those cases where x is\nobserved to be true and false, respectively. If these\ntwo parameters are known, then the observations of y\nin any two cases are independent, provided x is also\nobserved for at least one of those cases.\nIn general, given a belief-network structure B\nS\nfor\nU = {x\n1\n,...,x\nn\n}, we use r\ni\nto denote the number\nof states of variable x\ni\n, and q\ni\n=\nQ\nx\nl\ni\nr\nl\nto denote\nthe number of instances of \ni\n. We use the integer j\nto index these instances. That is, we write \ni\n= j to\ndenote the observation of the jth instance of the par-\nents of x\ni\n. We use \nijk\nto denote the long-run fraction\nof cases where x\ni\n= k, in those cases where \ni\n= j.\nWe use \nij\nto denote the union of \nijk\nover k, and\n\nBS\nto denote the union of \nij\nfor all insta');
INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(332, 583, 'Neural Network-Based Face Detection', 'IEEE\0TRANSACTIONS\0ON\0PATTERN\0ANALYSIS\0AND\0MACHINE\0INTELLIGENCE,\0\0VOL.\0\020,\0\0NO.\0\01,\0\0JANUARY\0\01998 23\nNeural\0Network\0Based\0Face\0Detection\nHenry\0A.\0Rowley,\0 Student\0Member,\0IEEE, \0Shumeet\0Baluja,\0and\0Takeo\0Kanade, \0Fellow,\0IEEE\nAbstractWe\0present\0a\0neural\0network\0based\0upright\0frontal\0face\0detection\0system.\0A\0retinally\0connected\0neural\0network\0examines\nsmall\0windows\0of\0an\0image\0and\0decides\0whether\0each\0window\0contains\0a\0face.\0The\0system\0arbitrates\0between\0multiple\0networks\0to\nimprove\0performance\0over\0a\0single\0network.\0We\0present\0a\0straightforward\0procedure\0for\0aligning\0positive\0face\0examples\0for\0train ing.\nTo\0collect\0negative\0examples,\0we\0use\0a\0bootstrap\0algorithm,\0which\0adds\0false\0detections\0into\0the\0training\0set\0as\0training\0progr esses.\nThis\0eliminates\0the\0difficult\0task\0of\0manually\0selecting\0nonface\0training\0examples,\0which\0must\0be\0chosen\0to\0span\0the\0entire\0spa ce\0of\nnonface\0images.\0Simple\0heuristics,\0such\0as\0using\0the\0fact\0that\0faces\0rarely\0overlap\0in\0images,\0can\0further\0improve\0the\0accuracy .\nComparisons\0with\0several\0other\0state\0of\0the\0art\0face\0detection\0systems\0are\0presented,\0showing\0that\0our\0system\0has\0comparable\nperformance\0in\0terms\0of\0detection\0and\0false\0positive\0rates.\nIndex\0Terms Face\0detection,\0pattern\0recognition,\0computer\0vision,\0artificial\0neural\0networks,\0machine\0learning.\n\0\0\0 F \0\0\0\n1INTRODUCTION\nN\0this\0paper,\0we\0present\0a\0neural\0network\0based\0algo\0\nrithm\0to\0detect\0upright,\0frontal\0views\0of\0faces\0in\0gray\0\nscale\0images.\n1\n\0The\0algorithm\0works\0by\0applying\0one\0or\nmore\0neural\0networks\0directly\0to\0portions\0of\0the\0input\0im\0\nage\0and\0arbitrating\0their\0results.\0Each\0network\0is\0trained\0to\noutput\0the\0presence\0or\0absence\0of\0a\0face.\0The\0algorithms\nand\0training\0methods\0are\0designed\0to\0be\0general,\0with\0little\ncustomization\0for\0faces.\nMany\0face\0detection\0researchers\0have\0used\0the\0idea\0that\nfacial\0images\0can\0be\0characterized\0directly\0in\0terms\0of\0pixel\nintensities.\0These\0images\0can\0be\0characterized\0by\0probabil\0\nistic\0models\0of\0the\0set\0of\0face\0images\0[4],\0[13],\0[15]\0or\0im\0\nplicitly\0by\0neural\0networks\0or\0other\0mechanisms\0[3],\0[12],\n[14],\0[19],\0[21],\0[23],\0[25],\0[26].\0The\0parameters\0for\0these\nmodels\0are\0adjusted\0either\0automatically\0from\0example\nimages\0(as\0in\0our\0work)\0or\0by\0hand.\0A\0few\0authors\0have\ntaken\0the\0approach\0of\0extracting\0features\0and\0applying\0ei\0\nther\0manually\0or\0automatically\0generated\0rules\0for\0evalu\0\nating\0these\0features\0[7],\0[11].\nTraining\0a\0neural\0network\0for\0the\0face\0detection\0task\0is\nchallenging\0because\0of\0the\0difficulty\0in\0characterizing\0proto\0\ntypical\0nonface\0images.\0Unlike\0face\0 recognition,\0in\0which\nthe\0classes\0to\0be\0discriminated\0are\0different\0faces,\0the\0two\nclasses\0to\0be\0discriminated\0in\0face\0 detection\0are\0images\0con\0\ntaining\0faces\0and\0images\0not\0containing\0faces.\0It\0is\0easy\0to\n1.\0An\0interactive\0demonstration\0of\0the\0system\0is\0available\0on\0the\0World\nWide\0Web\0at\0 http://www.cs.cmu.edu~har/faces.html ,\0which\0allows\0anyone\0to\nsubmit\0images\0for\0processing\0by\0the\0face\0detector,\0and\0to\0see\0the\0detection\nresults\0for\0pictures\0submitted\0by\0other\0people.\nget\0a\0representative\0sample\0of\0images\0which\0contain\0faces,\nbut\0much\0harder\0to\0get\0a\0representative\0sample\0of\0those\nwhich\0do\0not.\0We\0avoid\0the\0problem\0of\0using\0a\0huge\0training\nset\0for\0nonfaces\0by\0selectively\0adding\0images\0to\0the\0training\nset\0as\0training\0progresses\0[21].\0This\0bootstrap\0method\0re\0\nduces\0the\0size\0of\0the\0training\0set\0needed.\0The\0use\0of\0arbitra\0\ntion\0between\0multiple\0networks\0and\0heuristics\0to\0clean\0up\0the\nresults\0significantly\0improves\0the\0accuracy\0of\0the\0detector.\nDetailed\0descriptions\0of\0the\0example\0collection\0and\ntraining\0methods,\0network\0architecture,\0and\0arbitration\nmethods\0are\0given\0in\0Section\02.\0In\0Section\03,\0the\0perform\0\nance\0of\0the\0system\0is\0examined.\0We\0find\0that\0the\0system\0is\nable\0to\0detect\090.5\0percent\0of\0the\0faces\0over\0a\0test\0set\0of\0130\ncomplex\0images,\0with\0an\0acceptable\0number\0of\0false\0posi\0\ntives.\0Section\04\0briefly\0discusses\0some\0techniques\0that\0can\nbe\0used\0to\0make\0the\0system\0run\0faster,\0and\0Section\05\0com\0\npares\0this\0system\0with\0similar\0systems.\0Conclusions\0and\ndirections\0for\0future\0research\0are\0presented\0in\0Section\06.\n2DESCRIPTION\0OF\0THE\0 SYSTEM\nOur\0system\0operates\0in\0two\0stages:\0It\0first\0applies\0a\0set\0of\0neu\0\nral\0network\0based\0filters\0to\0an\0image\0and\0then\0uses\0an\0arbi\0\ntrator\0to\0combine\0the\0outputs.\0The\0filters\0examine\0each\0loca\0\ntion\0in\0the\0image\0at\0several\0scales,\0looking\0for\0locations\0that\nmight\0contain\0a\0face.\0The\0arbitrator\0then\0merges\0detections\nfrom\0individual\0filters\0and\0eliminates\0overlapping\0detections.\n2.1 Stage\0One:\0A\0Neural\0Network\0Based\0Filter\nThe\0first\0component\0of\0our\0system\0is\0a\0filter\0that\0receives\0as\ninput\0a\020\0 \020\0pixel\0region\0of\0the\0image\0and\0generates\0an\noutput\0ranging\0from\01\0to\0 -1,\0signifying\0the\0presence\0or\0ab\0\nsence\0of\0a\0face,\0respectively.\0To\0detect\0faces\0anywhere\0in\0the\ninput,\0the\0filter\0is\0applied\0at\0every\0location\0in\0the\0image.\0To\ndetect\0faces\0larger\0than\0the\0window\0size,\0the\0input\0image\0is\nrepeatedly\0reduced\0in\0size\0(by\0subsampling),\0and\0the\0filter\0is\napplied\0at\0each\0size.\0This\0filter\0must\0have\0some\0invariance\0to\nposition\0and\0scale.\0The\0amount\0of\0invariance\0de termines\0the\nnumber\0of\0scales\0and\0positions\0at\0which\0it\0must\0be\0applied.\nFor\0the\0work\0presented\0here,\0we\0apply\0the\0filter\0at\0every\n0162\08828/98/$10.00\0\01998\0IEEE\n\n H.A.\0Rowley\0and\0T.\0Kanade\0are\0with\0the\0Department\0of\0Computer\0Science,\nCarnegie\0Mellon\0University,\05000\0Forbes\0Avenue,\0Pittsburgh,\0PA\015213.\nE\0mail:\0{har,\0tk}@cs.cmu.edu.\n S.\0Baluja\0is\0with\0the\0Justsystem\0Pittsburgh\0Research\0Center,\04616\0Henry\nStreet,\0Pittsburgh,\0PA\015213\0and\0is\0also\0associated\0with\0the\0Department\0of\nComputer\0Science\0and\0the\0Robotics\0Institute\0at\0Carnegie\0Mellon\0Univer\0\nsity.\0E\0mail:\0baluja@jprc.com.\nManuscript\0received\06\0May\01996;\0revised\09\0Oct.\01997.\0Recommended\0for\0accep\0\ntance\0by\0R.W.\0Picard.\nFor\0information\0on\0obtaining\0reprints\0of\0this\0article,\0please\0send\0e\0mail\0to:\ntpami@computer.org,\0and\0reference\0IEEECS\0Log\0Number\0105873.\nI24 IEEE\0TRANSACTIONS\0ON\0PATTERN\0ANALYSIS\0AND\0MACHINE\0INTELLIGENCE,\0\0VOL.\0\020,\0\0NO.\0\01,\0\0JANUARY\0\01998\npixel\0position\0in\0the\0image\0and\0scale\0the\0image\0down\0by\0a\nfactor\0of\01.2\0for\0each\0step\0in\0the\0pyramid.\nThe\0filtering\0algorithm\0is\0shown\0in\0Fig.\01.\0First,\0a\0preproc\0\nessing\0step,\0adapted\0from\0[21],\0is\0applied\0to\0a\0window\0of\0the\nimage.\0The\0window\0is\0then\0passed\0through\0a\0neural\0network,\nwhich\0decides\0whether\0the\0window\0contains\0a\0face.\0The\0pre\0\nprocessing\0first\0attempts\0to\0equalize\0the\0intensity\0values\nacross\0the\0window.\0We\0fit\0a\0function\0which\0varies\0linearly\nacross\0the\0window\0to\0the\0intensity\0values\0in\0an\0oval\0region\ninside\0the\0window.\0Pixels\0outside\0the\0oval\0(shown\0in\0Fig.\02a)\nmay\0represent\0the\0background,\0so\0those\0intensity\0values\0are\nignored\0in\0computing\0the\0lighting\0variation\0across\0the\0face.\nThe\0linear\0function\0will\0approximate\0the\0overall\0brightness\0of\neach\0part\0of\0the\0window\0and\0can\0be\0subtracted\0from\0the\nwindow\0to\0compensate\0for\0a\0variety\0of\0lighting\0conditions.\nThen,\0histogram\0equalization\0is\0performed,\0which\0nonline\0\narly\0maps\0the\0intensity\0values\0to\0expand\0the\0range\0of\0intensi\0\nties\0in\0the\0window.\0The\0histogram\0is\0computed\0for\0pixels\ninside\0an\0oval\0region\0in\0the\0window.\0This\0compensates\0for\ndifferences\0in\0camera\0input\0gains,\0as\0well\0as\0improving\0con\0\ntrast\0in\0some\0cases.\0The\0preprocessing\0steps\0are\0shown\0in\0Fig.\02.\nThe\0preprocessed\0window\0is\0then\0passed\0through\0a\0neural\nnetwork.\0The\0network\0has\0retinal\0connections\0to\0its\0input\nlayer;\0the\0receptive\0fields\0of\0hidden\0units\0are\0shown\0in\0Fig.\01.\nThere\0are\0three\0types\0of\0hidden\0units:\0four\0which\0look\0at\n10\0 \010\0pixel\0subregions,\016\0which\0look\0at\05\0 \05\0pixel\0subre\0\ngions,\0and\0six\0which\0look\0at\0overlapping\020\0 \05\0pixel\0hori\0\nzontal\0stripes\0of\0pixels.\0Each\0of\0these\0types\0was\0chosen\0to\nallow\0the\0hidden\0units\0to\0detect\0local\0features\0that\0might\0be\nimportant\0for\0face\0detection.\0In\0particular,\0the\0horizontal\nstripes\0allow\0the\0hidden\0units\0to\0detect\0such\0features\0as\nmouths\0or\0pairs\0of\0eyes,\0while\0the\0hidden\0units\0with\0square\nreceptive\0fields\0might\0detect\0features\0such\0as\0individual\0eyes,\nthe\0nose,\0or\0corners\0of\0the\0mouth.\0Although\0the\0figure\0shows\na\0single\0hidden\0unit\0for\0each\0subregion\0of\0the\0input,\0these\nunits\0can\0be\0replicated.\0For\0the\0experiments\0which\0are\0de\0\nscribed\0later,\0we\0use\0networks\0with\0two\0and\0three\0sets\0of\nthese\0hidden\0units.\0Similar\0input\0connection\0patterns\0are\ncommonly\0used\0in\0speech\0and\0character\0recognition\0tasks\n[10],\0[24].\0The\0network\0has\0a\0single,\0real\0valued\0output,\nwhich\0indicates\0whether\0or\0not\0the\0window\0contains\0a\0face.\nExamples\0of\0output\0from\0a\0single\0network\0are\0shown\0in\nFig.\03.\0In\0the\0figure,\0each\0box\0represents\0the\0position\0and\nsize\0of\0a\0window\0to\0which\0the\0neural\0network\0gave\0a\0posi\0\ntive\0response.\0The\0network\0has\0some\0invariance\0to\0position\nFig.\01.\0The\0basic\0algorithm\0used\0for\0face\0detection.\n(a)\n\0 \0 \0 \0\n(b)\n\0 \0 \0 \0\n(c)\n\0 \0 \0 \0\n(d)\n\0 \0 \0 \0\n(e)\nFig.\02.\0The\0steps\0in\0preprocessing\0a\0window.\0First,\0a\0linear\0function\0is\0fit\nto\0the\0intensity\0values\0in\0the\0window,\0and\0then\0subtracted\0out,\0correcting\nfor\0some\0extreme\0lighting\0conditions.\0Then,\0histogram\0equalization\0is\napplied,\0to\0correct\0for\0different\0camera\0gains\0and\0to\0improve\0contrast.\0For\neach\0of\0these\0steps,\0the\0mapping\0is\0computed\0based\0on\0pixels\0inside\0the\noval\0mask,\0and\0then\0applied\0to\0the\0entire\0window.\0(a)\0Oval\0mask\0for\0ig\0\nnoring\0background\0pixels.\0(b)\0Original\0window.\0(c)\0Best\0fit\0linear\0function.\n(d)\0Lighting\0corrected\0window\0(linear\0function\0subtracted).\0(e)\0Histogram\nequalized\0window.ROWLEY\0ET\0AL.:\0NEURAL\0NETWORK\0BASED\0FACE\0DETECTION 25\nand\0scale,\0which\0results\0in\0multiple\0boxes\0around\0some\nfaces.\0Note\0also\0that\0there\0are\0some\0false\0detections;\0they\nwill\0be\0eliminated\0by\0methods\0presented\0in\0Section\02.2.\nTo\0train\0the\0neural\0network\0used\0in\0stage\0one\0to\0serve\0as\nan\0accurate\0filter,\0a\0large\0number\0of\0face\0and\0nonface\0im\0\nages\0are\0needed.\0Nearly\01,050\0face\0examples\0were\0gathered\nfrom\0face\0databases\0at\0CMU,\0Harvard,\n2\n\0and\0from\0the\0World\nWide\0Web.\0The\0images\0contained\0faces\0of\0various\0sizes,\norientations,\0positions,\0and\0intensities.\0The\0eyes,\0tip\0of\0nose,\nand\0corners\0and\0center\0of\0the\0mouth\0of\0ea'),
(333, 584, 'Dynamic source routing in ad hoc wireless networks', 'Dynamic Source Routing\nin Ad Hoc Wireless Networks\nDavid B. Johnson\nDavid A. Maltz\nComputer Science Department\nCarnegie Mellon University\n5000 Forbes Avenue\nPittsburgh, PA 15213-3891\ndbj@cs.cmu.edu\nAbstract\nAn ad hoc network is a collection of wireless mobile hosts forming a temporary network without the\naid of any established infrastructure or centralized administration. In such an environment, it may be\nnecessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination,\ndue to the limited range of each mobile hosts wireless transmissions. This paper presents a protocol\nfor routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing\nchanges when host movement is frequent, yet requires little or no overhead during periods in which\nhosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in\nan ad hoc network, the protocol performs well over a variety of environmental conditions such as host\ndensity and movement rates. For all but the highest rates of host movement simulated, the overhead of\nthe protocol is quite low, falling to just 1% of total data packets transmitted for moderate movement rates\nin a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the\noptimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01\nof optimal.\n1. Introduction\nMobile hosts and wireless networking hardware are becoming widely available, and extensive work has\nbeen done recently in integrating these elements into traditional networks such as the Internet. Oftentimes,\nhowever, mobile users will want to communicate in situations in which no xed wired infrastructure such\nas this is available, either because it may not be economically practical or physically possible to provide\nthe necessary infrastructure or because the expediency of the situation does not permit its installation. For\nexample, a class of students may need to interact during a lecture, friends or business associates may run into\neach other in an airport terminal and wish to share les, or a group of emergency rescue workers may need\nto be quickly deployed after an earthquake or ood. In such situations, a collection of mobile hosts with\nwireless network interfaces may form a temporary network without the aid of any established infrastructure\nor centralized administration. This type of wireless network is known as an ad hoc network.\nA version of this paper will appear as a chapter in the book Mobile Computing, edited by Tomasz Imielinski and Hank Korth,\nKluwer Academic Publishers, 1996.If only two hosts, located closely together, are involved in the ad hoc network, no real routing protocol\nor routing decisions are necessary. In many ad hoc networks, though, two hosts that want to communicate\nmay not be within wireless transmission range of each other, but could communicate if other hosts between\nthem also participating in the ad hoc network are willing to forward packets for them. For example, in\nthe network illustrated in Figure 1, mobile host C is not within the range of host As wireless transmitter\n(indicated by the circle around A) and host A is not within the range of host Cs wireless transmitter. If A and\nC wish to exchange packets, they may in this case enlist the services of host B to forward packets for them,\nsince B is within the overlap between As range and Cs range. Indeed, the routing problem in a real ad hoc\nnetwork may be more complicated than this example suggests, due to the inherent nonuniform propagation\ncharacteristics of wireless transmissions and due to the possibility that any or all of the hosts involved may\nmove at any time.\nRouting protocols in conventional wired networks generally use either distance vector or link state\nrouting algorithms, both of which require periodic routing advertisements to be broadcast by each router. In\ndistance vector routing [9, 17, 26, 27, 29], each router broadcasts to each of its neighbor routers its view of\nthe distance to all hosts, and each router computes the shortest path to each host based on the information\nadvertised by each of its neighbors. In link state routing [10, 16, 18], each router instead broadcasts to all\nother routers in the network its view of the status of each of its adjacent network links, and each router then\ncomputes the shortest distance to each host based on the complete picture of the network formed from the\nmost recent link information from all routers. In addition to its use in wired networks, the basic distance\nvector algorithm has also been adapted for routing in wireless ad hoc networks, essentially treating each\nmobile host as a router [11, 19, 25].\nThis paper describes the design and performance of a routing protocol for ad hoc networks that instead\nuses dynamic source routing of packets between hosts that want to communicate. Source routing is a routing\ntechnique in which the sender of a packet determines the complete sequence of nodes through which to\nforward the packet; the sender explicitly lists this route in the packets header, identifying each forwarding\nhop by the address of the next node to which to transmit the packet on its way to the destination host.\nSource routing has been used in a number of contexts for routing in wired networks, using either statically\ndened or dynamically constructed source routes [4, 5, 12, 20, 22, 28], and has been used with statically\ncongured routes in the Tucson Amateur Packet Radio (TAPR) work for routing in a wireless network [14].\nThe protocol presented here is explicitly designed for use in the wireless environment of an ad hoc network.\nThere are no periodic router advertisements in the protocol. Instead, when a host needs a route to another\nhost, it dynamically determines one based on cached information and on the results of a route discovery\nprotocol.\nWe believe our dynamic source routing protocol offers a number of potential advantages over conven-\ntional routing protocols such as distance vector in an ad hoc network. First, unlike conventional routing\nprotocols, our protocol uses no periodic routing advertisement messages, thereby reducing network band-\nwidth overhead, particularly during periods when little or no signicant host movement is taking place.\nBattery power is also conserved on the mobile hosts, both by not sending the advertisements and by not\nneeding to receive them (since a host could otherwise reduce its power usage by putting itself into sleep or\nstandby mode when not busy with other tasks). Distance vector and link state routing, on the other hand,\nmust continue to send advertisements even when nothing changes, so that other mobile hosts will continue\nto consider those routes or network links as valid. In addition, many of the links between routers seen\nby the routing algorithm may be redundant [11]. Wired networks are usually explicitly congured to have\nonly one (or a small number) of routers connecting any two networks, but there are no explicit links in an\nad hoc network, and all communication is by broadcast transmissions. The redundant paths in a wireless\nenvironment unnecessarily increase the size of routing updates that must be sent over the network, and\nincrease the CPU overhead required to process each update and to compute new routes.\n2A C B\nFigure 1 A simple ad hoc network of three wireless mobile hosts\nIn addition, conventional routing protocols based on link state or distance vector algorithms may compute\nsome routes that do not work. In a wireless environment, network transmission between two hosts does not\nnecessarily work equally well in both directions, due to differing propagation or interference patterns around\nthe two hosts [1, 15]. For example, with distance vector routing, even though a host may receive a routing\nadvertisement from another mobile host, packets it might then transmit back to that host for forwarding may\nnot be able to reach it. Our protocol does not require transmissions between hosts to work bidirectionally,\nalthough we do make use of it when afforded, for example, by MAC-level protocols such as MACA [13] or\nMACAW [2] that ensure it.\nFinally, conventional routing protocols are not designed for the type of dynamic topology changes that\nmay be present in ad hoc networks. In conventional networks, links between routers occasionally go down\nor come up, and sometimes the cost of a link may change due to congestion, but routers do not generally\nmove around dynamically. In an environment with mobile hosts as routers, though, convergence to new,\nstable routes after such dynamic changes in network topology may be slow, particularly with distance vector\nalgorithms [20]. Our dynamic source routing protocol is able to adapt quickly to changes such as host\nmovement, yet requires no routing protocol overhead during periods in which such changes do not occur.\nSection 2 of this paper details our assumptions about the network and the mobile hosts. The basic\noperation of our dynamic source routing protocol is described in Section 3, and optimizations to this basic\noperation are described in Section 4. In Section 5, we present a preliminary evaluation of the performance\nof our protocol, based on a packet-level simulation. In Section 6, we discuss related protocols for wireless\nnetwork routing and for source routing, and in Section 7, we present conclusions and future work.\n2. Assumptions\nWe assume that all hosts wishing to communicate with other hosts within the ad hoc network are willing to\nparticipate fully in the protocols of the network. In particular, each host participating in the network should\nalso be willing to forward packets for other hosts in the network.\nWe refer to the number of hops necessary for a packet to reach from any host located at one extreme\nedge of the network to another host located at the opposite extreme, as the diameter of the network. For\nexample, the diam'),
(334, 585, 'ANALYSIS OF WIRELESS SENSOR NETWORKS FOR HABITAT MONITORING', 'Chapter 18\r\nANALYSIS OF WIRELESS SENSOR\r\nNETWORKS FORHABITATMONITORING\r\nJoseph Polastre1, Robert Szewczyk1, Alan Mainwaring2,\r\nDavid Culler1,2 and John Anderson3\r\n1University of California at Berkeley\r\nComputer Science Department\r\nBerkeley, CA 94720\r\n{polastre,szewczyk,culler}@cs.berkeley.edu\r\n2Intel Research Lab, Berkeley\r\n2150 Shattuck Ave. Suite 1300\r\nBerkeley, CA 94704\r\n{amm,dculler}@intel-research.net\r\n3College of the Atlantic\r\n105 Eden Street\r\nBar Harbor, ME 04609\r\njga@ecology.coa.edu\r\nAbstract We provide an in-depth study of applying wireless sensor networks (WSNs) to\r\nreal-world habitat monitoring. A set of system design requirements were devel-\r\noped that cover the hardware design of the nodes, the sensor network software,\r\nprotective enclosures, and system architecture to meet the requirements of biol-\r\nogists. In the summer of 2002, 43 nodes were deployed on a small island off the\r\ncoast of Maine streaming useful live data onto the web. Although researchers\r\nanticipate some challenges arising in real-world deployments of WSNs, many\r\nproblems can only be discovered through experience. We present a set of ex-\r\nperiences from a four month long deployment on a remote island. We analyze\r\nthe environmental and node health data to evaluate system performance. The\r\nclose integration of WSNs with their environment provides environmental data\r\nat densities previously impossible. We show that the sensor data is also useful\r\nfor predicting system operation and network failures. Based on over one million\r\n2 Polastre et. al.\r\ndata readings, we analyze the node and network design and develop network\r\nreliability profiles and failure models.\r\nKeywords: Wireless Sensor Networks, Habitat Monitoring, Microclimate Monitoring, Net-\r\nwork Architecture, Long-Lived Systems\r\n18.1 Introduction\r\nThe emergence of wireless sensor networks has enabled new classes of ap-\r\nplications that benefit a large number of fields. Wireless sensor networks have\r\nbeen used for fine-grain distributed control [27], inventory and supply-chain\r\nmanagement [25], and environmental and habitat monitoring [22].\r\nHabitat and environmental monitoring represent a class of sensor network\r\napplications with enormous potential benefits for scientific communities. In-\r\nstrumenting the environment with numerous networked sensors can enable\r\nlong-term data collection at scales and resolutions that are difficult, if not\r\nimpossible, to obtain otherwise. A sensors intimate connection with its im-\r\nmediate physical environment allows each sensor to provide localized mea-\r\nsurements and detailed information that is hard to obtain through traditional\r\ninstrumentation. The integration of local processing and storage allows sensor\r\nnodes to perform complex filtering and triggering functions, as well as to apply\r\napplication-specific or sensor-specific aggregation, filtering, and compression\r\nalgorithms. The ability to communicate not only allows sensor data and con-\r\ntrol information to be communicated across the network of nodes, but nodes to\r\ncooperate in performing more complex tasks. Many sensor network services\r\nare useful for habitat monitoring: localization [4], tracking [7, 18, 20], data\r\naggregation [13, 19, 21], and, of course, energy-efficient multihop routing [9,\r\n17, 32]. Ultimately the data collected needs to be meaningful to disciplinary\r\nscientists, so sensor design [24] and in-the-field calibration systems are cru-\r\ncial [5, 31]. Since such applications need to run unattended, diagnostic and\r\nmonitoring tools are essential [33].\r\nIn order to deploy dense wireless sensor networks capable of recording, stor-\r\ning, and transmitting microhabitat data, a complete system composed of com-\r\nmunication protocols, sampling mechanisms, and power management must be\r\ndeveloped. We let the application drive the system design agenda. Taking this\r\napproach separates actual problems from potential ones, and relevant issues\r\nfrom irrelevant ones from a biological perspective. The application-driven con-\r\ntext helps to differentiate problems with simple, concrete solutions from open\r\nresearch areas.\r\nOur goal is to develop an effective sensor network architecture for the do-\r\nmain of monitoring applications, not just one particular instance. Collaboration\r\nwith scientists in other fields helps to define the broader application space, as\r\nAnalysis of Wireless Sensor Networks for Habitat Monitoring 3\r\nwell as specific application requirements, allows field testing of experimental\r\nsystems, and offers objective evaluations of sensor network technologies. The\r\nimpact of sensor networks for habitat and environmental monitoring will be\r\nmeasured by their ability to enable new applications and produce new, other-\r\nwise unattainable, results.\r\nFew studies have been performed using wireless sensor networks in long-\r\nterm field applications. During the summer of 2002, we deployed an outdoor\r\nhabitat monitoring application that ran unattended for four months. Outdoor\r\napplications present an additional set of challenges not seen in indoor experi-\r\nments. While we made many simplifying assumptions and engineered out the\r\nneed for many complex services, we were able to collect a large set of environ-\r\nmental and node diagnostic data. Even though the collected data was not high\r\nenough quality to make scientific conclusions, the fidelity of the sensor data\r\nyields important observations about sensor network behavior. The data anal-\r\nysis discussed in this paper yields many insights applicable to most wireless\r\nsensor deployments. We examine traditional quality of service metrics such as\r\npacket loss; however, the sensor data combined with network metrics provide\r\na deeper understanding of failure modes including those caused by the sensor\r\nnodes close integration with its environment. We anticipate that with system\r\nevolution comes higher fidelity sensor readings that will give researchers an\r\neven better better understanding of sensor network behavior.\r\nIn the following sections, we explain the need for wireless sensor networks\r\nfor habitat monitoring in Section 18.2. The network architecture for data flow\r\nin a habitat monitoring deployment is presented in Section 18.3. We describe\r\nthe WSN application in Section 18.4 and analyze the network behaviors de-\r\nduced from sensor data on a network and per-node level in Section 18.5. Sec-\r\ntion 18.6 contains related work and Section 18.7 concludes.\r\n18.2 Habitat Monitoring\r\nMany research groups have proposed using WSNs for habitat and micro-\r\nclimate monitoring. Although there are many interesting research problems\r\nin sensor networks, computer scientists must work closely with biologists to\r\ncreate a system that produces useful data while leveraging sensor network re-\r\nsearch for robustness and predictable operation. In this section, we examine the\r\nbiological need for sensor networks and the requirements that sensor networks\r\nmust meet to collect useful data for life scientists.\r\n18.2.1 The Case For Wireless Sensor Networks\r\nLife scientists are interested in attaining data about an environment with\r\nhigh fidelity. They typically use sensors on probes and instrument as much\r\nof the area of interest as possible; however, densely instrumenting any area\r\n4 Polastre et. al.\r\nis expensive and involves a maze of cables. Examples of areas life scientists\r\ncurrently monitor are redwood canopies in forests, vineyard microclimates,\r\nclimate and occupancy patterns of seabirds, and animal tracking. With these\r\napplications in mind, we examine the current modes of sensing and introduce\r\nwireless sensor networks as a new method for obtaining environmental and\r\nhabitat data at scales and resolutions that were previously impractical.\r\nTraditional data loggers for habitat monitoring are typically large in size\r\nand expensive. They require that intrusive probes be placed in the area of\r\ninterest and the corresponding recording and analysis equipment immediately\r\nadjacent. Life scientists typically use these data loggers since they are commer-\r\ncially available, supported, and provide a variety of sensors. Probes included\r\nwith data loggers may create a shadow effecta situation that occurs when an\r\norganism alters its behavioral patterns due to an interference in their space or\r\nlifestyle [23]. Instead, biologists argue for the miniaturization of devices that\r\nmay be deployed on the surface, in burrows, or in trees. Since interference is\r\nsuch a large concern, the sensors must be inconspicuous. They should not dis-\r\nrupt the natural processes or behaviors under study [6]. One such data logger\r\nis the Hobo Data Logger [24] from Onset Corporation. Due to size, price, and\r\norganism disturbance, using these systems for fine-grained habitat monitoring\r\nis inappropriate.\r\nOther habitat monitoring studies install one or a few sophisticated weather\r\nstations an insignificant distance (as much as tens of meters) from the area\r\nof interest. A major concern with this method is that biologists cannot gauge\r\nwhether the weather station actually monitors a different microclimate due to\r\nits distance from the organism under study [12]. Using these readings, bi-\r\nologists make generalizations through coarse measurements and sparsely de-\r\nployed weather stations. A revolution for biologists would be the ability to\r\nmonitor the environment on the scale of the organism, not on the scale of the\r\nbiologist [28].\r\nLife scientists are increasingly concerned about the potential impacts of di-\r\nrect human interaction in monitoring plants and animals in field studies. Dis-\r\nturbance effects are of particular concern in a small island situation where it\r\nmay be physically impossible for researchers to avoid impacting an entire pop-\r\nulation. Islands often serve as refugia for species that cannot adapt to the pres-\r\nence of terrestrial mammals. In Maine, biologists have shown that even a 15\r\nminute visit to a seabird colony can result in up to 20% mortality among eggs\r\nand'),
(335, 586, 'Conditional random fields: Probabilistic models for segmenting and labeling sequence data', 'Conditional Random Fields: Probabilistic Models\r\nfor Segmenting and Labeling Sequence Data\r\nJohn Lafferty LAFFERTY@CS.CMU.EDU\r\nAndrew McCallum MCCALLUM@WHIZBANG.COM\r\nFernando Pereira FPEREIRA@WHIZBANG.COM\r\nWhizBang! LabsResearch, 4616 Henry Street, Pittsburgh, PA 15213 USA\r\nSchool of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213 USA\r\nDepartment of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104 USA\r\nAbstract\r\nWe present conditional random fields , a frame-\r\nwork for building probabilistic models to seg-\r\nment and label sequence data. Conditional ran-\r\ndom fields offer several advantages over hid-\r\nden Markov models and stochastic grammars\r\nfor such tasks, including the ability to relax\r\nstrong independence assumptions made in those\r\nmodels. Conditional random fields also avoid\r\na fundamental limitation of maximum entropy\r\nMarkov models (MEMMs) and other discrimi-\r\nnative Markov models based on directed graph-\r\nical models, which can be biased towards states\r\nwith few successor states. We present iterative\r\nparameter estimation algorithms for conditional\r\nrandom fields and compare the performance of\r\nthe resulting models to HMMs and MEMMs on\r\nsynthetic and natural-language data.\r\n1. Introduction\r\nThe need to segment and label sequences arises in many\r\ndifferent problems in several scientific fields. Hidden\r\nMarkov models (HMMs) and stochastic grammars are well\r\nunderstood and widely used probabilistic models for such\r\nproblems. In computational biology, HMMs and stochas-\r\ntic grammars have been successfully used to align bio-\r\nlogical sequences, find sequences homologous to a known\r\nevolutionary family, and analyze RNA secondary structure\r\n(Durbin et al., 1998). In computational linguistics and\r\ncomputer science, HMMs and stochastic grammars have\r\nbeen applied to a wide variety of problems in text and\r\nspeech processing, including topic segmentation, part-of-\r\nspeech (POS) tagging, information extraction, and syntac-\r\ntic disambiguation (Manning & Schutze, 1999).\r\nHMMs and stochastic grammars are generative models, as-\r\nsigning a joint probability to paired observation and label\r\nsequences; the parameters are typically trained to maxi-\r\nmize the joint likelihood of training examples. To define\r\na joint probability over observation and label sequences,\r\na generative model needs to enumerate all possible ob-\r\nservation sequences, typically requiring a representation\r\nin which observations are task-appropriate atomic entities,\r\nsuch as words or nucleotides. In particular, it is not practi-\r\ncal to represent multiple interacting features or long-range\r\ndependencies of the observations, since the inference prob-\r\nlem for such models is intractable.\r\nThis difficulty is one of the main motivations for looking at\r\nconditional models as an alternative. A conditional model\r\nspecifies the probabilities of possible label sequences given\r\nan observation sequence. Therefore, it does not expend\r\nmodeling effort on the observations, which at test time\r\nare fixed anyway. Furthermore, the conditional probabil-\r\nity of the label sequence can depend on arbitrary, non-\r\nindependent features of the observation sequence without\r\nforcing the model to account for the distribution of those\r\ndependencies. The chosen features may represent attributes\r\nat different levels of granularity of the same observations\r\n(for example, words and characters in English text), or\r\naggregate properties of the observation sequence (for in-\r\nstance, text layout). The probability of a transition between\r\nlabels may depend not only on the current observation,\r\nbut also on past and future observations, if available. In\r\ncontrast, generative models must make very strict indepen-\r\ndence assumptions on the observations, for instance condi-\r\ntional independence given the labels, to achieve tractability.\r\nMaximum entropy Markov models (MEMMs) are condi-\r\ntional probabilistic sequence models that attain all of the\r\nabove advantages (McCallum et al., 2000). In MEMMs,\r\neach source state1 has a exponential model that takes the\r\nobservation features as input, and outputs a distribution\r\nover possible next states. These exponential models are\r\ntrained by an appropriate iterative scaling method in the\r\n1Output labels are associated with states; it is possible for sev-\r\neral states to have the same label, but for simplicity in the rest of\r\nthis paper we assume a one-to-one correspondence.\r\nmaximum entropy framework. Previously published exper-\r\nimental results show MEMMs increasing recall and dou-\r\nbling precision relative to HMMs in a FAQ segmentation\r\ntask.\r\nMEMMs and other non-generative finite-state models\r\nbased on next-state classifiers, such as discriminative\r\nMarkov models (Bottou, 1991), share a weakness we call\r\nhere the label bias problem: the transitions leaving a given\r\nstate compete only against each other, rather than against\r\nall other transitions in the model. In probabilistic terms,\r\ntransition scores are the conditional probabilities of pos-\r\nsible next states given the current state and the observa-\r\ntion sequence. This per-state normalization of transition\r\nscores implies a conservation of score mass (Bottou,\r\n1991) whereby all the mass that arrives at a state must be\r\ndistributed among the possible successor states. An obser-\r\nvation can affect which destination states get the mass, but\r\nnot how much total mass to pass on. This causes a bias to-\r\nward states with fewer outgoing transitions. In the extreme\r\ncase, a state with a single outgoing transition effectively\r\nignores the observation. In those cases, unlike in HMMs,\r\nViterbi decoding cannot downgrade a branch based on ob-\r\nservations after the branch point, and models with state-\r\ntransition structures that have sparsely connected chains of\r\nstates are not properly handled. The Markovian assump-\r\ntions in MEMMs and similar state-conditional models in-\r\nsulate decisions at one state from future decisions in a way\r\nthat does not match the actual dependencies between con-\r\nsecutive states.\r\nThis paper introduces conditional random fields (CRFs), a\r\nsequence modeling framework that has all the advantages\r\nof MEMMs but also solves the label bias problem in a\r\nprincipled way. The critical difference between CRFs and\r\nMEMMs is that a MEMM uses per-state exponential mod-\r\nels for the conditional probabilities of next states given the\r\ncurrent state, while a CRF has a single exponential model\r\nfor the joint probability of the entire sequence of labels\r\ngiven the observation sequence. Therefore, the weights of\r\ndifferent features at different states can be traded off against\r\neach other.\r\nWe can also think of a CRF as a finite state model with un-\r\nnormalized transition probabilities. However, unlike some\r\nother weighted finite-state approaches (LeCun et al., 1998),\r\nCRFs assign a well-defined probability distribution over\r\npossible labelings, trained by maximum likelihood or MAP\r\nestimation. Furthermore, the loss function is convex,2 guar-\r\nanteeing convergence to the global optimum. CRFs also\r\ngeneralize easily to analogues of stochastic context-free\r\ngrammars that would be useful in such problems as RNA\r\nsecondary structure prediction and natural language pro-\r\ncessing.\r\n2In the case of fully observable states, as we are discussing\r\nhere; if several states have the same label, the usual local maxima\r\nof Baum-Welch arise.\r\n0\r\n1r:_\r\n4\r\nr:_\r\n2i:_\r\n3\r\nb:rib\r\n5o:_\r\nb:rob\r\nFigure 1. Label bias example, after (Bottou, 1991). For concise-\r\nness, we place observation-label pairs o : l on transitions rather\r\nthan states; the symbol   represents the null output label.\r\nWe present the model, describe two training procedures and\r\nsketch a proof of convergence. We also give experimental\r\nresults on synthetic data showing that CRFs solve the clas-\r\nsical version of the label bias problem, and, more signifi-\r\ncantly, that CRFs perform better than HMMs and MEMMs\r\nwhen the true data distribution has higher-order dependen-\r\ncies than the model, as is often the case in practice. Finally,\r\nwe confirm these results as well as the claimed advantages\r\nof conditional models by evaluating HMMs, MEMMs and\r\nCRFs with identical state structure on a part-of-speech tag-\r\nging task.\r\n2. The Label Bias Problem\r\nClassical probabilistic automata (Paz, 1971), discrimina-\r\ntive Markov models (Bottou, 1991), maximum entropy\r\ntaggers (Ratnaparkhi, 1996), and MEMMs, as well as\r\nnon-probabilistic sequence tagging and segmentation mod-\r\nels with independently trained next-state classifiers (Pun-\r\nyakanok & Roth, 2001) are all potential victims of the label\r\nbias problem.\r\nFor example, Figure 1 represents a simple finite-state\r\nmodel designed to distinguish between the two words rib\r\nand rob. Suppose that the observation sequence is r i b.\r\nIn the first time step, r matches both transitions from the\r\nstart state, so the probability mass gets distributed roughly\r\nequally among those two transitions. Next we observe i.\r\nBoth states 1 and 4 have only one outgoing transition. State\r\n1 has seen this observation often in training, state 4 has al-\r\nmost never seen this observation; but like state 1, state 4\r\nhas no choice but to pass all its mass to its single outgoing\r\ntransition, since it is not generating the observation, only\r\nconditioning on it. Thus, states with a single outgoing tran-\r\nsition effectively ignore their observations. More generally,\r\nstates with low-entropy next state distributions will take lit-\r\ntle notice of observations. Returning to the example, the\r\ntop path and the bottom path will be about equally likely,\r\nindependently of the observation sequence. If one of the\r\ntwo words is slightly more common in the training set, the\r\ntransitions out of the start state will slightly prefer its cor-\r\nresponding transition, and that words state sequence will\r\nalways win. This behavior is demonstrated experimentally\r\nin Section 5.\r\nLeon Bottou (1991) discussed t');
INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(336, 587, 'The Lumigraph', 'The Lumigraph\r\nSteven J. Gortler Radek Grzeszczuk Richard Szeliski Michael F. Cohen\r\nMicrosoft Research\r\nAbstract\r\nThis paper discusses a new method for capturing the complete ap-\r\npearanceof both synthetic and real world objects and scenes, repres-\r\nenting this information, and then using this representation to render\r\nimages of the object from new camera positions. Unlike the shape\r\ncapture process traditionally used in computer vision and the render-\r\ning process traditionally used in computer graphics, our approach\r\ndoes not rely on geometric representations. Instead we sample and\r\nreconstruct a 4D function, which we call a Lumigraph. The Lu-\r\nmigraph is a subset of the complete plenoptic function that describes\r\nthe flow of light at all positions in all directions. With the Lu-\r\nmigraph, new images of the object can be generated very quickly, in-\r\ndependent of the geometric or illumination complexity of the scene\r\nor object. The paper discusses a complete working system includ-\r\ning the capture of samples, the construction of the Lumigraph, and\r\nthe subsequent rendering of images from this new representation.\r\n1 Introduction\r\nThe process of creating a virtual environment or object in computer\r\ngraphics begins with modeling the geometric and surface attributes\r\nof the objects in the environment along with any lights. An image\r\nof the environment is subsequently rendered from the vantage point\r\nof a virtual camera. Great effort has been expendedto develop com-\r\nputer aided design systems that allow the specification of complex\r\ngeometry and material attributes. Similarly, a great deal of work has\r\nbeen undertakento produce systemsthat simulate the propagation of\r\nlight through virtual environments to create realistic images.\r\nDespite these efforts, it has remained difficult or impossible to\r\nrecreate much of the complex geometry and subtle lighting effects\r\nfound in the real world. The modeling problem can potentially be\r\nbypassed by capturing the geometry and material properties of ob-\r\njects directly from the real world. This approach typically involves\r\nsome combination of cameras, structured light, range finders, and\r\nmechanical sensing devices such as 3D digitizers. When success-\r\nful, the results can be fed into a rendering program to create images\r\nof real objects and scenes. Unfortunately, these systems are still un-\r\nable to completely capture small details in geometry and material\r\nproperties. Existing rendering methods also continue to be limited\r\nin their capability to faithfully reproduce real world illumination,\r\neven if given accurate geometric models.\r\nQuicktime VR [6] was one of the first systems to suggest that the\r\ntraditional modeling/rendering process can be skipped. Instead, a\r\nseries of captured environment maps allow a user to look around a\r\nscene from fixed points in space. One can also flip through differ-\r\nent views of an object to create the illusion of a 3D model. Chen and\r\nWilliams [7] and Werner et al [30] have investigated smooth inter-\r\npolation between images by modeling the motion of pixels (i.e., the\r\noptical flow) as one moves from one camera position to another. In\r\nPlenoptic Modeling [19], McMillan and Bishop discuss finding the\r\ndisparity of each pixel in stereo pairs of cylindrical images. Given\r\nthe disparity (roughly equivalent to depth information), they can\r\nthen move pixels to create images from new vantage points. Similar\r\nwork using stereo pairs of planar images is discussed in [14].\r\nThis paper extends the work begun with Quicktime VR and Plen-\r\noptic Modeling by further developing the idea of capturing the com-\r\nplete flow of light in a region of the environment. Such a flow is de-\r\nscribed by a plenoptic function[1]. The plenoptic function is a five\r\ndimensional quantity describing the flow of light at every 3D spa-\r\ntial position \0x\0 y\0 z for every 2D direction \0\0 . In this paper,\r\nwe discuss computational methods for capturing and representing\r\na plenoptic function, and for using such a representation to render\r\nimages of the environment from any arbitrary viewpoint.\r\nUnlike Chen and Williams view interpolation [7] and McMil-\r\nlan and Bishops plenoptic modeling [19], our approach does not\r\nrely explicitly on any optical flow information. Such information\r\nis often difficult to obtain in practice, particularly in environments\r\nwith complex visibility relationships or specular surfaces. We do,\r\nhowever, use approximate geometric information to improve the\r\nquality of the reconstruction at lower sampling densities. Previous\r\nflow basedmethods implicitly rely on diffuse surface reflectance, al-\r\nlowing them to use a pixel from a single image to represent the ap-\r\npearanceof a single geometric location from a variety of viewpoints.\r\nIn contrast, our approach regularly samples the full plenoptic func-\r\ntion and thus makes no assumptions about reflectance properties.\r\nIf we consider only the subset of light leaving a bounded ob-\r\nject (or equivalently entering a bounded empty region of space),\r\nthe fact that radiance along any ray remains constant\0 allows us to\r\nreduce the domain of interest of the plenoptic function to four di-\r\nmensions. This paper first discusses the representation of this 4D\r\nfunction which we call a Lumigraph. We then discuss a system\r\nfor sampling the plenoptic function with an inexpensive hand-held\r\ncamera, and developing the captured light into a Lumigraph. Fi-\r\nnally this paper describes how to use texture mapping hardware to\r\nquickly reconstruct images from any viewpoint with a virtual cam-\r\nera model. The Lumigraph representation is applicable to synthetic\r\nobjects as well, allowing us to encode the complete appearance of\r\na complex model and to rerender the object at speeds independent\r\nof the model complexity. We provide results on synthetic and real\r\nsequences and discuss work that is currently underway to make the\r\nsystem more efficient.\r\n\0We are assuming the medium (i.e., the air) to be transparent.\r\n2 Representation\r\n2.1 From 5D to 4D\r\nThe plenoptic function is a function of 5 variables representing po-\r\nsition and direction  . If we assume the air to be transparent then\r\nthe radiance along a ray through empty space remains constant. If\r\nwe furthermore limit our interest to the light leaving the convex hull\r\nof a bounded object, then we only need to represent the value of the\r\nplenoptic function along some surface that surrounds the object. A\r\ncube was chosen for its computational simplicity (see Figure 1). At\r\nany point in space, one can determine the radiance along any ray in\r\nany direction, by tracing backwards along that ray through empty\r\nspace to the surface of the cube. Thus, the plenoptic function due to\r\nthe object can be reduced to 4 dimensions .\r\nThe idea of restricting the plenoptic function to some surround-\r\ning surface has been used before. In full-parallax holographic ste-\r\nreograms [3], the appearance of an object is captured by moving a\r\ncamera along some surface (usually a plane) capturing a 2D array of\r\nphotographs. This array is then transferred to a single holographic\r\nimage, which can display the appearanceof the 3D object. The work\r\nreported in this paper takes many of its concepts from holographic\r\nstereograms.\r\nGlobal illumination researchers have used the surface restric-\r\nted plenoptic function to efficiently simulate light-transfer between\r\nregions of an environment containing complicated geometric ob-\r\njects. The plenoptic function is represented on the surface of a cube\r\nsurrounding some region; that information is all that is needed to\r\nsimulate the light transfer from that region of space to all other re-\r\ngions [17]. In the context of illumination engineering, this idea has\r\nbeen used to model and represent the illumination due to physical\r\nluminaires. Ashdown [2] describes a gantry for moving a camera\r\nalong a sphere surrounding a luminaire of interest. The captured in-\r\nformation can then be used to represent the light source in global\r\nillumination simulations. Ashdown traces this idea of the surface-\r\nrestricted plenoptic function back to Levin [15].\r\nA limited version of the work reported here has been described\r\nby Katayama et al. [11]. In their system, a camera is moved along a\r\ntrack, capturing a 1D array of images of some object. This inform-\r\nation is then used to generate new images of the object from other\r\npoints in space. Because they only capture the plenoptic function\r\nalong a line, they only obtain horizontal parallax, and distortion is\r\nintroduced as soon as the new virtual camera leaves the line. Finally,\r\nin work concurrent to our own, Levoy and Hanrahan [16] represent\r\na 4D function that allows for undistorted, full parallax views of the\r\nobject from anywhere in space.\r\n2.2 Parameterization of the 4D Lumigraph\r\nThere are many potential ways to parameterize the four dimensions\r\nof the Lumigraph. We adopt a parameterization similar to that used\r\nin digital holographic stereograms [9] and also used by Levoy and\r\nHanrahan [16]. We begin with a cube to organize a Lumigraph\r\nand, without loss of generality, only consider for discussion a single\r\nsquare face of the cube (the full Lumigraph is constructed from six\r\nsuch faces).\r\nWe only consider a snapshot of the function, thus time is eliminated.\r\nWithout loss of generality, we also consider only a monochromatic func-\r\ntion (in practice 3 discrete color channels), eliminating the need to consider\r\nwavelength. We furthermore ignore issues of dynamic range and thus limit\r\nourselves to scalar values lying in some finite range.\r\nIn an analogous fashion one can reconstruct the complete plenoptic\r\nfunction inside an empty convex region by representing it only on the sur-\r\nface bounding the empty region. At any point inside the region, one can find\r\nthe light entering from any direction by finding that directions intersection\r\nwith the region boundary.\r\ns\r\nt\r\nFigure 1: The surface of a cube holds all the radiance informati'),
(337, 588, 'Recovering High Dynamic Range Radiance Maps from Photographs', 'Recovering High Dynamic Range Radiance Maps from Photographs\r\nPaul E. Debevec Jitendra Malik\r\nUniversity of California at Berkeley1\r\nABSTRACT\r\nWe present a method of recovering high dynamic range radiance\r\nmaps from photographs taken with conventional imaging equip-\r\nment. In our method, multiple photographs of the scene are taken\r\nwith different amounts of exposure. Our algorithm uses these dif-\r\nferently exposed photographs to recover the response function of the\r\nimaging process, up to factor of scale, using the assumption of reci-\r\nprocity. With the known response function, the algorithm can fuse\r\nthe multiple photographs into a single, high dynamic range radiance\r\nmap whose pixel values are proportional to the true radiance values\r\nin the scene. We demonstrate our method on images acquired with\r\nboth photochemical and digital imaging processes. We discuss how\r\nthis work is applicable in many areas of computer graphics involv-\r\ning digitized photographs, including image-based modeling, image\r\ncompositing, and image processing. Lastly, we demonstrate a few\r\napplications of having high dynamic range radiance maps, such as\r\nsynthesizing realistic motion blur and simulating the response of the\r\nhuman visual system.\r\nCR Descriptors: I.2.10 [Artificial Intelligence]: Vision and\r\nScene Understanding - Intensity, color, photometry and threshold-\r\ning; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and\r\nRealism - Color, shading, shadowing, and texture; I.4.1 [Image\r\nProcessing]: Digitization - Scanning; I.4.8 [Image Processing]:\r\nScene Analysis - Photometry, Sensor Fusion.\r\n1 Introduction\r\nDigitized photographs are becoming increasingly important in com-\r\nputer graphics. More than ever, scanned images are used as texture\r\nmaps for geometric models, and recent work in image-based mod-\r\neling and rendering uses images as the fundamental modeling prim-\r\nitive. Furthermore, many of todays graphics applications require\r\ncomputer-generated images to mesh seamlessly with real photo-\r\ngraphic imagery. Properly using photographically acquired imagery\r\nin these applications can greatly benefit from an accurate model of\r\nthe photographic process.\r\nWhen we photograph a scene, either with film or an elec-\r\ntronic imaging array, and digitize the photograph to obtain a two-\r\ndimensional array of brightness values, these values are rarely\r\n1Computer Science Division, University of California at Berkeley,\r\nBerkeley, CA 94720-1776. Email: debevec@cs.berkeley.edu, ma-\r\nlik@cs.berkeley.edu. More information and additional results may be found\r\nat: http://www.cs.berkeley.edu/debevec/Research\r\ntrue measurements of relative radiance in the scene. For example, if\r\none pixel has twice the value of another, it is unlikely that it observed\r\ntwice the radiance. Instead, there is usually an unknown, nonlinear\r\nmapping that determines how radiance in the scene becomes pixel\r\nvalues in the image.\r\nThis nonlinear mapping is hard to know beforehand because it is\r\nactually the composition of several nonlinear mappings that occur\r\nin the photographic process. In a conventional camera (see Fig. 1),\r\nthe film is first exposed to light to form a latent image. The film is\r\nthen developed to change this latent image into variations in trans-\r\nparency, or density, on the film. The film can then be digitized using\r\na film scanner, which projects light through the film onto an elec-\r\ntronic light-sensitive array, converting the image to electrical volt-\r\nages. These voltages are digitized, and then manipulated before fi-\r\nnally being written to the storage medium. If prints of the film are\r\nscanned rather than the film itself, then the printing process can also\r\nintroduce nonlinear mappings.\r\nIn the first stage of the process, the film response to variations\r\nin exposure X (which is Et, the product of the irradiance E the\r\nfilm receives and the exposure time t) is a non-linear function,\r\ncalled the characteristic curve of the film. Noteworthy in the typ-\r\nical characteristic curve is the presence of a small response with no\r\nexposure and saturation at high exposures. The development, scan-\r\nning and digitization processes usually introduce their own nonlin-\r\nearities which compose to give the aggregate nonlinear relationship\r\nbetween the image pixel exposures X and their values Z.\r\nDigital cameras, which use charge coupled device (CCD) arrays\r\nto image the scene, are prone to the same difficulties. Although the\r\ncharge collected by a CCD element is proportional to its irradiance,\r\nmost digital cameras apply a nonlinear mapping to the CCD outputs\r\nbefore they are written to the storage medium. This nonlinear map-\r\nping is used in various ways to mimic the response characteristics of\r\nfilm, anticipate nonlinear responses in the display device, and often\r\nto convert 12-bit output from the CCDs analog-to-digital convert-\r\ners to 8-bit values commonly used to store images. As with film,\r\nthe most significant nonlinearity in the response curve is at its sat-\r\nuration point, where any pixel with a radiance above a certain level\r\nis mapped to the same maximum image value.\r\nWhy is this any problem at all? The most obvious difficulty,\r\nas any amateur or professional photographer knows, is that of lim-\r\nited dynamic rangeone has to choose the range of radiance values\r\nthat are of interest and determine the exposure time suitably. Sunlit\r\nscenes, and scenes with shiny materials and artificial light sources,\r\noften have extreme differences in radiance values that are impossi-\r\nble to capture without either under-exposing or saturating the film.\r\nTo cover the full dynamic range in such a scene, one can take a series\r\nof photographs with different exposures. This then poses a prob-\r\nlem: how can we combine these separate images into a composite\r\nradiance map? Here the fact that the mapping from scene radiance\r\nto pixel values is unknown and nonlinear begins to haunt us. The\r\npurpose of this paper is to present a simple technique for recover-\r\ning this response function, up to a scale factor, using nothing more\r\nthan a set of photographs taken with varying, known exposure du-\r\nrations. With this mapping, we then use the pixel values from all\r\navailable photographs to construct an accurate map of the radiance\r\nin the scene, up to a factor of scale. This radiance map will cover\r\nsensor\r\nirradiance\r\n(E)\r\nscene\r\nradiance\r\n(L)\r\nlatent\r\nimage\r\nfilm\r\ndensity\r\nanalog\r\nvoltages\r\ndigital\r\nvalues\r\nfinal\r\ndigital\r\nvalues\r\n(Z)\r\nDigital Camera\r\nFilm Camera\r\nsensor\r\nexposure\r\n(X)\r\nLens Film Development CCD ADC RemappingShutter\r\nFigure 1: Image Acquisition Pipeline shows how scene radiance becomes pixel values for both film and digital cameras. Unknown nonlin-\r\near mappings can occur during exposure, development, scanning, digitization, and remapping. The algorithm in this paper determines the\r\naggregate mapping from scene radiance L to pixel values Z from a set of differently exposed images.\r\nthe entire dynamic range captured by the original photographs.\r\n1.1 Applications\r\nOur technique of deriving imaging response functions and recover-\r\ning high dynamic range radiance maps has many possible applica-\r\ntions in computer graphics:\r\nImage-based modeling and rendering\r\nImage-based modeling and rendering systems to date (e.g. [11, 15,\r\n2, 3, 12, 6, 17]) make the assumption that all the images are taken\r\nwith the same exposure settings and film response functions. How-\r\never, almost any large-scale environment will have some areas that\r\nare much brighter than others, making it impossible to adequately\r\nphotograph the scene using a single exposure setting. In indoor\r\nscenes with windows, this situation often arises within the field of\r\nview of a single photograph, since the areas visible through the win-\r\ndows can be far brighter than the areas inside the building.\r\nBy determining the response functions of the imaging device, the\r\nmethod presented here allows one to correctly fuse pixel data from\r\nphotographs taken at different exposure settings. As a result, one\r\ncan properly photograph outdoor areas with short exposures, and in-\r\ndoor areas with longer exposures, without creating inconsistencies\r\nin the data set. Furthermore, knowing the response functions can\r\nbe helpful in merging photographs taken with different imaging sys-\r\ntems, such as video cameras, digital cameras, and film cameras with\r\nvarious film stocks and digitization processes.\r\nThe area of image-based modeling and rendering is working to-\r\nward recovering more advanced reflection models (up to complete\r\nBRDFs) of the surfaces in the scene (e.g. [21]). These meth-\r\nods, which involve observing surface radiance in various directions\r\nunder various lighting conditions, require absolute radiance values\r\nrather than the nonlinearly mapped pixel values found in conven-\r\ntional images. Just as important, the recovery of high dynamic range\r\nimages will allow these methods to obtain accurate radiance val-\r\nues from surface specularities and from incident light sources. Such\r\nhigher radiance values usually become clamped in conventional im-\r\nages.\r\nImage processing\r\nMost image processing operations, such as blurring, edge detection,\r\ncolor correction, and image correspondence, expect pixel values to\r\nbe proportional to the scene radiance. Because of nonlinear image\r\nresponse, especially at the point of saturation, these operations can\r\nproduce incorrect results for conventional images.\r\nIn computer graphics, one common image processing operation\r\nis the application of synthetic motion blur to images. In our re-\r\nsults (Section 3), we will show that using true radiance maps pro-\r\nduces significantly more realistic motion blur effects for high dy-\r\nnamic range scenes.\r\nImage compositing\r\nMany applications in computer graphics involve compositing im-\r\nage data from images obtained by different processes. For exam-\r\nple, a background matte might be shot with a still camera, live\r\naction might be shot with a different fi'),
(338, 589, 'Statecharts: A Visual Formalism For Complex Systems', '\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'),
(339, 590, 'Closed-Form Solution of Absolute Orientation using Orthonormal Matrices', 'Reprinted from Journal of the Optical Society of America A, Vol, 4, page 629, April 1987\r\nCopyright  1987 by the Optical Society of America and reprinted by permission of the copyright owner .\r\nClosed-form solution of absolute orientation using uni t\r\nquaternions\r\nBerthold K. P. Horn\r\nDepartment of Electrical Engineering. University of Hawaii at Manisa, Honolulu\r\n. Hawaii 9672 0\r\nReceived August 6, 1986 : accepted November 25, 1986\r\nFinding the relationship between two coordinate systems using pairs of measurements of the coordinates of a\r\nnumber of points in both systems is a classic photogrammetric task . It finds applications i nstereopho ogrammetry\r\nand in robotics . I present here a closed-form solution to the least-squares problem for three or more paints\r\n.\r\nCurrently various empirical, graphical, and numerical iterative methods are in use. Derivation of the solution is\r\nsimplified by use of unit quaternions to represent rotation . I emphasize a symmetry property that a solution to thi s\r\nproblem ought to possess. The best translational offset is the difference between the centroid of the coordinates i n\r\none system and the rotated and scaled centroid of the coordinates in the other system\r\n. The best scale is equal to th e\r\nratio of the root-mean-square deviations of the coordinates in the two systems from their respective centroids\r\n.\r\nThese exact results are to be preferred to approximate methods based on measurements of a few selected points\r\n.\r\nThe unit quaternion representing the best rotation is the eigenvector associated with the most positive eigenvalue o f\r\na symmetric 4 X 4 matrix . The elements of this matrix are combinations of sums of products of corresponding\r\ncoordinates of the points .\r\n1 . INTRODUCTION\r\nSuppose that we are given the coordinates of a number o f\r\npoints as measured in two different Cartesian coordinate\r\nsystems (Fig . 1.) . The photogrammetric problem of recover-\r\ning the transformation between the two systems from thes e\r\nmeasurements is referred to as that of absolut e orientation.1\r\nIt occurs in several contexts, foremost in relating a stereo\r\nmodel developed from pairs of aerial photographs to a geo-\r\ndetic coordinate system . It also is of importance in robotics ,\r\nin which measurements in a camera coordinate system mus t\r\nbe related to coordinates in a system attached to a mechani -\r\ncal manipulator. Here one speaks of the determination o f\r\nthe hand-eye transform . 2\r\nA. Previous Wor k\r\nThe problem of absolute orientation is usually treated in a n\r\nempirical, graphical, or numerical iterativ e fashion.1,3,4\r\nThompson gives a solution to this problem when thre e\r\npoints are measured . His method, as well as the simpler one\r\nof Schut,6 depends on selective neglect of the extra con-\r\nstraints available when all coordinates of three points ar e\r\nknown. Schut uses unit quaternions and arrives at a set o f\r\nlinear equations . I present a simpler solution to this specia l\r\ncase in Subsection 2 .A that does not require solution of a\r\nsystem of linear equations. These methods all suffer fro m\r\nthe defect that they cannot handle more than three points .\r\nPerhaps more importantly, they do not even use all the\r\ninformation available from the three points .\r\nOswal and Balasubramanian7 developed a least-squares\r\nmethod that can handle more than three points, but thei r\r\nmethod does not enforce the orthonormality of the rotatio n\r\nmatrix . An iterative method is then used to square up the\r\nresult--bringing it closer to being orthonormal . The meth-\r\nod for doing this is iterative, and the result is not the solutio n\r\nof the original least-squares problem .\r\n0740-3232/87/040629-14$02 .00\r\nI present a closed-form solution to the least-squares prob -\r\nlem in Sections 2 and 4 and show in Section 5 that it simpli -\r\nfies greatly when only three points are used . This is impor -\r\ntant, since at times only three points may be available . The\r\nsolution is different from those described at the beginning o f\r\nthis section because it does not selectively neglect informa -\r\ntion provided by the measurements-it uses all of it .\r\nThe groundwork for the application of quaternions i n\r\nphotogrammetry was laid by Schut8 and Thompson . 9 In\r\nrobotics, Salamin10 and Taylor have been the main propa-\r\ngandists . The use of unit quaternions to represent rotatio n\r\nis reviewed in Section 3 and the appendixes (see also Ref. 2) .\r\nB. Minimum Number of Points\r\nThe transformation between two Cartesian coordinate sys-\r\ntems can be thought of as the result of a rigid-body motio n\r\nand can thus be decomposed into a rotation and a transla -\r\ntion. In stereophotogrammetry, in addition, the scale ma y\r\nnot be known . There are obviously three degrees of freedo m\r\nto translation . Rotation has another three (direction of the\r\naxis about which the rotation takes place plus the angle o f\r\nrotation about this axis) . Scaling adds one more degree o f\r\nfreedom. Three points known in both coordinate system s\r\nprovide nine constraints (three coordinates each), more tha n\r\nenough to permit determination of the seven unknowns .\r\nBy discarding two of the constraints, seven equations i n\r\nseven unknowns can be developed that allow one to recove r\r\nthe parameters . I show in Subsection 2 .A how to find the\r\nrotation in a similar fashion, provided that the three point s\r\nare not collinear . Two points clearly do not provide enoug h\r\nconstraint.\r\nC. Least Sum of Squares of Errors\r\nIn practice, measurements are not exact, and so greate r\r\naccuracy in determining the transformation parameters wil l\r\nbe sought for by using more than three points. We no longe r\r\n 1987 Optical Society of America\r\n630\r\n\r\nJ . Opt . Soc . Am . ANol . 4, No. 4/April 1987\r\n\r\nBerthold K . P . Horn\r\nC\r\n\r\nit\r\nFig . 1 . The coordinates of a number of points is measured in tw o\r\ndifferent coordinate systems . The transformation between the two\r\nsystems is to be found .\r\nexpect to be able to find a transformation that maps th e\r\nmeasured coordinates of points in one system exactly into\r\nthe measured coordinates of these points in the other. In-\r\nstead, we minimize the sum of squares of residual errors .\r\nFinding the best set of transformation parameters is not\r\neasy . In practice, various empirical, graphical, and numeri-\r\ncal procedures are in use . These are iterative in nature .\r\nThat is, given an approximate solution, such a method leads\r\nto a better, but still imperfect, answer . The iterative meth-\r\nod is applied repeatedly until the remaining error is negligi-\r\nble .\r\nAt times, information is available that permits one t o\r\nobtain so good an initial guess of the transformation parame -\r\nters that a single step of the iteration brings one close enoug h\r\nto the true solution of the least-squares problem to eliminat e\r\nthe need for further iteration in a practical situation .\r\nD. Closed-Form Solution\r\nIn this paper I present a closed-form solution to the least -\r\nsquares problem of absolute orientation, one that does no t\r\nrequire iteration. One advantage of a closed-form solution\r\nis that it provides one in a single step with the best possibl e\r\ntransformation, given the measurements of the points in th e\r\ntwo coordinate systems . Another advantage is that one\r\nneed not find a good initial guess, as one does when a n\r\niterative method is used .\r\nI give the solution in a form in which unit quaternions ar e\r\nused to represent rotations . The solution for the desired\r\nquaternion is shown to be the eigenvector of a symmetric 4 X\r\n4 matrix associated with the most positive eigenvalue. The\r\nelements of this matrix are simple combinations of sums o f\r\nproducts of corresponding coordinates of the points . To\r\nfind the eigenvalues, a quartic equation has to be solve d\r\nwhose coefficients are sums of products of elements of th e\r\nmatrix . It is shown that this quartic is particularly simple ,\r\nsince one ofits coefficients is zero . It simplifies even more\r\nwhen one or the other of the sets of measurements is copla-\r\nnar .\r\nE. Orthonormal Matrice s\r\nWhile unit quaternions constitute an elegant representatio n\r\nfor rotation, most of us are more familiar with orthonormal\r\nmatrices with positive determinant . Fortunately, the ap-\r\npropriate 3 X 3 rotation matrix can be easily constructe d\r\nfrom the four components of the unit quaternions as is show n\r\nin Subsection 3 .E. Working directly with matrices is diffi-\r\ncult because of the need to deal with six nonlinear con-\r\nst raints that ensure that the matrix is orthonormal . A\r\nclosed-form solution for the rotation matrix using manipula-\r\ntions of matrices will be presented in a subsequent paper .\r\nThis closed-form solution requires the computation of the\r\nsquare root of a symmetric 3 X 3 matrix .\r\nF. Symmetry of the Solution\r\nLet us call the two coordinate systems  left  and right . A\r\ndesirable property of a solution method is that, when applied\r\nto the problem of finding the best transformation from th e\r\nright to the left system, it gives the exact inverse of the bes t\r\ntransformation from the left to the right system . I show i n\r\nSubsection 2 .D that the scale factor has to be treated in a\r\nparticular way to guarantee that this will happen . Symme-\r\ntry is guaranteed when one uses unit quaternions to repre-\r\nsent rotation .\r\n2. SOLUTION METHOD S\r\nAs we shall see, the translation and the scale factor are eas y\r\nto determine once the rotation is known . The difficult par t\r\nof the problem is finding the rotation . Given three noncol-\r\nlinear points, we can easily construct a useful triad in each o f\r\nthe left and the right coordinate systems (Fig. 2) . Let the\r\norigin be at the first point . Take the line from the first t o\r\nthe second point to be the direction of the new x axis . Place\r\nthe new y axis at right angles to the new x axis in the plan e\r\nformed by the three points . The new z axis is then made t o\r\nbe orthogonal to the x and y axes, with orientation chosen to\r\nsatisfy the right-hand rule. '),
(346, 603, 'Term Paper', 'Term  Paper in CRYPTOLOGY on Attacks on RSA\n\nSubmitted to - Prof.R.K.Sharma\n\nSubmitted byGaurav Agarwal 2009EE10390\n\nAttacks On RSA\n\nAbstract\n\nRSA- Rivest, Shamir, Adleman It is an algorithm for public key cryptography. The security of RSA is based upon difficulty in factorization of a number and the RSA problem. There are number of attacks such as side channel attack, adaptive chosen cipher text attack and timing attack that have previously been attempted on the RSA cryptosystem. Here, I will discuss various fault based attacks on RSA.\nI will also try to describe the latest fault based attack attempted on 1024 bit RSA authentication. Hardware fault attack on RSA can be used on RSA modules for which CRT is used to speedup the process. A set of incorrect responses or faults are sent into the device created either naturally or purposefully. Based on the incorrect responses from the device due to the presence of faults, secrets of the device can be extracted. The recent fault based attack on RSA talks about introducing faults in the hardware and generating the private key, if the hardware is compromised. Hardware designs are mostly responsible to execute complex cryptographic algorithms in a reasonable time. The occurrence of transient hardware failures is a common phenomenon and errors can be introduced to the system by forcing the operative conditions. Fixed window exponentiation is an algorithm that computes (m^d) mod n. The fault based attack which is implemented recently exploits hardware faults injected at the server side of public key authentication. The fault based attack on FEW is implemented to recover the private key using extraction algorithm discussed in the paper\n\nFault Attacks: The papers works on the security issues of cryptosystems implemented on the tamper-proof devices like smart IC Cards from the viewpoint of presence of hardware faults. This type of cryptanalysis is known as fault-based cryptanalysis. There are mainly two categories of fault attacks. They are: (1) Transient fault attack and (2) Permanent fault attack. The first kind of attack is to induce a temporary fault which can be either a computational fault or a fault when accessing the data. The second kind of attack is a permanent fault on some of the stored parameters of the RSA-CRT. Assumptions: Eve has physical access to the tamper-proof device and Eve can induce faults into the device. Eve knows the message m to sign or the correct signature S on message m. The device can output a faulty signature S on message m.\n\nCRT Based Cryptanalysis Chinese Remainder Theorem (CRT)\nChinese Remainder Theorem tells that given a set of integers n1,n2,n3...., nk that are pair wise relatively prime then the congruences s = s1 (mod n1) s = s2 (mod n2) : : s = sk (mod nk) has a unique solution modulo n = n1 * n2 * .. *nk. Let p and q be the two moduli. dp = d mod (p-1) dq = d mod(q-1) Sp = (m^dp) mod p Sq = (m^dq) mod q The Signature S can be computed using any of the below two CRT recombination techniques.  Gauss Algorithm: S = (Sp * q * (q-1 mod p) + Sq * p * (p-1 mod q)) mod n  Garner Algorithm: S = Sq + ((Sp  Sq) * (q-1 mod p) mod p) * q\nBellcore Attack - Cryptanalysis Suppose a random error occurs during the computation of Sp, (Let Sp denote the error result) but the computation of Sq is error free. Applying CRT on both Sp and Sq will yield a faulty signature S. This CRT based hardware fault attack enables the factorization of n by computing q = gcd ( S  S mod n, n) or q = gcd (((S)^e  m) mod n, n). Similarly, p can be derived from a faulty signature S computed from applying CRT on a faulty S q and a correct Sp. For applying the CRT recombination, either Gauss or Garners Algorithm can be used\n\n.\n\nSimple Software Countermeasures:  One of the simple approach is to perform the calculation twice. Limitation: This approach is very time-consuming and also it cannot provide a satisfactory solution since a permanent error may be undetectable.  The second approach is to verify the correctness by comparing the inverse result with the input m. A RSA signature S computed from m can be verified by raising S to the eth power and compare with m. Limitation: This approach is not a satisfactory solution since the parameter e could be large and also this procedure could be time-consuming. Shamirs Countermeasure: Shamirs basic idea is to select a random integer r and do the following computations:\n\nPermanent Fault Attack on RSA-CRT: Consider that a permanent fault was induced on the storage of p and the value becomes p #. From this an erroneous value of Sp# will be obtained. Without proper countermeasures the faulty signature S# can be used to factorize n. If permanent fault occurs on dp or dp then both Sp and Sq will be faulty. Since both Sp and Sq are faulty, this does not help in factorization of n and hence CRT cannot be attacked. Feasibility of fault attacks: Spike attacks are used to precisely trigger a fault at certain computation of RSA-CRT. Spikes are\n\nnothing but voltage fluctuations, which generate a fault during a computation. Typically a smart card must be able to tolerate on the contact Vcc a supply voltage between 4,5V and 5,5V, where the standard voltage is specified at 5V. Within this range, the smart card will work properly. However, a deviation of the external power supply of much more than the specified 10% tolerance could cause problems for the functionality of smart card IC. Indeed, it could lead to a wrong computation result provided the chip is able to finish its computation completely. A specific type of power spike is determined by nine different parameters. These nine parameters are determined by a combination of time and voltage values as well as by the shape of the transition as shown in the figure.\n\nThe fault based attack on RSA authentication The new fault based attack on RSA authentication takes into consideration, flaws in the fixed window exponentiation module. It injects the faults in the hardware for signature generation and determines the servers private key by collecting and analyzing the signatures with a single bit fault. In the public key authentication, RSA is the most commonly adopted system. The client sends a unique message to the server who signs it using its private key and sends back the signature to the client for authentication. It requires generation of a suitable pair of public key (n, e) and private key (n, d) so that m = ((m^d)^ e mod n) holds true. The signature is generated by the server as (s = m^d modn), where (n, d) is the private key for the server. The client authenticates the server using (s e mod n). The fault based attack requires limited knowledge of the hardware and proximity to inject the faults in the system. If sufficiently large amount of corrupted signatures are collected, it is possible to determine the private key of the server. The paper is based on the theorem which states that, given a public key authentication system, < n, d, e > where n and e are known and d is not known, and for which the signature with the private key d of length N is computed using the fixed-window exponentiation (FWE) algorithm with a window size w, we call k the number of windows in the private key d, that is, k = N/w. Let us call ^s a corrupted signature of the message m computed with the private key d. Assume that a single-bit binary value change has occurred at the output of any of the squaring operations in FWE during the computation of ^s. An attacker that can collect at least S = k  ln(2k) different pairs < m, ^s > has a probability pr = 1/2 to recover the private key d of N bits in polynomial time.\n\nFixed window exponentiation algorithm (FWE) The fixed window exponentiation algorithm is similar to square and multiply, but instead of working on a single bit in the private key d of length N, FWE defines a window of width w and partitions the private key d into N/w parts. If the window size is changed to 1, it turns out to be the simple square and multiply algorithm. FWE uses the fixed size window to avoid timing based attack on the system. The modular exponentiation for window of size w is a small number ranging from (2w - 1). It gathers the result of computation for each window in the accumulator and performs the multiplication to m^d[win_size] to compute (s = md mod n) The m^d[win_size] values ranging from 0 to 2win_size  1 are generally precomputed and act as a lookup table to reduce the computation. Algorithm for the Fixed window exponentiation  1 FWE(m, d, n, win size) 2 num win = #bits(d) / win size 3 acc = 1 4 for(win idx in [num win-1..0] ) 5 for(sqr iter in [0..win size-1] ) 6 acc = (acc * acc) mod n 7 d[win idx] = 8 bits(d, win idx*win size,win size) 9 acc = (acc * m^d[win idx]) mod n 10 return acc\n\nPrivate Key Recovery\nThe paper gives an example of 16 bit private key with 4 windows having 4 bit size. The value of d can be recovered with the equation \n\nFor the 16 bit private key, most significant window is assumed to be found as d3*. The key for window d2 can be determined using the above equation by varying the values of <di, p, f>. The experiment conducted for the private key search algorithm that is executed on a cluster of 80 machines for the FPGA (field programmable gate array) device to collect 10.000 message, corrupted signature pairs. The private key is recovered in approximately 104 hours.\n\nThe faults are generated by varying the voltage supply to the point at which the rate of the fault occurrence is maximized. For the device, it is found to be at 1.25V. In the overall faulty signatures, 88% of FWE invocation led to the faulty signatures, out of which 12% signatures are having a single bit fault.\n\nConclusion:\nIf the hardware is unprotected, there is a very high chance that it could be attacked. The end to end fault based attack is conducted successfully on RSA based system and fixed window exponentiation module. The attack is placed on widely used OpenSSL libra');
INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(347, 604, 'gt', 'Lecture Notes on\n\nGRAPH THEORY\nTero Harju\nDepartment of Mathematics University of Turku FIN-20014 Turku, Finland e-mail: harju@utu.fi\n\n2007\n\nContents\n\n1\n\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1 Graphs and their plane figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 Subgraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.3 Paths and cycles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 Connectivity of Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.1 Bipartite graphs and trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.2 Connectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Tours and Matchings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Eulerian graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Hamiltonian graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Matchings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Colourings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1 Edge colourings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Ramsey Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Vertex colourings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Graphs on Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Planar graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Colouring planar graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Genus of a graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 30 32 36 43 43 47 52 60 60 67 74\n\n2\n\n3\n\n4\n\n5\n\n6\n\nDirected Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 6.1 Digraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 6.2 Network Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n\n1 Introduction\nGraph theory can be said to have its beginning in 1736 when E ULER considered the (general case of the) Knigsberg bridge problem: Is there a walking route that crosses each of the seven bridges of Knigsberg exactly once? (Solutio Problematis ad geometriam situs pertinentis, Commentarii Academiae Scientiarum Imperialis Petropolitanae 8 (1736), pp. 128-140.) It took 200 years before the first book on graph theory was written. This was done by KNIG in 1936. (Theorie der endlichen und unendlichen Graphen, Teubner, Leipzig, 1936. Translation in English, 1990.) Since then graph theory has developed into an extensive and popular branch of mathematics, which has been applied to many problems in mathematics, computer science, and other scientific and not-so-scientific areas. For the history of early graph theory, see N.L. B IGGS , R.J. L LOYD Press, 1986.\nAND\n\nR.J. W ILSON, Graph Theory 1736  1936, Clarendon\n\nThere seem to be no standard notations or even definitions for graph theoretical objects. This is natural, because the names one uses for these objects reflect the applications. So, for instance, if we consider a communications network (say, for email) as a graph, then the computers, which take part in this network, are called nodes rather than vertices or points. On the other hand, other names are used for molecular structures in chemistry, flow charts in programming, human relations in social sciences, and so on. These lectures study finite graphs and majority of the topics is included in J.A. B ONDY\nAND\n\nU.S.R. M URTY, Graph Theory with Applications, Macmillan, 1978.\n\nR. D IESTEL, Graph Theory, Springer-Verlag, 1997. F. H ARARY, Graph Theory, Addison-Wesley, 1969. D.B. W EST, Introduction to Graph Theory, Prentice Hall, 1996. R.J. W ILSON, Introduction to Graph Theory, Longman, (3rd ed.) 1985. In these lectures we study combinatorial aspects of graphs. For more algebraic topics and methods, see N. B IGGS, Algebraic Graph Theory, Cambridge University Press, (2nd ed.) 1993. and for computational aspects, see S. E VEN, Graph Algorithms, Computer Science Press, 1979.\n\n3 In these lecture notes we mention several open problems that have gained respect among the researchers. Indeed, graph theory has the advantage that it contains easily formulated open problems that can be stated early in the theory. Finding a solution to any one of these problems is on another layer of difficulty. Sections with a star () in their heading are optional.\n\nNotations and notions\n For a finite set X , |X | denotes its size (cardinality, the number of its elements).  Let [1, n] = {1, 2, . . . , n}, and in general, for integers i  n.  For a real number x, the floor and the ceiling of x are the integers x = max{k  Z | k  x} and x = min{k  Z | x  k}.  A family {X1 , X2 , . . . , Xk } of subsets Xi  X of a set X is a partition of X , if X=\ni[1,k ]\n\n[i, n] = {i, i + 1, . . . , n}\n\nXi\n\nand\n\nXi  Xj =  for all different i and j .\n\n For two sets X and Y , X  Y = {(x, y ) | x  X, y  Y } is their Cartesian product.  For two sets X and Y ,\n\nis their symmetric difference. Here X  Y = {x | x  X, x  / Y }.  Two numbers n, k  N (often n = |X | and k = |Y | for sets X and Y ) have the same parity, if both are even, or both are odd, that is, if n  k (mod 2). Otherwise, they have opposite parity. Graph theory has abundant examples of NP-complete problems. Intuitively, a problem is in P 1 if there is an efficient (practical) algorithm to find a solution to it. On the other hand, a problem is in NP 2 , if it is first efficient to guess a solution and then efficient to check that this solution is correct. It is conjectured (and not known) that P = NP. This is one of the great problems in modern mathematics and theoretical computer science. If the guessing in NP-problems can be replaced by an efficient systematic search for a solution, then P=NP. For any one NP-complete problem, if it is in P, then necessarily P=NP.\n1 2\n\nX Y = (X  Y )  (Y  X )\n\nSolvable  by an algorithm  in polynomially many steps on the size of the problem instances. Solvable nondeterministically in polynomially many steps on the size of the problem instances.\n\n1.1 Graphs and their plane figures\n\n4\n\n1.1 Graphs and their plane figures\nLet V be a finite set, and denote by E (V ) = {{u, v } | u, v  V, u = v } . the subsets of V of two distinct elements. D EFINITION . A pair G = (V, E ) with E  E (V ) is called a graph (on V ). The elements of V are the vertices, and those of E the edges of the graph. The vertex set of a graph G is denoted by VG and its edge set by EG . Therefore G = (VG , EG ). In literature, graphs are also called simple graphs; vertices are called nodes or points; edges are called lines or links. The list of alternatives is long (but still finite). A pair {u, v } is usually written simply as uv . Notice that then uv = vu. In order to simplify notations, we also write v  G instead of v  VG . D EFINITION . For a graph G, we denote G = |VG | and G = |EG | . The number G of the vertices is called the order of G, and G is the size of G. For an edge e = uv  EG , the vertices u and v are its ends. Vertices u and v are adjacent or neighbours, if e = uv  EG . Two edges e1 = uv and e2 = uw having a common end, are adjacent with each other. A graph G can be represented as a plane figure by drawing a line (or a curve) between the points u and v (representing vertices) if e = uv is an edge of G. The figure on the right is a drawing of the graph G with VG = {v1 , v2 , v3 , v4 , v5 , v6 } and EG = {v1 v2 , v1 v3 , v2 v3 , v2 v4 , v5 v6 }.\n\nv1\n\nv3\n\nv6\n\nv2\n\nv4\n\nv5\n\nOften we shall omit the identities (names v ) of the vertices in our figures, in which case the vertices are drawn as anonymous circles. Graphs can be generalized by allowing loops vv and parallel (or multiple) edges between vertices to obtain a multigraph G = (V, E,  ), where E = {e1 , e2 , . . . , em } is a set (of symbols), and  : E  E (V )  {vv | v  V } is a function that attaches an unordered pair of vertices to each e  E :  (e) = uv . Note that we can have  (e1 ) =  (e2 ). This is drawn in the b figure of G by placing two (parallel) edges that connect the common ends. On the right there is (a drawing of) a multigraph G with vertices V = {a, b, c} and edges  (e1 ) = aa, a c  (e2 ) = ab,  (e3 ) = bc, and  (e4 ) = bc.\n\n1.1 Graphs and their plane figures Later we concentrate on (simple) graphs. D EFINITION . We also study directed graphs or digraphs D = (V, E ), where the edges have a direction, that is, the edges are ordered: E  V  V . In this case, uv = vu.\n\n5\n\nThe directed graphs have representations, where the edges are drawn as arrows. A digraph can contain edges uv and vu of opposite directions. Graphs and digraphs can also be coloured, labelled, and weighted: D EFINITION . A function  : VG  K is a vertex colouring of G by a set K of c'),
(348, 605, '01sassu', 'SBP 200\nLABORATORY REPORT I\nVISUALIZING BIO-MOLECULES (SPECIFICALLY PROTEINS)\n\nUNDER THE GUIDANCE OF\nPROF. ADITYA MITTAL MS. CHANCHAL ACHARYA\n\nSUBMITTED BY:\nASHWINI CHOUDHARY 2009CS10184 HIMANSHU MEENIA 2009CS10192 PARIKSHIT SHARMA 2009CS10204\n\nEXPERIMENT  PROTEIN EXTRACTION FROM MONOCOT AND DICOT PLANT LEAVES. EXPERIMENT  ASSESSING PROTEIN CONCENTRATION BY BRADFORD ASSAY USING SPECTROSCOPY. EXPERIMENT  FINDING MOLECULAR WEIGHTS OF PROTEINS USING SDS PAGE EXPERIMENT.\n\nDAY1 EXPERIMENT 1: PROTEIN EXTRACTION FROM MONOCOT AND DICOT PLANT LEAVES\nMonocots (monocotyledons) and dicots (dicotyledons) are the two major groups of flowering plants (or angiosperms) that are traditionally recognized. Monocot seedlings typically have one cotyledon (seed-leaf), in contrast to the two cotyledons typical of dicots. The basic way to distinguish the two types of leaves is that the major leaf veins in monocots are parallel whereas they are reticulated in dicot leaves.\n\nAIM\nThe target was to extract plant proteins from same amount (same biomass) of two different types of plant leaves viz. monocots and dicots.\n\nAPPARATUS & REAGENT USED\nPlant leaves (monocot and dicot), pestle and mortar, weighing balance, centrifuge machine, 70% ethanol, 50mM Tris-Cl, 5mM EDTA, 5mM BME and 1% PVP.\n\nREAGENT PREPARATION:\nPREPARATION OF 70% ETHANOL Take a measuring cylinder and mix 70ml of 100% ethanol with 30ml distilled water to get 70% ethanol (100ml). This solution is used for wiping all flasks, beakers, pestle and mortar throughout the experiment before using them to avoid contamination.\n\nPREPARATION OF TRIS-CL BUFFER We needed to prepare 200ml of 50mM Tris-Cl buffer (pH=7.5). Weight of Tris base required=50 mmol/lit * 0.2 lit * 121.14 gm/mol = 1.21 gms Add this amount of TriS base to 100ml distilled water and then monitoring the pH constantly using pH meter add 1N HCl to the solution using a 200-1000l pipette to get a pH of 7.5. Now add 0.292gm EDTA(5mM) , BME(5mM) solution and 2gm PVP(1%) to the above solution. Finally add distilled water to this solution so that the total volume becomes 200ml. Store the buffer at 4C temperature.\n\nROLES OF DIFFERENT BUFFER COMPONENTS\nTRIS-CL to form the buffer solution at pH 7.5 (to maintain the pH at 7.5). EDTA (Ethylenediaminetetraacetic acid) is a chelating agent, it binds to the divalent cations which are cofactors of enzymes like proteases. BME (Beta mercaptoethanol) is a strong reducing agent, hence it breaks SS bonds present in cysteine. BME also helps in removal of tannins and poly phenolic compounds in crude plant extract. PVP (Ployvinylpyrrolidone) helps to resolve the phenol content. CENTRIFUGING AT 4C helps us to maintain a suitable temperature for the proteins as the temperature increases due to the mechanical work done inside the centrifuge which may denature the protein.\n\nLEAF SAMPLES USED\n\nFigure 1: Day1 dicot\n\nFigure 2: Day2 dicot(Pilkhan)\n\nFigure 3: Day1 and Day2 monocot\n\nDifferent dicot leaves were chosen on Day 1 and Day 2 to see variation in the same family of leaves while the monocot leaf was kept same to study concentration change caused by storage and disintegration of proteins The monocot taken belongs to grass leaf family having high resemblance to Dichanthelium laxiflorum\n\nPROCEDURE:\n1. Take the two types of leaves and weigh both of them in equal amount (0.5gm) using weighing balance. 2. Now using the pestle and mortar crush the leaves and add 2ml of buffer solution and then get the liquid extract in separate eppendorf tubes. 3. Place the eppendorf tubes diagonally in the centrifuge and setting the temperature at 4C and the rotation speed at 13000rcf centrifuge for 15mins. 4. After this the pellet settles down at the base of eppendorf tube and the supernatant (soluble protein) is separated from it using a 200-1000l pipette and is stored in new eppendorf tubes. 5. Store these proteins at a very low temperature so as to avoid denaturation of proteins.\n\nOBSERVATIONS AND INFERENCES\nThe protein got from the monocot leaf was found to be light in colour (light greenish) whereas the protein got from dicot leaf was dark in color (brown). These colors may be attributed to the fact that monocots have more chlorophyll content as compared to dicots whereas there is a much large content and variety of proteins in dicots.\n\nPRECAUTIONS\n1. While weighing the salt or leaves in weighing balance always tare so as to account for papers weight and finalize the measurement only with closed lid so as to avoid error due to air interference. 2. Always wipe out different components as flasks, bottles, etc. with 70% ethanol before using them to provide sterile conditions. 3. While checking the pH of the buffer, HCl must not be dropped on the electrode this may generate error. 4. Place the appendorf tubes exactly opposite to each other with opening end facing out so as to avoid spilling of the extract inside the centrifuge. 5. Wipe out the centrifuge after using it as the ice deposited may melt and harm the machine. 6. While using micropipette, dont push the spring to the extreme when taking a sample and never leave the spring compressed after use as this may deteriorate the accuracy of the micropipette.\n\nDAY2 EXPERIMENT 2: ASSESSING PROTEIN CONCENTRATION BY BRADFORD ASSAY USING SPECTROSCOPY.\n\nINTRODUCTION:\nTo measure the concentration of protein in our solution we will measure the Optical density of a known protein, Bovine serum albumin (BSA) and plot a curve between concentration and Optical density. After getting the OD readings and plotting the curve we can measure OD by just giving concentration as input.\n\nREAGENTS USED:\n1. Bradford Reagent (Coomassie Brilliant Blue G250 0.05% (w/v), Ethanol 25 % (v/v), Phosphoric acid 42.5 % (v/v)) 2. BSA\n\nPROCEDURE:\n       Take 6 eppendorf tubes, add distilled 800,780,760....(L) water to it After this add 200L of Bradford to each tube known amount (in g) of BSA was added to make a total volume of 1ml Incubate the tubes for 5 minutes at room temperature measure the OD. First of all take the sample without BSA enzyme and set the base value for the OD measuring apparatus (spectrophotometer) Then take other samples one by one and note down the OD values\n\n     \n\nThe results obtained are shown in Figure 2.1 Plot the curve between Concentration (in g) of BSA and OD. Get the equation of the straight line. Add 750L water and 200L Bradford reagent to 50L protein extract Measure the OD for all the samples (Figure 2.2). Now use the equation of the straight line obtained above to get the concentration of protein in the leaf\n\nConc. of Vol. of BSA(L) Volume of Bradford BSA(g) Water(L) reagent (L) 0 0 800 200 4 40 760 200 6 60 740 200 8 80 720 200 10 100 700 200 12 120 680 200 Figure 2.1: Optical Density for various concentration of BSA\n\nOD Set as zero 0.216 0.249 0.270 0.279 0.286\n\nVolume of Protein Volume of water (L) (L) D1 50 750 M1 50 750 D2 50 750 M2 50 750 Figure2.2: Optical density for the protein extract D1: Day1 dicot sample Day 2 monocot sample D2: Day 2 dicot sample\n\nProtein\n\nBradford reagent (L) 200 200 200 200\n\nOD 0.358 0.314 1.254 0.351\n\nM1: Day 1 monocot sample M2:\n\nRESULTS AND CALCULATIONS:\n0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 0 5 10 15 Linear (Series1)\n\nEquation of line comes out to be: Y=0.008X+0.192\n\nConcentration of protein D1= 20.75 g Concentration of protein M1= 15.25 g Concentration of protein D2= 132.75 g Concentration of protein M2= 19.87 g\n\nWhen same experiment was done on day 2 we got the following line: Y=0.044X\n\nConcentration of protein in D1= 3.7 g Concentration of protein in M1= 2.77 g Concentration of protein in D2= 24.13 g Concentration of protein in M2= 3.63 g\n\nOBSERVATIONS:\n\n1. The results clearly shows that D2 has more protein concentration. 2. Also in the reading for the 2nd set the protein concentration has decreased, the reason could be denaturing and disintegration of proteins as the 2nd set of readings were taken after 1 week.\n\nPRECAUTIONS:\n1.) 2.) 3.) 4.) Do not touch the tip of pipette to the apprendorf tube Clean the cuvette to remove finger prints before taking OD reading The marker direction on the cuvette should be same for all the readings Bradford reagent should be kept in dark place\n\nDay3 and Day4\nEXPERIMENT 3: SDS-PAGE GEL ELECTROPHORESIS\n\nAIM\nVisualizing separate proteins of different molecular weight using SDS page\n\nREAGENTS USED\n1) ACRYLAMIDE AND N,N - METHYLENE-BIS-ACRYLAMIDE\nAcrylamide  29%(w/v) Bisacrylamide- 1%(w/v)\n\n2) SODIUM DODECYL SULFATE(SDS)\n10%(w/v)\n\n3) TRIS BUFFERS FOR THE PREPARATION OF RESOLVING AND STACKING GELS\nResolving gel (pH  8.8)  needs 1.5M Tris base Molecular weight  121.14 g , hence weight required = 181.71/lt. Need to prepare 100ml solution , hence 18.17g Stacking gel(pH - 6.8)  need 1M Tris base Hence 12.14g for 100ml solution pH of both was adjusted using HCl after dissolving in deionized H2O\n\n4) T EMED(N,N,N,N  TERTAMETHYLETHYLENEDIAMINE) 5) AMMONIUM PERSULFATE\nPrepared 10%(w/v) solution\n\n6) TRIS-GLYCINE ELECTROPHORESIS BUFFER\n\n7) STAINING SOLUTION\nGlacial acetic acid Methanol Coomassie Brilliant Blue\n\n8) DESTAINING SOLUTION :\nGlacial Acetic Acid Methanol\n\n12% RESOLVING GEL        3.2 ml- H2O 4.0 ml- 30% Acrylamide 2.6 ml- 1.5M Tris(pH 8.8) 100L- 10%SDS 100L- 10% APS 6 L - TEMED 5% STACKING GEL        1400 L - H2O 350 L - 30% Acrylamide 250 L - 1M Tris(pH 6.8) 20 L - 10% SDS 20 L - 10% APS 5 L - TEMED\n\nGEL EQUIPMENT ASSEMBLING AND CASTING\n  Clean all parts of the gel casting unit thoroughly Assemble the gel casting tray by placing the glass plates together with a spacer between them (1mm in our case). Clip the clamps onto the glass plates to hold them in place by rotating it to the 1mm mark Fill the gap in between the glass plates with water to check for any leakage from the base or sides of the casting unit. If no leakage is found discard the water else redo step two Prepare the resolving gel soup by adding above mentioned components\n\n\n\n\n\n  \n \n\nPou'),
(349, 606, 'title', 'Test Plan Sample\n\n1.\n\nINTRODUCTION ..................................................................................... 4 1.1. Overview of System X ..................................................................... 4 1.2. Purpose of this Document ................................................................ 4 1.3. Formal Reviewing ........................................................................... 5 1.3.1. Formal Review Points .............................................................. 5 1.4. Objectives of System Test ................................................................ 5 1.5. Software Quality Assurance involvement ............................................ 6\n\n2.\n\nSCOPE AND OBJECTIVES ......................................................................... 7 2.1. Scope of Test Approach - System Functions ........................................ 7 2.1.1. INCLUSIONS.......................................................................... 7 2.1.2. EXCLUSIONS ......................................................................... 7 2.1.3. SPECIFIC EXCLUSIONS ........................................................... 8 2.2. Testing Process............................................................................... 8 2.2.1. Exclusions ............................................................................. 9 2.3. Testing Scope................................................................................. 9 2.3.1. Functional Testing................................................................... 9 2.3.2. Integration Testing ............................................................... 10 2.3.3. Business (User) Acceptance Test............................................. 10 2.3.4. Performance Testing.............................................................. 10 2.3.5. Regression Testing ................................................................ 11 2.3.6. Bash & Multi-User Testing ...................................................... 11 2.3.7. Technical Testing .................................................................. 11 2.3.8. Operations Acceptance Testing (OAT) ...................................... 11 2.4. System Test Entrance/Exit Criteria .................................................. 11 2.4.1. Entrance Criteria .................................................................. 11 2.4.2. Exit Criteria ......................................................................... 12\n\n3.\n\nTEST PHASES AND CYCLES .................................................................... 14 3.1. System Testing Cycles ................................................................... 14 3.1.1. Automated Testing................................................................ 15 3.2. Software Delivery ......................................................................... 15 3.3. Formal Reviewing ......................................................................... 16 3.3.1. Formal Review Points ............................................................ 16 3.3.2. Progress/Results Monitoring ................................................... 17\n\n4. 5.\n\nSystem Test Schedule ........................................................................... 18 RESOURCES ........................................................................................ 20 5.1. Human ........................................................................................ 20\n\n5.2. Hardware .................................................................................... 21 5.2.1. Hardware components required .............................................. 21 5.3. Software ..................................................................................... 22 5.3.1. Test IMS environments .......................................................... 22 5.3.2. Test Environment Software .................................................... 22 5.3.3. Error Measurement System.................................................... 22 6. ROLES AND RESPONSIBILITIES .............................................................. 23 6.1. Management Team........................................................................ 23 6.2. Testing Team................................................................................ 23 6.3. Business Team ............................................................................. 24 6.4. Testing Support Team.................................................................... 24 7. 8. 9. Error Management & Configuration Management ....................................... 26 STATUS REPORTING.............................................................................. 27 8.1. Status Reporting........................................................................... 27 Issues, Risks and Assumptions ............................................................... 28 9.1. Issues/Risks ................................................................................ 28 9.2. Assumptions ................................................................................ 29 10. 11. Formal Signoff................................................................................ 30 APPENDICES .................................................................................. 31 11.1. 11.2. 11.3. 11.4. 11.5. 11.6. 12. 12.1. 12.2. 12.3. 12.4. 12.5. 12.6. 12.7. 12.8. 12.9. Purpose of Error Review Team...................................................... 31 Error Review Team Meeting Agenda. ............................................. 31 Classification of Bugs.................................................................. 31 Procedure for maintenance of Error Management system. ................ 32 Overnight Processing - Checking Accounting & Audit & CIS .............. 33 SOFTWARE QUALITY ASSURANCE MEASURES ................................ 34 Online Error Input Form .............................................................. 35 Check-Off Control Documentation................................................. 35 Verification/Checkoff & Output Testing .......................................... 35 Online Non Fixed Error Report...................................................... 35 Errors Assigned to Development Team .......................................... 35 SQA Lines of Communication ....................................................... 35 Error Process Paths .................................................................... 35 System Test Support .................................................................. 35 Error Status Flow ....................................................................... 35\n\nCONTROL DOCUMENTATION ............................................................. 35\n\n1. INTRODUCTION\n1.1. Overview of System X\nTo aim of this phase of the project is to implement a new X System platform that will enable:        Removal of legacy office systems Introduction of ABC Processing of Special Transactions No constraint on location of capture Enable capture of transactions for other processing systems New Reconciliation Process Positioning for European ECU Currency and future initiatives\n\nThis programme will result in significant changes to the current departmental and inter-office processes. The functionality will be delivered on a phased basis. Phase 1 will incorporate the following facilities :     Replacement of the legacy System A New Reconciliation System Outsourcing system for departments in different european countries. New/Revised Audit Trail & Query Facilities\n\n1.2. Purpose of this Document\nThis document is to serve as the Draft Test Approach for the Business Systems Development Project. Preparation for this test consists of three major stages: The Test Approach sets the scope of system testing, the overall strategy to be adopted, the activities to be completed, the general resources required\n\nand the methods and processes to be used to test the release. It also details the activities, dependencies and effort required to conduct the System Test.   Test Planning details the activities, dependencies and effort required to conduct the System Test. Test Conditions/Cases documents the tests to be applied, the data to be processed, the automated testing coverage and the expected results\n\n1.3. Formal Reviewing\n1.3.1. Formal Review Points 1. Design Documentation 2. Testing Approach 3. Unit Test Plans 4. Unit Test Conditions & Results 5. System Test Conditions 6. System Test Progress 7. Post System Test Review\n\n1.4. Objectives of System Test\nAt a high level, this System Test intends to prove that : The functionality, delivered by the development team, is as specified by the business in the Business Design Specification Document and the Requirements Documentation.  The software is of high quality; the software will replace/support the intended business functions and achieves the standards required by the company for the development of new systems.  The software delivered interfaces correctly with existing systems, including Windows 98.\n\n1.5. Software Quality Assurance involvement\n\nThe above V Model shows the optimum testing process, where test preparation commences as soon as the Requirements Catalogue is produced. System Test planning commenced at an early stage, and for this reason, the System test will benefit from Quality initiatives throughout the project lifecycle. The responsibility for testing between the Project & Software Qualtiy Assurance (S.Q.A.) is as follows:    Unit Test is the responsibility of the Development Team System Testing is the responsibility of SQA User Acceptance Testing is the Responsibility of the User Representatives Team Technology Compliance Testing is the responsibility of the Systems Installation & Support Group\n\n2. SCOPE AND OBJECTIVES\n2.1. Scope of Test Approach - System Functions\n2.1.1. INCLUSIONS The contents of this release are as follows :Phase 1 Deliverables o o o o o o o o o '),
(350, 607, 'srs software', 'Test Plan Sample\n\n1.\n\nINTRODUCTION ..................................................................................... 4 1.1. Overview of System X ..................................................................... 4 1.2. Purpose of this Document ................................................................ 4 1.3. Formal Reviewing ........................................................................... 5 1.3.1. Formal Review Points .............................................................. 5 1.4. Objectives of System Test ................................................................ 5 1.5. Software Quality Assurance involvement ............................................ 6\n\n2.\n\nSCOPE AND OBJECTIVES ......................................................................... 7 2.1. Scope of Test Approach - System Functions ........................................ 7 2.1.1. INCLUSIONS.......................................................................... 7 2.1.2. EXCLUSIONS ......................................................................... 7 2.1.3. SPECIFIC EXCLUSIONS ........................................................... 8 2.2. Testing Process............................................................................... 8 2.2.1. Exclusions ............................................................................. 9 2.3. Testing Scope................................................................................. 9 2.3.1. Functional Testing................................................................... 9 2.3.2. Integration Testing ............................................................... 10 2.3.3. Business (User) Acceptance Test............................................. 10 2.3.4. Performance Testing.............................................................. 10 2.3.5. Regression Testing ................................................................ 11 2.3.6. Bash & Multi-User Testing ...................................................... 11 2.3.7. Technical Testing .................................................................. 11 2.3.8. Operations Acceptance Testing (OAT) ...................................... 11 2.4. System Test Entrance/Exit Criteria .................................................. 11 2.4.1. Entrance Criteria .................................................................. 11 2.4.2. Exit Criteria ......................................................................... 12\n\n3.\n\nTEST PHASES AND CYCLES .................................................................... 14 3.1. System Testing Cycles ................................................................... 14 3.1.1. Automated Testing................................................................ 15 3.2. Software Delivery ......................................................................... 15 3.3. Formal Reviewing ......................................................................... 16 3.3.1. Formal Review Points ............................................................ 16 3.3.2. Progress/Results Monitoring ................................................... 17\n\n4. 5.\n\nSystem Test Schedule ........................................................................... 18 RESOURCES ........................................................................................ 20 5.1. Human ........................................................................................ 20\n\n5.2. Hardware .................................................................................... 21 5.2.1. Hardware components required .............................................. 21 5.3. Software ..................................................................................... 22 5.3.1. Test IMS environments .......................................................... 22 5.3.2. Test Environment Software .................................................... 22 5.3.3. Error Measurement System.................................................... 22 6. ROLES AND RESPONSIBILITIES .............................................................. 23 6.1. Management Team........................................................................ 23 6.2. Testing Team................................................................................ 23 6.3. Business Team ............................................................................. 24 6.4. Testing Support Team.................................................................... 24 7. 8. 9. Error Management & Configuration Management ....................................... 26 STATUS REPORTING.............................................................................. 27 8.1. Status Reporting........................................................................... 27 Issues, Risks and Assumptions ............................................................... 28 9.1. Issues/Risks ................................................................................ 28 9.2. Assumptions ................................................................................ 29 10. 11. Formal Signoff................................................................................ 30 APPENDICES .................................................................................. 31 11.1. 11.2. 11.3. 11.4. 11.5. 11.6. 12. 12.1. 12.2. 12.3. 12.4. 12.5. 12.6. 12.7. 12.8. 12.9. Purpose of Error Review Team...................................................... 31 Error Review Team Meeting Agenda. ............................................. 31 Classification of Bugs.................................................................. 31 Procedure for maintenance of Error Management system. ................ 32 Overnight Processing - Checking Accounting & Audit & CIS .............. 33 SOFTWARE QUALITY ASSURANCE MEASURES ................................ 34 Online Error Input Form .............................................................. 35 Check-Off Control Documentation................................................. 35 Verification/Checkoff & Output Testing .......................................... 35 Online Non Fixed Error Report...................................................... 35 Errors Assigned to Development Team .......................................... 35 SQA Lines of Communication ....................................................... 35 Error Process Paths .................................................................... 35 System Test Support .................................................................. 35 Error Status Flow ....................................................................... 35\n\nCONTROL DOCUMENTATION ............................................................. 35\n\n1. INTRODUCTION\n1.1. Overview of System X\nTo aim of this phase of the project is to implement a new X System platform that will enable:        Removal of legacy office systems Introduction of ABC Processing of Special Transactions No constraint on location of capture Enable capture of transactions for other processing systems New Reconciliation Process Positioning for European ECU Currency and future initiatives\n\nThis programme will result in significant changes to the current departmental and inter-office processes. The functionality will be delivered on a phased basis. Phase 1 will incorporate the following facilities :     Replacement of the legacy System A New Reconciliation System Outsourcing system for departments in different european countries. New/Revised Audit Trail & Query Facilities\n\n1.2. Purpose of this Document\nThis document is to serve as the Draft Test Approach for the Business Systems Development Project. Preparation for this test consists of three major stages: The Test Approach sets the scope of system testing, the overall strategy to be adopted, the activities to be completed, the general resources required\n\nand the methods and processes to be used to test the release. It also details the activities, dependencies and effort required to conduct the System Test.   Test Planning details the activities, dependencies and effort required to conduct the System Test. Test Conditions/Cases documents the tests to be applied, the data to be processed, the automated testing coverage and the expected results\n\n1.3. Formal Reviewing\n1.3.1. Formal Review Points 1. Design Documentation 2. Testing Approach 3. Unit Test Plans 4. Unit Test Conditions & Results 5. System Test Conditions 6. System Test Progress 7. Post System Test Review\n\n1.4. Objectives of System Test\nAt a high level, this System Test intends to prove that : The functionality, delivered by the development team, is as specified by the business in the Business Design Specification Document and the Requirements Documentation.  The software is of high quality; the software will replace/support the intended business functions and achieves the standards required by the company for the development of new systems.  The software delivered interfaces correctly with existing systems, including Windows 98.\n\n1.5. Software Quality Assurance involvement\n\nThe above V Model shows the optimum testing process, where test preparation commences as soon as the Requirements Catalogue is produced. System Test planning commenced at an early stage, and for this reason, the System test will benefit from Quality initiatives throughout the project lifecycle. The responsibility for testing between the Project & Software Qualtiy Assurance (S.Q.A.) is as follows:    Unit Test is the responsibility of the Development Team System Testing is the responsibility of SQA User Acceptance Testing is the Responsibility of the User Representatives Team Technology Compliance Testing is the responsibility of the Systems Installation & Support Group\n\n2. SCOPE AND OBJECTIVES\n2.1. Scope of Test Approach - System Functions\n2.1.1. INCLUSIONS The contents of this release are as follows :Phase 1 Deliverables o o o o o o o o o ');
INSERT INTO `paper_contents` (`id`, `paperId`, `title`, `contents`) VALUES
(351, 608, 'Test', '  \n     \n\n       \n\n \n    \n\n        \n\n  \n\n\n\n       \n                 \n \n      \n   \n \n\n\n\n   \n\n  \n\n \n\n\n      \n  \n\n     \n  \n             \n       \n    \n                                     \n          \n                    \n  \n    \n                 \n  \n         \n                                      \n      \n  \n   \n                         \n    \n     \n  \n \n\n       \n     \n  \n     \n   \n          \n     \n           \n   \n            \n             \n     \n               \n           \n         \n                            \n \n      \n       \n     \n                                     \n                       \n               \n       \n             \n          \n     \n       \n\n        \n   \n  \n                  \n  \n\n              \n  \n  \n        \n                        \n            \n               \n                   \n            \n   \n\n\n\n  \n       \n           \n      \n                \n                \n      \n     \n  \n              \n        \n     \n             \n   \n               \n       \n            \n                         \n              \n    \n     \n                       \n   \n \n                          \n                         \n                \n         \n       \n      \n      \n   \n    \n    \n\n       \n\n\n\n\n\n        \n     \n\n\n                                                        \n\n                 \n   \n         \n     \n                       \n    \n  \n              \n                       \n         \n     \n                                     \n               \n  \n\n    \n         \n              \n              \n     \n      \n                            \n                                 \n                 \n                \n      \n   \n          \n       \n \n        \n                 \n        \n          \n              \n         \n           \n  \n        \n                   \n          \n          \n \n      \n    \n                 \n    \n\n\n\n\n   \n\n   \n\n    \n \n\n   \n\n           \n   \n        \n    \n        \n             \n            \n                                                    \n\n    \n \n                \n           \n                    \n        \n            \n             \n    \n             \n                                          \n   \n\n\n\n\n\n\n      \n            \n       \n    \n  \n                                      \n      \n                     \n \n       \n         \n\n        \n  \n   \n         \n            \n   \n                                \n     \n    \n           \n          \n              \n     \n  \n        \n    \n\n \n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n             \n           \n       \n          \n         \n   \n          \n    \n     \n                                  \n   \n                 \n     \n  \n   \n           \n    \n                \n                  \n                       \n           \n        \n     \n    \n \n  \n                \n               \n \n\n   \n\n   \n\n\n\n \n\n \n\n \n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n \n\n \n\n\n\n\n\n \n\n  \n\n  \n\n  \n\n\n\n\n\n\n\n \n\n  \n \n\n   \n\n\n\n \n\n                                 \n\n\n       \n\n          \n            \n \n    \n \n             \n                    \n\n                      \n         \n     \n       \n  \n   \n    \n         \n \n    \n \n\n\n\n \n\n\n\n\n\n     \n       \n            \n                                       \n    \n        \n           \n \n        \n    \n             \n                 \n            \n               \n      \n        \n        \n               \n                    \n                                                              \n             \n                                                     \n                    \n         \n                      \n   \n        \n                 \n               \n             \n         \n                          \n                   \n          \n       \n \n            \n             \n \n         \n           \n  \n                 \n     \n                                                         \n     \n\n\n\n  \n      \n\n\n\n \n\n\n\n  \n     \n  \n\n       \n                  \n  \n       \n \n     \n                         \n          \n  \n                             \n     \n      \n \n                       \n             \n                        \n               \n        \n    \n             \n                                \n    \n  \n \n                                       ');

-- --------------------------------------------------------

--
-- Table structure for table `paper_key`
--

CREATE TABLE IF NOT EXISTS `paper_key` (
  `Id_key` int(10) unsigned NOT NULL,
  `Id_ppr` int(10) unsigned NOT NULL,
  PRIMARY KEY (`Id_key`,`Id_ppr`),
  KEY `Id_key` (`Id_key`),
  KEY `Id_ppr` (`Id_ppr`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

-- --------------------------------------------------------

--
-- Table structure for table `paper_tag`
--

CREATE TABLE IF NOT EXISTS `paper_tag` (
  `Id_tag` int(10) unsigned NOT NULL,
  `Id_ppr` int(10) unsigned NOT NULL,
  PRIMARY KEY (`Id_tag`,`Id_ppr`),
  KEY `Id_tag` (`Id_tag`),
  KEY `Id_ppr` (`Id_ppr`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

--
-- Dumping data for table `paper_tag`
--

INSERT INTO `paper_tag` (`Id_tag`, `Id_ppr`) VALUES
(1, 542),
(1, 543),
(1, 544),
(1, 545),
(1, 546),
(1, 547),
(1, 548),
(1, 567),
(1, 568),
(1, 569),
(1, 570),
(1, 571),
(1, 572),
(1, 573),
(1, 574),
(1, 575),
(1, 606),
(1, 608),
(2, 549),
(2, 550),
(2, 551),
(2, 552),
(2, 553),
(2, 554),
(2, 555),
(2, 556),
(2, 557),
(2, 576),
(2, 577),
(2, 578),
(2, 579),
(2, 580),
(2, 581),
(2, 582),
(2, 583),
(2, 584),
(2, 585),
(2, 603),
(3, 549),
(3, 558),
(3, 559),
(3, 560),
(3, 561),
(3, 562),
(3, 563),
(3, 564),
(3, 565),
(3, 566),
(3, 570),
(3, 586),
(3, 587),
(3, 588),
(3, 589),
(3, 590),
(3, 604),
(3, 605),
(3, 607);

-- --------------------------------------------------------

--
-- Table structure for table `papers`
--

CREATE TABLE IF NOT EXISTS `papers` (
  `Id_ppr` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(100) NOT NULL,
  `year` int(4) DEFAULT NULL,
  PRIMARY KEY (`Id_ppr`),
  UNIQUE KEY `name` (`name`),
  KEY `Id_ppr` (`Id_ppr`)
) ENGINE=InnoDB  DEFAULT CHARSET=latin1 AUTO_INCREMENT=609 ;

--
-- Dumping data for table `papers`
--

INSERT INTO `papers` (`Id_ppr`, `name`, `year`) VALUES
(542, 'On Spectral Clustering: Analysis and an algorithm', 2001),
(543, 'The CN2 Induction Algorithm', 1989),
(544, 'The algorithmic analysis of hybrid systems', 1995),
(545, 'Instance-based learning algorithms', 1991),
(546, 'Fast algorithms for mining association rules', 1994),
(547, 'Hierarchical mixtures of experts and the EM algorithm', 1994),
(548, 'Experiments with a New Boosting Algorithm', 1996),
(549, 'Constraint Networks', 1992),
(550, 'The structure and function of complex networks', 2003),
(551, 'A Survey on Sensor Networks', 2002),
(552, 'Network Information Flow', 2000),
(553, 'Wireless Ad Hoc Networks', 2002),
(554, 'Support-Vector Networks', 1995),
(555, 'Statistical Mechanics Of Complex Networks', 2001),
(556, 'The capacity of wireless networks', 2000),
(557, 'Resilient Overlay Networks', 2001),
(558, 'An introduction to variational methods for graphical models', 1999),
(559, 'Learning in graphical models', 2004),
(560, 'Sketchpad: A man-machine graphical communication system', 2003),
(561, 'Decimation of triangle meshes', 1992),
(562, 'Progressive Meshes', 0),
(563, 'Surface Simplification Using Quadric Error Metrics', 1997),
(564, 'Tcl and the Tk Toolkit', 1994),
(565, 'Plenoptic Modeling: An Image-Based Rendering System', 1995),
(566, 'The eyes have it: A task by data type taxonomy for information visualizations', 1996),
(567, 'A Fast Algorithm for Particle Simulations', 1987),
(568, 'A data locality optimizing algorithm', 1991),
(569, 'Algorithmic mechanism design', 1999),
(570, 'Factor Graphs and the Sum-Product Algorithm', 1998),
(571, 'A Fast Quantum Mechanical Algorithm for Database Search', 1996),
(572, 'A training algorithm for optimal margin classifiers', 1992),
(573, 'Graph-based algorithms for Boolean function manipulation', 1986),
(574, 'The FERET evaluation methodology for face recognition algorithms', 0),
(575, 'A universal algorithm for sequential data compression', 1977),
(576, 'Wireless sensor networks: a survey', 2002),
(577, 'SPINS: Security Protocols for Sensor Networks', 2001),
(578, 'System architecture directions for networked sensors', 2000),
(579, 'Snort - Lightweight Intrusion Detection for Networks', 1999),
(580, 'A tutorial on learning with Bayesian networks', 1995),
(581, 'Next century challenges: Scalable coordination in sensor networks', 1999),
(582, 'Learning Bayesian networks: The combination of knowledge and statistical data', 1995),
(583, 'Neural Network-Based Face Detection', 1998),
(584, 'Dynamic source routing in ad hoc wireless networks', 1996),
(585, 'ANALYSIS OF WIRELESS SENSOR NETWORKS FOR HABITAT MONITORING', 2004),
(586, 'Conditional random fields: Probabilistic models for segmenting and labeling sequence data', 2001),
(587, 'The Lumigraph', 1996),
(588, 'Recovering High Dynamic Range Radiance Maps from Photographs', 0),
(589, 'Statecharts: A Visual Formalism For Complex Systems', 1987),
(590, 'Closed-Form Solution of Absolute Orientation using Orthonormal Matrices', 1988),
(603, 'Term Paper', 2012),
(604, 'gt', 2012),
(605, '01sassu', 2012),
(606, 'title', 2012),
(607, 'srs software', 2012),
(608, 'Test', 2013);

-- --------------------------------------------------------

--
-- Table structure for table `tags`
--

CREATE TABLE IF NOT EXISTS `tags` (
  `Id_tag` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(40) NOT NULL,
  PRIMARY KEY (`Id_tag`),
  UNIQUE KEY `name` (`name`)
) ENGINE=InnoDB  DEFAULT CHARSET=latin1 AUTO_INCREMENT=4 ;

--
-- Dumping data for table `tags`
--

INSERT INTO `tags` (`Id_tag`, `name`) VALUES
(1, 'Algorithm'),
(3, 'Graphics'),
(2, 'Networks');

-- --------------------------------------------------------

--
-- Table structure for table `user_tags`
--

CREATE TABLE IF NOT EXISTS `user_tags` (
  `Id_usr` int(10) unsigned NOT NULL,
  `Id_tag` int(10) unsigned NOT NULL,
  PRIMARY KEY (`Id_usr`,`Id_tag`),
  KEY `Id_usr` (`Id_usr`),
  KEY `Id_tag` (`Id_tag`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

-- --------------------------------------------------------

--
-- Table structure for table `users`
--

CREATE TABLE IF NOT EXISTS `users` (
  `Id_usr` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `Email_id` varchar(45) NOT NULL,
  `username` varchar(30) NOT NULL,
  `userpass` varchar(30) NOT NULL,
  `priv` smallint(6) NOT NULL DEFAULT '1',
  PRIMARY KEY (`Id_usr`),
  UNIQUE KEY `Email_id` (`Email_id`),
  UNIQUE KEY `username` (`username`)
) ENGINE=InnoDB  DEFAULT CHARSET=latin1 AUTO_INCREMENT=15 ;

--
-- Dumping data for table `users`
--

INSERT INTO `users` (`Id_usr`, `Email_id`, `username`, `userpass`, `priv`) VALUES
(13, 'abcd@gmail.com', 'abc', 'a', 0),
(14, 'r@r', 'r', 'r', 1);

--
-- Constraints for dumped tables
--

--
-- Constraints for table `paper_author`
--
ALTER TABLE `paper_author`
  ADD CONSTRAINT `paper_author_ibfk_1` FOREIGN KEY (`Id_aut`) REFERENCES `authors` (`Id_aut`) ON DELETE CASCADE ON UPDATE CASCADE,
  ADD CONSTRAINT `paper_author_ibfk_2` FOREIGN KEY (`Id_ppr`) REFERENCES `papers` (`Id_ppr`) ON DELETE CASCADE ON UPDATE CASCADE;

--
-- Constraints for table `paper_key`
--
ALTER TABLE `paper_key`
  ADD CONSTRAINT `paper_key_ibfk_1` FOREIGN KEY (`Id_key`) REFERENCES `keywords` (`Id_key`) ON DELETE CASCADE ON UPDATE CASCADE,
  ADD CONSTRAINT `paper_key_ibfk_2` FOREIGN KEY (`Id_ppr`) REFERENCES `papers` (`Id_ppr`) ON DELETE CASCADE ON UPDATE CASCADE;

--
-- Constraints for table `paper_tag`
--
ALTER TABLE `paper_tag`
  ADD CONSTRAINT `paper_tag_ibfk_1` FOREIGN KEY (`Id_tag`) REFERENCES `tags` (`Id_tag`) ON DELETE CASCADE ON UPDATE CASCADE,
  ADD CONSTRAINT `paper_tag_ibfk_2` FOREIGN KEY (`Id_ppr`) REFERENCES `papers` (`Id_ppr`) ON DELETE CASCADE ON UPDATE CASCADE;

--
-- Constraints for table `user_tags`
--
ALTER TABLE `user_tags`
  ADD CONSTRAINT `user_tags_ibfk_1` FOREIGN KEY (`Id_usr`) REFERENCES `users` (`Id_usr`) ON DELETE CASCADE ON UPDATE CASCADE,
  ADD CONSTRAINT `user_tags_ibfk_2` FOREIGN KEY (`Id_tag`) REFERENCES `tags` (`Id_tag`) ON DELETE CASCADE ON UPDATE CASCADE;
--
-- Database: `ase`
--
CREATE DATABASE `ase` DEFAULT CHARACTER SET latin1 COLLATE latin1_swedish_ci;
USE `ase`;

-- --------------------------------------------------------

--
-- Table structure for table `admin`
--

CREATE TABLE IF NOT EXISTS `admin` (
  `name` text NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

-- --------------------------------------------------------

--
-- Table structure for table `ipmatch`
--

CREATE TABLE IF NOT EXISTS `ipmatch` (
  `tag` varchar(30) NOT NULL,
  `ip` varchar(30) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

--
-- Dumping data for table `ipmatch`
--

INSERT INTO `ipmatch` (`tag`, `ip`) VALUES
('Graphics', '127.0.0.1'),
('Algorithm', '127.0.0.1'),
('Networks', '127.0.0.1'),
('Machine Learning', '127.0.0.1'),
('Operating System', '127.0.0.1'),
('Software Engineering', '127.0.0.1');

-- --------------------------------------------------------

--
-- Table structure for table `users`
--

CREATE TABLE IF NOT EXISTS `users` (
  `Id_usr` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `Email_id` varchar(45) NOT NULL,
  `username` varchar(30) NOT NULL,
  `userpass` varchar(30) NOT NULL,
  `priv` smallint(6) NOT NULL DEFAULT '1',
  PRIMARY KEY (`Id_usr`),
  UNIQUE KEY `Email_id` (`Email_id`),
  UNIQUE KEY `username` (`username`)
) ENGINE=InnoDB  DEFAULT CHARSET=latin1 AUTO_INCREMENT=20 ;

--
-- Dumping data for table `users`
--

INSERT INTO `users` (`Id_usr`, `Email_id`, `username`, `userpass`, `priv`) VALUES
(3, 'shashank.verma590@gmail.com', 'asdf', 'a', 1),
(10, 'abcd@gmail.com', 'cs1090218', 'i', 1),
(11, 'sabcd@gmail.com', 'arbit', 'a', 1),
(12, 'a@a', 'himanshu', 'him', 0),
(13, 'ar@ar', 'ar', 'a', 1),
(14, 'abc@gmail.com', 'prodigy', 'g', 1),
(15, 'abhi.hakunamatata@gmail.com', 'abhiitd', 'qosaqome', 1),
(16, 'a@b.com', 'a', 'p', 1),
(17, 'ui@gmail.com', 'alpha', 'i', 1),
(18, 'soft@gmail.com', 'software', 'him', 1),
(19, 'new@new.com', 'new', 'new', 1);

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
